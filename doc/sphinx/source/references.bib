% Encoding: UTF-8

@TechReport{Ericsson2015,
  author      = {Ericsson},
  title       = {Cloud RAN - The Benefits of Cirtualization, Centralisation and Coordination},
  institution = {Ericsson},
  year        = {2015},
  file        = {:pdf/Ericsson2015 - Cloud RAN - The Benefits of Cirtualization, Centralisation and Coordination.pdf:PDF},
  groups      = {Cloud-RAN},
  url         = {https://www.ericsson.com/assets/local/publications/white-papers/wp-cloud-ran.pdf},
}

@TechReport{Huawei2013,
  author      = {Huawei},
  title       = {{5G}: A technology vision},
  institution = {Huawei},
  year        = {2013},
  groups      = {5G},
  url         = {https://www.huawei.com/ilink/en/download/HW_314849},
}

@InProceedings{Nikaein2015,
  author    = {N. Nikaein},
  title     = {Processing Radio Access Network Functions in the Cloud: Critical Issues and Modeling},
  booktitle = {International Workshop on Mobile Cloud Computing and Services (MCS)},
  year      = {2015},
  pages     = {36--43},
  publisher = {ACM},
  doi       = {10.1145/2802130.2802136},
  file      = {:pdf/Nikaein2015 - Processing Radio Access Network Functions in the Cloud\: Critical Issues and Modeling.pdf:PDF},
  groups    = {Cloud-RAN},
}

@Book{Dahlman2013,
  title     = {4G: LTE/LTE-Advanced for Mobile Broadband},
  publisher = {Academic press},
  year      = {2013},
  author    = {Dahlman, E. and Parkvall, S. and Skold, J.},
  groups    = {4G},
}

@Book{Knuth1973,
  title     = {The Art of Computer Programming},
  publisher = {Addison-Wesley},
  year      = {1973},
  author    = {Knuth, D.E.},
  number    = {3},
  file      = {:pdf/Knuth1973 - The Art of Computer Programming.pdf:PDF},
  pages     = {207-209},
}

@Article{Schreier1932,
  author  = {J. Schreier},
  title   = {On Tournament Elimination Systems},
  journal = {Mathesis Polska},
  year    = {1932},
  volume  = {7},
  pages   = {154--160},
}

@InProceedings{Furtak2007,
  author    = {T. Furtak and J. N. Amaral and R. Niewiadomski},
  title     = {Using {SIMD} Registers and Instructions to Enable Instruction-Level Parallelism in Sorting Algorithms},
  booktitle = {Symposium on Parallel Algorithms and Architectures},
  year      = {2007},
  pages     = {348--357},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Most contemporary processors offer some version of Single Instruction Multiple Data (SIMD) machinery - vector registers and instructions to manipulate data stored in such registers. The central idea of this paper is to use these SIMD resources to improve the performance of the tail of recursive sorting algorithms. When the number of elements to be sorted reaches a set threshold, data is loaded into the vector registers, manipulated in-register, and the result stored back to memory. Three implementations of sorting with two different SIMD machineries - x86-64's SSE2 and G5's AltiVec - demonstrate that this idea delivers significant speed improvements. The improvements provided are orthogonal to the gains obtained through empirical search for a suitable sorting algorithm [11]. When integrated with the Dynamically Tuned Sorting Library (DTSL) this new code generation strategy reduces the time spent by DTSL up to 22% for moderately-sized arrays, with greater relative reductions for small arrays. Wall-clock performance of d-heaps is improved by up to 39% using a similar technique.},
  acmid     = {1248436},
  doi       = {10.1145/1248377.1248436},
  file      = {:pdf/Furtak2007 - Using SIMD Registers and Instructions to Enable Instruction-Level Parallelism in Sorting Algorithms.pdf:PDF},
  groups    = {Single Instruction Multiple Data (SIMD), Sort},
  isbn      = {978-1-59593-667-7},
  keywords  = {SIMD, instruction-level parallelism, quicksort, sorting, sorting networks, vectorization},
  location  = {San Diego, California, USA},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/1248377.1248436},
}

@Article{Matsumoto1998,
  author  = {M. Matsumoto and T. Nishimura},
  title   = {Mersenne Twister: a 623-Dimensionally Equidistributed Uniform Pseudo-Random Number Generator},
  journal = {ACM Transactions on Modeling and Computer Simulation (TOMACS)},
  year    = {1998},
  volume  = {8},
  number  = {1},
  pages   = {3--30},
  doi     = {10.1145/272991.272995},
  file    = {:pdf/Matsumoto1998 - Mersenne Twister\: a 623-Dimensionally Equidistributed Uniform Pseudo-Random Number Generator.pdf:PDF},
  groups  = {Pseudo-Random Number Generator (PRNG)},
}

@Misc{Walter,
  author   = {J. Walter and M. Koch},
  title    = {uBLAS},
  note     = {\url{www.boost.org/libs/numeric}},
  abstract = {uBLAS is a C++ template class library that provides BLAS level 1, 2, 3 functionality for dense, packed and sparse matrices. The design and implementation unify mathematical notation via operator overloading and efficient code generation via expression templates.

uBLAS provides templated C++ classes for dense, unit and sparse vectors, dense, identity, triangular, banded, symmetric, hermitian and sparse matrices. Views into vectors and matrices can be constructed via ranges, slices, adaptor classes and indirect arrays. The library covers the usual basic linear algebra operations on vectors and matrices: reductions like different norms, addition and subtraction of vectors and matrices and multiplication with a scalar, inner and outer products of vectors, matrix vector and matrix matrix products and triangular solver. The glue between containers, views and expression templated operations is a mostly STL conforming iterator interface.},
  groups   = {Single Instruction Multiple Data (SIMD)},
  url      = {http://www.boost.org/libs/numeric},
}

@InProceedings{Henretty2011,
  author    = {T. Henretty and K. Stock and L-N. Pouchet and F. Franchetti and J. Ramanujam and P. Sadayappan},
  title     = {Data Layout Transformation for Stencil Computations on Short-Vector {SIMD} Architectures},
  booktitle = {International Conference on Compiler Construction (CC)},
  year      = {2011},
  file      = {:pdf/Henretty2011 - Data Layout Transformation for Stencil Computations on Short-Vector SIMD Architectures.pdf:PDF},
  groups    = {Single Instruction Multiple Data (SIMD)},
  keywords  = {stencil, data, layout, transformation, vectorization,simd},
}

@InProceedings{Videau2013,
  author    = {B. Videau and V. Marangozova-Martin and L. Genovese and T. Deutsch},
  title     = {Optimizing {3D} Convolutions for Wavelet Transforms on {CPUs} with {SSE} Units and {GPUs}},
  booktitle = {International European Conference on Parallel and Distributed Computing (Euro-Par)},
  year      = {2013},
  publisher = {Springer},
  doi       = {10.1007/978-3-642-40047-6_82},
  file      = {:pdf/Videau2013 - Optimizing 3D Convolutions for Wavelet Transforms on CPUs with SSE Units and GPUs.pdf:PDF},
  groups    = {Single Instruction Multiple Data (SIMD)},
  isbn      = {978-3-642-40047-6},
  keywords  = {optimization},
  url       = {https://doi.org/10.1007/978-3-642-40047-6_82},
}

@Article{Igual2011,
  author   = {F. D. Igual and E. Chan and E. S. Quintana-Ortí and G. Quintana-Ortí and R. A. {van de Geijn} and F. G. {Van Zee}},
  title    = {The {FLAME} Approach: From Dense Linear Algebra Algorithms to High-Performance Multi-Accelerator Implementations},
  journal  = {Elsevier Journal of Parallel and Distributed Computing (JPDC)},
  year     = {2012},
  volume   = {72},
  number   = {9},
  pages    = {1134--1143},
  month    = {Sept},
  issn     = {0098-3500},
  doi      = {10.1016/j.jpdc.2011.10.014},
  file     = {:pdf/Igual2011 - The FLAME Approach\: From Dense Linear Algebra Algorithms to High-Performance Multi-Accelerator Implementations.pdf:PDF},
  keywords = {flame, libflame, supermatrix, dense, linear algebra, parallel,runtime,scheduling,multi accelerator},
}

@InProceedings{Meng2010,
  author    = {L. Meng and Y. Voronenko and J. R. Johnson and M. Moreno Maza and F. Franchetti and Y. Xie},
  title     = {Spiral-Generated Modular {FFT} Algorithms},
  booktitle = {International Workshop on Parallel and Symbolic Computation (PASCO)},
  year      = {2010},
  series    = {PASCO '10},
  pages     = {169--170},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {This paper presents an extension of the Spiral system to automatically generate and optimize FFT algorithms for the discrete Fourier transform over finite fields. The generated code is intended to support modular algorithms for multivariate polynomial computations in the modpn library used by Maple. The resulting code provides an order of magnitude speedup over the original implementations in the modpn library, and the Spiral system provides the ability to automatically tune the FFT code to different computing platforms.},
  acmid     = {1837235},
  doi       = {10.1145/1837210.1837235},
  file      = {:pdf/Meng2010 - Spiral-Generated Modular FFT Algorithms.pdf:PDF},
  isbn      = {978-1-4503-0067-4},
  keywords  = {FFT, autotuning, code generation, high performance computing, modular arithmetic, vectorization, FFT, autotuning, code generation, high performance computing, modular arithmetic, vectorization},
  location  = {Grenoble, France},
  numpages  = {2},
  url       = {http://doi.acm.org/10.1145/1837210.1837235},
}

@Misc{Gallager1963,
  author = {Robert G. Gallager},
  title  = {Low-Density Parity-Check Codes},
  year   = {1963},
  groups = {LDPC Codes},
}

@Book{Wyglinski2009,
  title     = {Cognitive Radio Communications and Networks: Principles and Practice},
  publisher = {Academic Press},
  year      = {2009},
  author    = {A. M. Wyglinski and M. Nekovee and T. Hou},
  editor    = {Elsevier},
  file      = {:pdf/Wyglinski2009 - Cognitive Radio Communications and Networks\: Principles and Practice.pdf:PDF},
}

@InProceedings{Dutta2010,
  author    = {P. Dutta and Y. Kuo and A. Ledeczi and T. Schmid and P. Volgyesi},
  title     = {Putting the Software Radio on a Low-calorie Diet},
  booktitle = {Workshop on Hot Topics in Networks (HotNets)},
  year      = {2010},
  publisher = {ACM},
  abstract  = {Modern software-defined radios are large, expensive, and power-hungry devices and this, we argue, hampers their more widespread deployment and use, particularly in low-power, size-constrained application settings like mobile phones and sensor networks. To rectify this problem, we propose to put the software-defined radio on a diet by redesigning it around just two core chips -- an integrated RF transceiver and a Flash-based, mixed-signal FPGA. Modern transceivers integrate almost all RF front-end functions while emerging FPGAs integrate nearly all of required signal conditioning and processing functions. And, unlike conventional FPGAs, Flash-based FPGAs offer sleep mode power draws measured in the microamps and startup times measured in the microseconds, both of which are critical for low-power operation. If our platform architecture vision is realized, it will be possible to hold a software-defined radio in the palm of one's hand, build it for $100, and power it for days using the energy in a typical mobile phone battery. This will make software radios deployable in high densities and broadly accessible for research and education.},
  acmid     = {1868467},
  doi       = {10.1145/1868447.1868467},
  file      = {:pdf/Dutta2010 - Putting the Software Radio on a Low-calorie Diet.pdf:PDF},
  groups    = {Software Defined Radio (SDR)},
  isbn      = {978-1-4503-0409-2},
  keywords  = {energy-efficiency, low-power communications, software defined radios, wireless sensor networks},
  url       = {http://doi.acm.org/10.1145/1868447.1868467},
}

@Article{Shaik2013,
  author  = {S. Shaik and S. Angadi},
  title   = {Architecture and Component Selection for {SDR} Applications},
  journal = {International Journal of Engineering Trends and Technology (IJETT)},
  year    = {2013},
  volume  = {4},
  number  = {4},
  pages   = {691-694},
  file    = {:pdf/Shaik2013 - Architecture and Component Selection for SDR Applications.pdf:PDF},
  groups  = {Software Defined Radio (SDR)},
  url     = {http://www.ijettjournal.org/volume-4/issue-4/IJETT-V4I4P236.pdf},
}

@Misc{ETSI,
  author = {ETSI},
  title  = {{3GPP} - {TS} 136.212 - {Multiplexing} and channel coding ({R.} 11)},
  file   = {online:http\://www.etsi.org/deliver/etsi_ts/136200_136299/136212/11.01.00_60/ts_136212v110100p.pdf:PDF},
  groups = {4G},
}

@Article{Benkeser2009,
  author   = {Benkeser, C. and Burg, A. and Cupaiuolo, T. and Qiuting Huang},
  title    = {Design and Optimization of an {HSDPA} Turbo Decoder {ASIC}},
  journal  = {IEEE Journal of Solid-State Circuits (JSSC)},
  year     = {2009},
  volume   = {44},
  number   = {1},
  pages    = {98-106},
  abstract = {The turbo decoder is the most challenging component in a digital HSDPA receiver in terms of computation requirement and power consumption, where large block size and recursive algorithm prevent pipelining or parallelism to be effectively deployed. This paper addresses the complexity and power consumption issues at algorithmic, arithmetic and gate levels of ASIC design, in order to bring power consumption and die area of turbo decoders to a level commensurate with wireless application. Realized in 0.13 &nbsp;mum CMOS technology, the turbo decoder ASIC measures 1.2&nbsp;mm2 excluding pads, and can achieve 10.8 Mb/s throughput while consuming only 32 mW.},
  doi      = {10.1109/JSSC.2008.2007166},
  file     = {:pdf/Benkeser2009 - Design and Optimization of an HSDPA Turbo Decoder ASIC.pdf:PDF},
  groups   = {Turbo Codes, Hardware Decoders},
}

@Article{Sun2011,
  author  = {Y. Sun and J. R. Cavallaro},
  title   = {Efficient Hardware Implementation of a Highly-Parallel {3GPP} {LTE/LTE}-Advance Turbo Decoder},
  journal = {Elsevier Integration, the VLSI Journal},
  year    = {2011},
  volume  = {44},
  number  = {4},
  pages   = {305--315},
  file    = {:pdf/Sun2011 - Efficient Hardware Implementation of a Highly-Parallel 3GPP LTE LTE-Advance Turbo Decoder.pdf:PDF},
  groups  = {Turbo Codes, Hardware Decoders},
}

@Book{Stroustrup2013,
  title     = {The C++ Programming Language},
  publisher = {Addison-Wesley Professional},
  year      = {2013},
  author    = {Stroustrup, Bjarne},
  edition   = {4th},
  isbn      = {0321563840, 9780321563842},
  file      = {:pdf/Stroustrup2013 - The C++ Programming Language.pdf:PDF},
  groups    = {C++},
}

@Article{Gottschling2009,
  author   = {P. Gottschling and David S. Wise and Adwait Joshi},
  title    = {Generic Support of Algorithmic and Structural Recursion for Scientific Computing},
  journal  = {The International Journal of Parallel, Emergent and Distributed Systems ({IJPEDS})},
  year     = {2009},
  volume   = {24},
  number   = {6},
  pages    = {479 - 503},
  month    = {12/2009},
  note     = {Accepted},
  abstract = {Recursive algorithms, like quick-sort and recursive data structures, like trees, play a central role in programming. In the context of scientific computing, recursive algorithms and memory layouts are shown here to provide excellent cache and Translation Lookaside Buffer (TLB) locality independently of the platform. We show how, for the first time, generic programming and object-oriented programming allow us to abstract a multitude of dense-matrix memory layouts: from conventional row-major and column-major layouts over Z- and И-Morton orders to block-wise combinations of them. All are provided by a single class that is based on our new matrix abstraction.

The algorithmic recursion is supported in generic fashion by classes modelling the new recursator, an analogue of the Standard Template Library iterator. Although this concept supports recursion in general, we focus again on matrix operations. Results are presented for matrix multiplication, on both conventional and tiled representations using both homogeneous and heterogeneous matrix representations. Reaching about 60% peak performance in portable C++ code establishes competitive performance without explicit prefetching and other platform-specific tuning. Comparisons with the manufacturers' libraries show superior locality. These new techniques are embedded in the Matrix Template Library, Version 4 (MTL4).},
  doi      = {http://10.1080/17445760902758560},
  keywords = {dilated integers},
}

@InProceedings{Spampinato2014,
  author    = {D. G. Spampinato and M. P\"{u}schel},
  title     = {A Basic Linear Algebra Compiler},
  booktitle = {International Symposium on Code Generation and Optimization (CGO)},
  year      = {2014},
  series    = {CGO '14},
  pages     = {23:23--23:32},
  address   = {New York, NY, USA},
  publisher = {ACM, IEEE},
  acmid     = {2544155},
  articleno = {23},
  doi       = {10.1145/2544137.2544155},
  file      = {:pdf/Spampinato2014 - A Basic Linear Algebra Compiler.pdf:PDF},
  groups    = {Single Instruction Multiple Data (SIMD), Kernel},
  isbn      = {978-1-4503-2670-4},
  keywords  = {Basic linear algebra, DSL, Program synthesis, SIMD vectorization, Small matrices, Tiling},
  location  = {Orlando, FL, USA},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/2544137.2544155},
}

@Article{Williams2009,
  author     = {S. Williams and A. Waterman and D. Patterson},
  title      = {Roofline: An Insightful Visual Performance Model for Multicore Architectures},
  journal    = {Communications of the ACM},
  year       = {2009},
  volume     = {52},
  number     = {4},
  pages      = {65--76},
  month      = apr,
  issn       = {0001-0782},
  acmid      = {1498785},
  address    = {New York, NY, USA},
  doi        = {10.1145/1498765.1498785},
  file       = {:pdf/Williams2009 - Roofline\: An Insightful Visual Performance Model for Multicore Architectures.pdf:PDF},
  issue_date = {April 2009},
  numpages   = {12},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/1498765.1498785},
}

@Article{Treibig2009,
  author    = {J. Treibig and G. Hager},
  title     = {Introducing a Performance Model for Bandwidth-Limited Loop Kernels},
  journal   = {Computing Research Repository (CoRR)},
  year      = {2009},
  volume    = {abs/0905.0792},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-0905-0792},
  file      = {:pdf/Treibig2009 - Introducing a Performance Model for Bandwidth-Limited Loop Kernels.pdf:PDF},
  timestamp = {Mon, 05 Dec 2011 18:05:00 +0100},
  url       = {http://arxiv.org/abs/0905.0792},
}

@InProceedings{Cassagne2016b,
  author    = {A. Cassagne and O. Aumage and C. Leroux and D. Barthou and B. {Le Gal}},
  title     = {Energy Consumption Analysis of Software Polar Decoders on Low Power Processors},
  booktitle = {European Signal Processing Conference (EUSIPCO)},
  year      = {2016},
  pages     = {642--646},
  month     = aug,
  publisher = {IEEE},
  abstract  = {This paper presents a new dynamic and fully generic implementation of a Successive Cancellation (SC) decoder (multi-precision support and intra-/inter-frame strategy support). This fully generic SC decoder is used to perform comparisons of the different configurations in terms of throughput, latency and energy consumption. A special emphasis is given on the energy consumption on low power embedded processors for software defined radio (SDR) systems. A N=4096 code length, rate 1/2 software SC decoder consumes only 14 nJ per bit on an ARM Cortex-A57 core, while achieving 65 Mbps. Some design guidelines are given in order to adapt the configuration to the application context.},
  doi       = {10.1109/EUSIPCO.2016.7760327},
  file      = {:pdf/Cassagne2016b - Energy Consumption Analysis of Software Polar Decoders on Low Power Processors.pdf:PDF;:pdf/Cassagne2016b - Energy Consumption Analysis of Software Polar Decoders on Low Power Processors [poster].pdf:PDF},
  groups    = {Polar Codes, Software Decoders, HoF Polar - SC, AFF3CT},
  keywords  = {decoding, energy consumption, software radio, telecommunication power management, ARM Cortex-A57, SC decoder implementation, SDR system, low power embedded processor, software defined radio system, software polar decoder energy consumption analysis, successive cancellation decoder implementation, Bit error rate, Decoding, Encoding, Energy consumption, Program processors, Throughput},
}

@InProceedings{Cassagne2016a,
  author    = {A. Cassagne and T. Tonnellier and C. Leroux and B. {Le Gal} and O. Aumage and D. Barthou},
  title     = {Beyond {G}bps Turbo decoder on multi-core {CPUs}},
  booktitle = {International Symposium on Turbo Codes and Iterative Information Processing (ISTC)},
  year      = {2016},
  pages     = {136--140},
  month     = sep,
  publisher = {IEEE},
  abstract  = {This paper presents a high-throughput implementation of a portable software turbo decoder. The code is optimized for traditional multi-core CPUs (like x86) and it is based on the Enhanced max-log-MAP turbo decoding variant. The code follows the LTE-Advanced specification. The key of the high performance comes from an inter-frame SIMD strategy combined with a fixed-point representation. Our results show that proposed multi-core CPU implementation of turbo-decoders is a challenging alternative to GPU implementation in terms of throughput and energy efficiency. On a high-end processor, our software turbo-decoder exceeds 1 Gbps information throughput for all rate-1/3 LTE codes with K $<$; 4096.},
  doi       = {10.1109/ISTC.2016.7593092},
  file      = {:pdf/Cassagne2016a - Beyond Gbps Turbo Decoder on Multi-Core CPUs.pdf:PDF;:pdf/Cassagne2016a - Beyond Gbps Turbo Decoder on Multi-Core CPUs [poster].pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP, AFF3CT},
  keywords  = {codecs, maximum likelihood decoding, microprocessor chips, turbo codes, Gbps turbo decoder, energy efficiency, enhanced max-log-MAP turbo decoding variant, inter-frame SIMD strategy, multicore CPU, portable software turbo decoder, rate-l/3 LTE codes, Instruction sets, Measurement},
}

@Misc{AFF3CT2016,
  author   = {AFF3CT},
  title    = {{AFF3CT}: The first software release},
  year     = {2016},
  abstract = {This release contains Polar, Turbo, RSC (Recursive Systematic Convolutional), Repetition and RA (Repeat and Accumulate) codes. The simulation includes a BPSK modulation and an AWGN channel. Some Polar, RSC and Turbo decoders are optimized with SIMD instructions.},
  doi      = {10.5281/zenodo.55668},
  url      = {http://dx.doi.org/10.5281/zenodo.55668},
}

@InProceedings{Xianjun2013,
  author    = {J. Xianjun and C. Canfeng and P. Jääskeläinen and V. Guzma and H. Berg},
  title     = {A 122{Mb}/s Turbo decoder using a mid-range {GPU}},
  booktitle = {International Wireless Communications and Mobile Computing Conference (IWCMC)},
  year      = {2013},
  pages     = {1090--1094},
  month     = jul,
  publisher = {IEEE},
  abstract  = {Parallel implementations of Turbo decoding has been studied extensively. Traditionally, the number of parallel sub-decoders is limited to maintain acceptable code block error rate performance loss caused by the edge effect of code block division. In addition, the sub-decoders require synchronization to exchange information in the iterative process. In this paper, we propose loosening the synchronization between the sub-decoders to achieve higher utilization of parallel processor resources. Our method allows high degree of parallel processor utilization in decoding of a single code block providing a scalable software-based implementation. The proposed implementation is demonstrated using a graphics processing unit. We achieve 122.8Mbps decoding throughput using a medium range GPU, the Nvidia GTX480. This is, to the best of our knowledge, the fastest Turbo decoding throughput achieved with a GPU-based implementation.},
  doi       = {10.1109/IWCMC.2013.6583709},
  file      = {:pdf/Xianjun2013 - A 122Mbs Turbo decoder using a mid-range GPU.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  issn      = {2376-6492},
  keywords  = {graphics processing units, parallel processing, turbo codes, GPU based implementation, Nvidia GTX480, Turbo decoder, Turbo decoding throughput, code block division, code block error rate performance loss, edge effect, graphics processing unit, iterative process, medium range GPU, mid range GPU, parallel processor resources, parallel processor utilization, scalable software based implementation, single code block, subdecoders, synchronization, Decoding, Graphics processing units, Iterative decoding, Synchronization, Throughput, Turbo codes, GPGPU, Loose synchronization, Massively parallel computation, OpenCL, Turbo decoder},
}

@InProceedings{Belfanti2013,
  author    = {S. Belfanti and C. Roth and M. Gautschi and C. Benkeser and Q. Huang},
  title     = {A 1{Gb}ps {LTE}-Advanced Turbo-Decoder {ASIC} in 65{nm} {CMOS}},
  booktitle = {Symposium on VLSI Circuits},
  year      = {2013},
  pages     = {C284--C285},
  month     = jun,
  publisher = {IEEE},
  abstract  = {This paper presents a turbo-decoder ASIC for 3GPP LTE-Advanced supporting all specified code rates and block sizes. The highly parallelized architecture employs 16 SISO decoders with an optimized state-metric initialization scheme that reduces SISO-decoder latency, which is key for achieving very-high throughput. A novel CRC implementation for parallel turbo decoding prevents the decoder from performing redundant turbo iterations. The 65nm ASIC achieves a record data throughput of 1.013Gbps at 5.5 iterations with unprecedented energy efficiency of 0.17nJ/bit/iter.},
  file      = {:pdf/Belfanti2013 - A 1Gbps LTE-Advanced Turbo-Decoder ASIC in 65nm CMOS.pdf:PDF},
  groups    = {Turbo Codes, Hardware Decoders},
  issn      = {2158-5601},
  keywords  = {3G mobile communication, Long Term Evolution, decoding, 3GPP LTE advanced, CMOS, LTE advanced turbo decoder ASIC, SISO decoder latency, optimized state metric initialization scheme, parallel turbo decoding, redundant turbo iterations, size 65 nm, unprecedented energy efficiency, Application specific integrated circuits, Bit error rate, Decoding, Iterative decoding, Long Term Evolution, Throughput, ASIC implementation, CRC, LTE-Advanced, early termination, mobile communications, turbo decoder},
}

@Article{Giard2015,
  author    = {P. Giard and G. Sarkis and C. Thibeault and W. J. Gross},
  title     = {237~{Gb}it/s Unrolled Hardware Polar Decoder},
  journal   = {IET Electronics Letters},
  year      = {2015},
  volume    = {51},
  number    = {10},
  pages     = {762--763},
  issn      = {0013-5194},
  abstract  = {A new architecture for a polar decoder using a reduced complexity successive-cancellation (SC) decoding algorithm is presented. This novel fully unrolled, deeply pipelined architecture is capable of achieving a coded throughput of over 237 Gbit/s for a (1024, 512) polar code implemented using a field-programmable gate array. This decoder is two orders of magnitude faster than state-of-the-art polar decoders.},
  doi       = {10.1049/el.2014.4432},
  file      = {:pdf/Giard2015 - 237 Gbps Unrolled Hardware Polar Decoder.pdf:PDF},
  groups    = {Polar Codes, Hardware Decoders},
  keywords  = {decoding, field programmable gate arrays, FPGA, complexity successive cancellation, decoding algorithm, field programmable gate array, pipelined architecture, polar code, unrolled hardware polar decoder},
  publisher = {IEEE},
}

@InProceedings{Robertson1995,
  author    = {P. Robertson and E. Villebrun and P. Hoeher},
  title     = {A Comparison of Optimal and Sub-Optimal {MAP} Decoding Algorithms Operating in the Log Domain},
  booktitle = {International Conference on Communications (CC)},
  year      = {1995},
  volume    = {2},
  pages     = {1009--1013 vol.2},
  month     = jun,
  publisher = {IEEE},
  abstract  = {For estimating the states or outputs of a Markov process, the symbol-by-symbol MAP algorithm is optimal. However, this algorithm, even in its recursive form, poses technical difficulties because of numerical representation problems, the necessity of nonlinear functions and a high number of additions and multiplications. MAP like algorithms operating in the logarithmic domain presented in the past solve the numerical problem and reduce the computational complexity, but are suboptimal especially at low SNR (a common example is the max-log-MAP because of its use of the max function). A further simplification yields the soft-output Viterbi algorithm (SOVA). We present a log-MAP algorithm that avoids the approximations in the max-log-MAP algorithm and hence is equivalent to the true MAP, but without its major disadvantages. We compare the (log-)MAP, max-log-MAP and SOVA from a theoretical point of view to illuminate their commonalities and differences. As a practical example forming the basis for simulations, we consider Turbo decoding, where recursive systematic convolutional component codes are decoded with the three algorithms, and we also demonstrate the practical suitability of the log-MAP by including quantization effects. The SOVA is, at 10\textsuperscript{-4}, approximately 0.7 dB inferior to the (log-)MAP, the max-log-MAP lying roughly in between. We also present some complexity comparisons and conclude that the three algorithms increase in complexity in the order of their optimality},
  doi       = {10.1109/ICC.1995.524253},
  file      = {:pdf/Robertson1995 - A Comparison of Optimal and Sub-Optimal MAP Decoding Algorithms Operating in the Log Domain.pdf:PDF},
  groups    = {Turbo Codes},
  keywords  = {Markov processes, Viterbi decoding, computational complexity, convolutional codes, maximum likelihood decoding, maximum likelihood estimation, quantisation (signal), Markov process, SOVA, Turbo decoding, computational complexity reduction, log domain, log-MAP algorithm, low SNR, max function, max-log-MAP, optimal MAP decoding algorithms, quantization effects, recursive systematic convolutional component codes, simulations, soft-output Viterbi algorithm, suboptimal MAP decoding algorithms, Communications technology, Computational complexity, Convolution, Convolutional codes, Decoding, Markov processes, Quantization, State estimation, Turbo codes, Viterbi algorithm},
}

@InProceedings{Huang2011,
  author    = {L. Huang and Y. Luo and H. Wang and F. Yang and Z. Shi and D. Gu},
  title     = {A High Speed Turbo Decoder Implementation for {CPU}-Based {SDR} System},
  booktitle = {International Conference on Communication Technology and Applications (ICCTA)},
  year      = {2011},
  pages     = {19--23},
  month     = oct,
  publisher = {IEEE},
  abstract  = {More and more CPU-based SDR systems appear in recent two years. Such system requires high speed real-time signal processing. In this paper, we present our effort on the speed optimization of Turbo decoder, the most computation-demanding module in all baseband modules. We jointly consider the algorithm parallelism and the processor architecture. Single Instruction Multiple Data (SIMD) instruction is used for software implementation. The evaluation results show that this proposed design can achieve a maximum of 124 Mbps throughput for single Soft Input Soft Output (SISO) module with Max-Log-MAP algorithm.},
  doi       = {10.1049/cp.2011.0622},
  file      = {:pdf/Huang2011 - A High Speed Turbo Decoder Implementation for CPU-Based SDR System.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  keywords  = {MIMO communication, maximum likelihood estimation, signal processing, software radio, telecommunication computing, turbo codes, CPU-based SDR system, SIMD instruction, baseband module, bit rate 124 Mbit/s, computation-demanding module, high speed real-time signal processing, high speed turbo decoder implementation, max-log-MAP algorithm, processor architecture, single SISO module, single instruction multiple data instruction, single soft input soft output module, software-defined radio, speed optimization, CPU-based SDR, MAP algorithm, SIMD optimization, Turbo decoder},
}

@Article{Shannon1948,
  author   = {C. E. Shannon},
  title    = {A Mathematical Theory of Communication},
  journal  = {The Bell System Technical Journal},
  year     = {1948},
  volume   = {27},
  number   = {4},
  pages    = {623--656},
  month    = oct,
  issn     = {0005-8580},
  abstract = {In this final installment of the paper we consider the case where the signals or the messages or both are continuously variable, in contrast with the discrete nature assumed until now. To a considerable extent the continuous case can be obtained through a limiting process from the discrete case by dividing the continuum of messages and signals into a large but finite number of small regions and calculating the various parameters involved on a discrete basis. As the size of the regions is decreased these parameters in general approach as limits the proper values for the continuous case. There are, however, a few new effects that appear and also a general change of emphasis in the direction of specialization of the general results to particular cases.},
  doi      = {10.1002/j.1538-7305.1948.tb00917.x},
  file     = {:pdf/Shannon1948 - A Mathematical Theory of Communication.pdf:PDF},
}

@InProceedings{Giard2016,
  author    = {P. Giard and A. Balatsoukas-Stimming and T. C. Müller and A. Burg and C. Thibeault and W. J. Gross},
  title     = {A Multi-Gbps Unrolled Hardware List Decoder for a Systematic Polar Code},
  booktitle = {Asilomar Conference on Signals, Systems, and Computers (ACSSC)},
  year      = {2016},
  pages     = {1194--1198},
  month     = nov,
  publisher = {IEEE},
  abstract  = {Polar codes are a new class of block codes with an explicit construction that provably achieve the capacity of various communications channels, even with the low-complexity successive-cancellation (SC) decoding algorithm. Yet, the more complex successive-cancellation list (SCL) decoding algorithm is gathering more attention lately as it significantly improves the error-correction performance of short-to moderate-length polar codes, especially when they are concatenated with a cyclic redundancy check code. However, as SCL decoding explores several decoding paths, existing hardware implementations tend to be significantly slower than SC-based decoders. In this paper, we show how the unrolling technique, which has already been used in the context of SC decoding, can be adapted to SCL decoding yielding a multi-Gbps SCL-based polar decoder with an error-correction performance that is competitive when compared to an LDPC code of similar length and rate. Post-place-and-route ASIC results for 28 nm CMOS are provided showing that this decoder can sustain a throughput greater than 10 Gbps at 468 MHz with an energy efficiency of 7.25 pJ/bit.},
  doi       = {10.1109/ACSSC.2016.7869561},
  file      = {:pdf/Giard2016 - A Multi-Gbps Unrolled Hardware List Decoder for a Systematic Polar Code.pdf:PDF;:pdf/Giard2016a - Multi-Mode Unrolled Architectures for Polar Decoders.pdf:PDF;:pdf/Giard2016b - Low-Latency Software Polar Decoders.pdf:PDF},
  groups    = {Polar Codes, Hardware Decoders},
  keywords  = {block codes, channel capacity, channel coding, cyclic redundancy check codes, decoding, error correction codes, CMOS, block codes, communication channel capacity, complex successive-cancellation list decoding algorithm, concatenated code, cyclic redundancy check code, energy efficiency, error-correction performance, frequency 468 MHz, low-complexity successive-cancellation decoding algorithm, multi-Gbps unrolled hardware list decoder, post-place-and-route ASIC, short-moderate-length polar codes, size 28 nm, systematic polar code, unrolling technique, Decoding, Hardware, Measurement, Parity check codes, Standards, Systematics, Throughput},
}

@Article{Box1958,
  author    = {G. E. P. Box and M. E. Muller and others},
  title     = {A Note on the Generation of Random Normal Deviates},
  journal   = {The Annals of Mathematical Statistics},
  year      = {1958},
  volume    = {29},
  number    = {2},
  pages     = {610--611},
  file      = {:pdf/Box1958 - A Note on the Generation of Random Normal Deviates.pdf:PDF},
  groups    = {Pseudo-Random Number Generator (PRNG)},
  keywords  = {Box-Muller, Box, Muller},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Wang2014,
  author   = {R. Wang and R. Liu},
  title    = {A Novel Puncturing Scheme for Polar Codes},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2014},
  volume   = {18},
  number   = {12},
  pages    = {2081--2084},
  month    = dec,
  issn     = {1089-7798},
  abstract = {A novel kind of punctured polar codes is proposed in this paper. The codes are constructed by certain constraints on both puncturing patterns and frozen sets such that the values of the punctured bits are known to the decoder. As a result, the initialized log-likelihood ratios of the punctured bits can be set to infinity (or minus infinity), which guarantees a better decoding performance. Furthermore, explicit puncturing patterns are designed for arbitrary code lengths. The simulation results show that the proposed codes outperform the conventional punctured polar codes.},
  doi      = {10.1109/LCOMM.2014.2364845},
  file     = {:pdf/Wang2014 - A Novel Puncturing Scheme for Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {codes, arbitrary code lengths, log-likelihood ratios, polar codes, puncturing patterns, puncturing scheme, Bit error rate, Decoding, Error probability, Generators, Channel codes, channel codes, polar codes, length-compatible, polar code construction, puncturing, length-compatible, polar code construction, polar codes, puncturing},
}

@InProceedings{Liu2013,
  author    = {C. Liu and Z. Bie and C. Chen and X. Jiao},
  title     = {A Parallel {LTE} Turbo Decoder on {GPU}},
  booktitle = {International Conference on Communication Technology (ICCT)},
  year      = {2013},
  pages     = {609--614},
  month     = nov,
  publisher = {IEEE},
  abstract  = {Turbo codes were developed by Claude Berrou in 1993, which were based on convolutional codes and Viterbi algorithm and could closely approach the channel capacity. Software Defined Radio (SDR) promises to revolutionize the communication industry by delivering low-cost, flexible software solutions. Graphics Processing Units (GPUs) offer unprecedented application performance since they have many cores. GPUs have been widely used to accelerate a wide range of applications. In this paper, a 3GPP LTE Turbo decoder on GPU is designed, which can offer high throughput. Sub-frame-level parallelism and trellis state-level parallelism have been implemented by some researchers. In this paper, we propose a new method to accelerate our decoder: sub-decoder-level parallelism. In addition, shared memory is utilized to keep memory access fast. Finally, the bit error rate (BER) performance and throughput of the decoder are presented.},
  doi       = {10.1109/ICCT.2013.6820447},
  file      = {:pdf/Liu2013 - A Parallel LTE Turbo Decoder on GPU.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  keywords  = {Long Term Evolution, channel capacity, convolutional codes, error statistics, graphics processing units, turbo codes, 3GPP LTE turbo decoder, BER, Claude Berrou, GPU, Long Term Evolution, SDR, Viterbi algorithm, bit error rate, channel capacity, convolutional codes, graphics processing units, memory access, software defined radio, sub-frame-level parallelism, trellis state-level parallelism, turbo codes, Bit error rate, Decoding, Graphics processing units, Instruction sets, Iterative decoding, Parallel processing, Throughput, GPU, Parallel computing, Turbo decoding},
}

@InProceedings{Ying2012,
  author    = {Y. Ying and K. You and L. Zhou and H. Quan and M. Jing and Z. Yu and X. Zeng},
  title     = {A Pure Software {LDPC} Decoder on a Multi-Core Processor Platform with Reduced Inter-Processor Communication Cost},
  booktitle = {International Symposium on Circuits and Systems (ISCAS)},
  year      = {2012},
  pages     = {2609--2612},
  month     = may,
  publisher = {IEEE},
  abstract  = {As an error correction code, Low Density Parity Check (LDPC) code has been widely used in various communication standards such as WiMAX and DVB-S2. But these continuously-evolving communication standards and the high development cost and low-flexibility of hardwired ASIC solutions have pushed LDPC researchers to turn to more cost-efficient and flexible implementation, and thus the multi-core processor based implementation of LDPC decoder is gaining increasing attention in the last few years. However, the performance of the multi-core processor based implementation is far below the hardwired ASICs, with one of the key reasons that the cost of communication between processors is very high. Three approaches are proposed in this paper to reduce the communication cost, including: optimized algorithm partitioning to reduce communication traffic, utilizing imbalanced communication between tasks to optimize mapping and reduce overall communication distance, and simplified data sending-receiving mechanism to reduce the cost of identifying received data. By using these approaches, the communication time of the proposed implementation of LDPC decoder only accounts for 12.2\% of total decoding time, which generally occupies 50\% decoding time in the previously reported LDPC decoders on multi-core processors. And our work can achieve better throughput performance under the same hardware condition compared with other state-of-the-art works.},
  doi       = {10.1109/ISCAS.2012.6271839},
  file      = {:pdf/Ying2012 - A Pure Software LDPC Decoder on a Multi-Core Processor Platform with Reduced Inter-Processor Communication Cost.pdf:PDF},
  groups    = {LDPC Codes, Software Decoders, DVB},
  issn      = {0271-4302},
  keywords  = {WiMax, application specific integrated circuits, digital video broadcasting, error correction codes, multiprocessing systems, parity check codes, telecommunication standards, telecommunication traffic, DVB-S2, WiMAX, communication standards, communication traffic, data sending-receiving mechanism, error correction code, hardwired ASIC, inter-processor communication cost, low density parity check code, multicore processor platform, optimized algorithm partitioning, software LDPC decoder, Algorithm design and analysis, Decoding, Message passing, Multicore processing, Parity check codes, Partitioning algorithms, Program processors},
}

@InProceedings{Lin2014,
  author    = {J. Lin and C. Xiong and Z. Yan},
  title     = {A Reduced Latency List Decoding Algorithm for Polar Codes},
  booktitle = {International Workshop on Signal Processing Systems (SiPS)},
  year      = {2014},
  pages     = {1--6},
  month     = oct,
  publisher = {IEEE},
  abstract  = {The cyclic redundancy check (CRC) aided successive cancelation list (SCL) decoding algorithm has better error performance than the successive cancelation (SC) decoding algorithm for short or moderate polar codes. However, the CRC aided SCL (CA-SCL) decoding algorithm still suffer from long decoding latency. In this paper, a reduced latency list decoding (RLLD) algorithm for polar codes is proposed. For the proposed RLLD algorithm, all rate-0 nodes and part of rate-1 nodes are decoded instantly without traversing the corresponding subtree. A list maximum-likelihood decoding (LMLD) algorithm is proposed to decode the maximum likelihood (ML) nodes and the remaining rate-1 nodes. Moreover, a simplified LMLD (SLMLD) algorithm is also proposed to reduce the computational complexity of the LMLD algorithm. Suppose a partial parallel list decoder architecture with list size L = 4 is used, for an (8192, 4096) polar code, the proposed RLLD algorithm can reduce the number of decoding clock cycles and decoding latency by 6.97 and 6.77 times, respectively.},
  doi       = {10.1109/SiPS.2014.6986062},
  file      = {:pdf/Lin2014 - A Reduced Latency List Decoding Algorithm for Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {2162-3562},
  keywords  = {cyclic codes, cyclic redundancy check codes, decoding, error correction codes, CRC, LMLD algorithm, RLLD algorithm, SCL decoding algorithm, computational complexity, cyclic redundancy check, decoding clock cycles, decoding latency, list maximum-likelihood decoding, polar codes, reduced latency list decoding algorithm, successive cancellation list, Government},
}

@Article{Raymond2014,
  author   = {A. J. Raymond and W. J. Gross},
  title    = {A Scalable Successive-Cancellation Decoder for Polar Codes},
  journal  = {IEEE Transactions on Signal Processing (TSP)},
  year     = {2014},
  volume   = {62},
  number   = {20},
  pages    = {5339--5347},
  month    = oct,
  issn     = {1053-587X},
  abstract = {Polar codes are the first error-correcting codes to provably achieve channel capacity, asymptotically in code length, with an explicit construction. However, under successive-cancellation decoding, polar codes require very long code lengths to compete with existing modern codes. Nonetheless, the successive cancellation algorithm enables very-low-complexity implementations in hardware, due to the regular structure exhibited by polar codes. In this paper, we present an improved architecture for successive-cancellation decoding of polar codes, making use of a novel semi-parallel, encoder-based partial-sum computation module. We also provide quantization results for realistic code length N=2\textsuperscript{15}, and explore various optimization techniques such as a chained processing element and a variable quantization scheme. This design is shown to scale to code lengths of up to N=2\textsuperscript{21}, enabled by its low logic use, low register use and simple datapaths, limited almost exclusively by the amount of available SRAM. It also supports an overlapped loading of frames, allowing full-throughput decoding with a single set of input buffers.},
  doi      = {10.1109/TSP.2014.2347262},
  file     = {:pdf/Raymond2014 - A Scalable Successive-Cancellation Decoder for Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {channel capacity, decoding, error correction codes, optimisation, Polar codes, SRAM, channel capacity, error-correcting codes, full-throughput decoding, novel semi-parallel encoder-based partial-sum computation module, optimization techniques, scalable successive-cancellation decoder algorithm, variable quantization scheme, very-low-complexity implementations, Clocks, Computer architecture, Decoding, Indexes, Quantization (signal), Random access memory, Vectors, Error-correcting codes, hardware implementation, polar codes, successive-cancellation decoding},
}

@Article{Leroux2013,
  author   = {C. Leroux and A. J. Raymond and G. Sarkis and W. J. Gross},
  title    = {A Semi-Parallel Successive-Cancellation Decoder for Polar Codes},
  journal  = {IEEE Transactions on Signal Processing (TSP)},
  year     = {2013},
  volume   = {61},
  number   = {2},
  pages    = {289--299},
  month    = jan,
  issn     = {1053-587X},
  abstract = {Polar codes are a recently discovered family of capacity-achieving codes that are seen as a major breakthrough in coding theory. Motivated by the recent rapid progress in the theory of polar codes, we propose a semi-parallel architecture for the implementation of successive cancellation decoding. We take advantage of the recursive structure of polar codes to make efficient use of processing resources. The derived architecture has a very low processing complexity while the memory complexity remains similar to that of previous architectures. This drastic reduction in processing complexity allows very large polar code decoders to be implemented in hardware. An N=2\textsuperscript{17} polar code successive cancellation decoder is implemented in an FPGA. We also report synthesis results for ASIC.},
  doi      = {10.1109/TSP.2012.2223693},
  file     = {:pdf/Leroux2013 - A Semi-Parallel Successive-Cancellation Decoder for Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {application specific integrated circuits, codes, decoding, field programmable gate arrays, ASIC, FPGA, capacity-achieving codes, coding theory, polar codes, semi-parallel architecture, semi-parallel successive-cancellation decoder, successive cancellation decoding, very low processing complexity, Application specific integrated circuits, Complexity theory, Computer architecture, Decoding, Field programmable gate arrays, Hardware, Vectors, Codes, FPGA, VLSI, decoding, polar codes, successive cancellation},
}

@Article{Alamdar-Yazdi2011,
  author   = {A. Alamdar-Yazdi and F. R. Kschischang},
  title    = {A Simplified Successive-Cancellation Decoder for Polar Codes},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2011},
  volume   = {15},
  number   = {12},
  pages    = {1378--1380},
  month    = dec,
  issn     = {1089-7798},
  abstract = {A modification is introduced of the successive-cancellation decoder for polar codes, in which local decoders for rate-one constituent codes are simplified. This modification reduces the decoding latency and algorithmic complexity of the conventional decoder, while preserving the bit and block error rate. Significant latency and complexity reductions are achieved over a wide range of code rates.},
  doi      = {10.1109/LCOMM.2011.101811.111480},
  file     = {:pdf/Alamdar-Yazdi2011 - A Simplified Successive-Cancellation Decoder for Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {channel coding, error statistics, parity check codes, source coding, bit error rate, block error rate, constituent codes, local decoders, polar codes, successive cancellation decoder, Complexity theory, Decoding, Echo cancellers, Generators, Source coding, Vectors, Polar codes, successive-cancellation decoding, nodes, node, rate 1, rate one, rate 0, rate zero},
}

@InProceedings{Mishra2012,
  author    = {A. Mishra and A. J. Raymond and L. G. Amaru and G. Sarkis and C. Leroux and P. Meinerzhagen and A. Burg and W. J. Gross},
  title     = {A Successive Cancellation Decoder {ASIC} for a 1024\mbox{-}bit Polar Code in 180{nm} {CMOS}},
  booktitle = {Asian Solid-State Circuits Conference (A-SSCC)},
  year      = {2012},
  pages     = {205--208},
  month     = nov,
  publisher = {IEEE},
  abstract  = {This paper presents the first ASIC implementation of a successive cancellation (SC) decoder for polar codes. The implemented ASIC relies on a semi-parallel architecture where processing resources are reused to achieve good hardware efficiency. A speculative decoding technique is employed to increase the throughput by 25\% at the cost of very limited added complexity. The resulting architecture is implemented in a 180nm technology. The fabricated chip can be clocked at 150 MHz and uses 183k gates. It was verified using an FPGA testing setup and provides reference for the true silicon complexity of SC decoders for polar codes.},
  doi       = {10.1109/IPEC.2012.6522661},
  file      = {:pdf/Mishra2012 - A Successive Cancellation Decoder ASIC for a 1024-bit Polar Code in 180nm CMOS.pdf:PDF},
  groups    = {Polar Codes, Hardware Decoders},
  keywords  = {CMOS integrated circuits, application specific integrated circuits, codecs, decoding, field programmable gate arrays, ASIC implementation, CMOS, FPGA testing setup, polar code, silicon complexity, size 180 nm, speculative decoding technique, successive cancellation decoder},
}

@Article{Whaley2008,
  author    = {R. C. Whaley and A. M. Castaldo},
  title     = {Achieving Accurate and Context-Sensitive Timing for Code Optimization},
  journal   = {Software: Practice and Experience},
  year      = {2008},
  volume    = {38},
  number    = {15},
  pages     = {1621--1642},
  file      = {:pdf/Whaley2008 - Achieving Accurate and Context-Sensitive Timing for Code Optimization.pdf:PDF},
  keywords  = {atlas, methodology, accurate, optimization, timing, autotuning},
  publisher = {Wiley Online Library},
}

@Article{Li2012,
  author   = {B. Li and H. Shen and D. Tse},
  title    = {An Adaptive Successive Cancellation List Decoder for Polar Codes with Cyclic Redundancy Check},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2012},
  volume   = {16},
  number   = {12},
  pages    = {2044--2047},
  month    = dec,
  issn     = {1089-7798},
  abstract = {In this letter, we propose an adaptive SC (Successive Cancellation)-List decoder for polar codes with CRC. This adaptive SC-List decoder iteratively increases the list size until at least one survival path can pass CRC. Simulation shows that the adaptive SC-List decoder provides significant complexity reduction. We also demonstrate that polar code (2048, 1024) with 24-bit CRC decoded by our proposed adaptive SC-List decoder with very large maximum list size can achieve a frame error rate FER $\leq$ 10\textsuperscript{-3}{-3} at E\textsubscript{b}/N\textsubscript{o} = 1.1dB, which is about 0.25dB from the information theoretic limit at this block length.},
  doi      = {10.1109/LCOMM.2012.111612.121898},
  file     = {:pdf/Li2012 - An Adaptive Successive Cancellation List Decoder for Polar Codes with Cyclic Redundancy Check.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {adaptive codes, cyclic redundancy check codes, decoding, CRC code, FER, adaptive SC list decoder, adaptive successive cancellation list decoder, cyclic redundancy check codes, frame error rate, information theoretic limit, polar codes, word length 24 bit, Complexity theory, Cyclic redundancy check, Error analysis, Iterative decoding, Maximum likelihood decoding, Signal to noise ratio, Polar codes, list decoder},
}

@Article{Li2014,
  author    = {R. Li and Y. Dou and J. Xu and X. Niu and S. Ni},
  title     = {An Efficient Parallel {SOVA}-based Turbo Decoder for Software Defined Radio on {GPU}},
  journal   = {IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences},
  year      = {2014},
  volume    = {97},
  number    = {5},
  pages     = {1027--1036},
  doi       = {10.1587/transfun.E97.A.1027},
  file      = {:pdf/Li2014 - An Efficient Parallel SOVA-based Turbo Decoder for Software Defined Radio on GPU.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  publisher = {The Institute of Electronics, Information and Communication Engineers},
}

@InProceedings{Cassagne2015c,
  author    = {A. Cassagne and B. {Le Gal} and C. Leroux and O. Aumage and D. Barthou},
  title     = {An Efficient, Portable and Generic Library for Successive Cancellation Decoding of Polar Codes},
  booktitle = {International Workshop on Languages and Compilers for Parallel Computing (LCPC)},
  year      = {2015},
  month     = sep,
  publisher = {Springer},
  abstract  = {Error Correction Code decoding algorithms for consumer products such as Internet of Things (IoT) devices are usually implemented as dedicated hardware circuits. As processors are becoming increasingly powerful and energy efficient, there is now a strong desire to perform this processing in software to reduce production costs and time to market. The recently introduced family of Successive Cancellation decoders for Polar codes has been shown in several research works to efficiently leverage the ubiquitous SIMD units in modern CPUs, while offering strong potentials for a wide range of optimizations. The P-EDGE environment introduced in this paper, combines a specialized skeleton generator and a building blocks library routines to provide a generic, extensible Polar code exploration workbench. It enables ECC code designers to easily experiments with combinations of existing and new optimizations, while delivering performance close to state-of-art decoders.},
  date      = {2015-11-01},
  doi       = {10.1007/978-3-319-29778-1_19},
  file      = {:pdf/Cassagne2015c - An Efficient, Portable and Generic Library for Successive Cancellation Decoding of Polar Codes.pdf:PDF;:pdf/Cassagne2015c - An Efficient, Portable and Generic Library for Successive Cancellation Decoding of Polar Codes [slides].pdf:PDF;:pdf/Cassagne2015c - An Efficient, Portable and Generic Library for Successive Cancellation Decoding of Polar Codes [poster].pdf:PDF},
  groups    = {Polar Codes, Software Decoders, HoF Polar - SC, AFF3CT},
  journal   = {Languages and Compilers for Parallel Computing},
  url       = {http://dx.doi.org/10.1007/978-3-319-29778-1_19},
}

@InProceedings{Sarkis2014,
  author    = {G. Sarkis and P. Giard and C. Thibeault and W. J. Gross},
  title     = {Autogenerating Software Polar Decoders},
  booktitle = {Global Conference on Signal and Information Processing (GlobalSIP)},
  year      = {2014},
  pages     = {6--10},
  month     = dec,
  publisher = {IEEE},
  abstract  = {Polar decoders are well suited for high-speed software implementations. In this work, we present a framework for generating fully-unrolled software polar decoders with branchless data flow. We discuss the memory layout of data in these decoders and show the optimization techniques used. At 335 Mbps, when decoding a (2048, 1707) polar code, the resulting decoder has more than twice the speed of the state of the art floating-point software polar decoder.},
  doi       = {10.1109/GlobalSIP.2014.7032067},
  file      = {:pdf/Sarkis2014 - Autogenerating Software Polar Decoders.pdf:PDF;:pdf/Sarkis2014a - Fast Polar Decoders\: Algorithm and Implementation.pdf:PDF;:pdf/Sarkis2014b - Increasing the Speed of Polar List Decoders.pdf:PDF;:pdf/Sarkis2014a - Fast Polar Decoders\: Algorithm and Implementation2.pdf:PDF},
  groups    = {Polar Codes, Software Decoders, HoF Polar - SC},
  keywords  = {data flow computing, decoding, program compilers, source code (software), branchless data flow, data memory layout, floating-point software polar decoder, fully-unrolled software polar decoders, optimization techniques, software polar decoder autogeneration, Computer architecture, Hardware, Maximum likelihood decoding, Signal processing algorithms, Software, Throughput, decoder, polar codes, software},
}

@Article{Whaley2001,
  author    = {R. C. Whaley and A. Petitet and J. J. Dongarra},
  title     = {Automated Empirical Optimizations of Software and the {ATLAS} Project},
  journal   = {Elsevier Parallel Computing Journal},
  year      = {2001},
  volume    = {27},
  number    = {1},
  pages     = {3--35},
  doi       = {10.1016/S0167-8191(00)00087-9},
  file      = {:pdf/Whaley2001 - Automated Empirical Optimizations of Software and the ATLAS Project.pdf:PDF},
  publisher = {Elsevier},
}

@InProceedings{Jaeger2012,
  author    = {J. Jaeger and D. Barthou},
  title     = {Automatic Efficient Data Layout for Multithreaded Stencil Codes on {CPUs} and {GPUs}},
  booktitle = {International Conference on High Performance Computing (HiPC)},
  year      = {2012},
  pages     = {1--10},
  month     = dec,
  publisher = {IEEE},
  abstract  = {Stencil based computation on structured grids is a kernel at the heart of a large number of scientific applications. The variety of stencil kernels used in practice make this computation pattern difficult to assemble into a high performance computing library. With the multiplication of cores on a single chip, answering architectural alignment requirements became an even more important key to high performance. Along with vector accesses, data layout optimization must also consider concurrent parallel accesses. In this paper, we develop a strategy to automatically generate stencil codes for multicore vector architectures, searching for the best data layout possible to answer architectural alignment problems. We introduce a new method for aligning multidimensional data structures, called multipadding, that can be adapted to specificities of multicores and GPUs architectures. We present multiple methods with different level of complexity. We show on different stencil patterns that generated codes with multipadding display better performance than existing optimizations.},
  doi       = {10.1109/HiPC.2012.6507504},
  file      = {:pdf/Jaeger2012 - Automatic Efficient Data Layout for Multithreaded Stencil Codes on CPUs and GPUs.pdf:PDF},
  keywords  = {data structures, graphics processing units, grid computing, multi-threading, parallel processing, software libraries, CPU, GPU architectures, architectural alignment problems, architectural alignment requirements, automatic efficient data layout, computation pattern, concurrent parallel accesses, core multiplication, data layout optimization, high performance computing library, multicore vector architectures, multicores, multidimensional data structures, multipadding, multithreaded stencil codes, scientific applications, single chip, stencil based computation, stencil kernels, structured grids},
}

@Article{Wubben2014,
  author   = {D. Wubben and P. Rost and J. S. Bartelt and M. Lalam and V. Savin and M. Gorgoglione and A. Dekorsy and G. Fettweis},
  title    = {Benefits and Impact of Cloud Computing on {5G} Signal Processing: Flexible Centralization Through Cloud-{RAN}},
  journal  = {IEEE Signal Processing Magazine},
  year     = {2014},
  volume   = {31},
  number   = {6},
  pages    = {35--44},
  month    = nov,
  issn     = {1053-5888},
  abstract = {Cloud computing draws significant attention in the information technology (IT) community as it provides ubiquitous on-demand access to a shared pool of configurable computing resources with minimum management effort. It gains also more impact on the communication technology (CT) community and is currently discussed as an enabler for flexible, cost-efficient and more powerful mobile network implementations. Although centralized baseband pools are already investigated for the radio access network (RAN) to allow for efficient resource usage and advanced multicell algorithms, these technologies still require dedicated hardware and do not offer the same characteristics as cloud-computing platforms, i.e., on-demand provisioning, virtualization, resource pooling, elasticity, service metering, and multitenancy. However, these properties of cloud computing are key enablers for future mobile communication systems characterized by an ultradense deployment of radio access points (RAPs) leading to severe multicell interference in combination with a significant increase of the number of access nodes and huge fluctuations of the rate requirements over time. In this article, we will explore the benefits that cloud computing offers for fifth-generation (5G) mobile networks and investigate the implications on the signal processing algorithms.},
  doi      = {10.1109/MSP.2014.2334952},
  file     = {:pdf/Wubben2014 - Benefits and Impact of Cloud Computing on 5G Signal Processing\: Flexible Centralization Through Cloud-RAN.pdf:PDF},
  groups   = {5G},
  keywords = {4G mobile communication, cloud computing, information technology, mobile computing, radio access networks, radiofrequency interference, signal processing, 5G signal processing, RAP, access nodes, cloud computing, cloud-RAN, communication technology, information technology, multicell interference, radio access network, radio access points, resource pooling, service metering, 5G mobile communication, Cellular networks, Cloud computing, Decoding, Mobile communication, Mobile computing, Next generation networking, Radio access networks, Signal processing algorithms, Wireless cellular networks},
}

@InProceedings{Chen2013,
  author    = {Xiang Chen and Ji Zhu and Ziyu Wen and Yu Wang and Huazhong Yang},
  title     = {{BER} Guaranteed Optimization and Implementation of Parallel Turbo Decoding on {GPU}},
  booktitle = {International Conference on Communications and Networking in China (CHINACOM)},
  year      = {2013},
  pages     = {183--188},
  month     = aug,
  publisher = {IEEE},
  abstract  = {In this this paper, we present an optimized parallel implementation of a Bit Error Rate (BER) guaranteed turbo decoder on a General Purpose Graphic Process Unit (GPGPU). Actually, it is a critical task to implement complex communication signal processing over GPGPUs, since the parallelism over GPGPUs in general requires independent data streams for processing. So we explore both the inherent parallelisms and the extended sub-frame level parallelisms in turbo decoding and map them onto the recent GPU architecture. A guarding mechanism called Previous Iteration Value Initialization with Double Sided Training Window (PIVIDSTW) is used to minimize the loss of BER performance caused by sub-frame level parallelism, while the high throughput is still maintained. In addition, to explore the potential of parallelization in Turbo decoding on GPUs, the theoretical occupancy and scalability are analyzed with the consideration of the number of sub-frames per frame. Compared with previous work in [5] and [7], we achieve a better trade-off between BER performance and throughput concerns.},
  doi       = {10.1109/ChinaCom.2013.6694588},
  file      = {:pdf/Chen2013 - BER Guaranteed Optimization and Implementation of Parallel Turbo Decoding on GPU.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  keywords  = {error statistics, graphics processing units, turbo codes, BER guaranteed optimization, BER performance, GPGPU, GPU, GPU architecture, PIVIDSTW, bit error rate, complex communication signal processing, data streams, extended subframe level parallelisms, general purpose graphic process unit, parallel turbo decoding, parallelization, previous iteration value initialization with double sided training window, sub-frame level parallelism, theoretical occupancy, theoretical scalability, Bit error rate, Decoding, Graphics processing units, Instruction sets, Measurement, Parallel processing, Throughput},
}

@InProceedings{Niu2013,
  author    = {K. Niu and K. Chen and J. R. Lin},
  title     = {Beyond Turbo Codes: Rate-Compatible Punctured Polar Codes},
  booktitle = {International Conference on Communications (ICC)},
  year      = {2013},
  pages     = {3423--3427},
  month     = jun,
  publisher = {IEEE},
  abstract  = {CRC (cyclic redundancy check) concatenated polar codes are superior to the turbo codes under the successive cancellation list (SCL) or successive cancellation stack (SCS) decoding algorithms. But the code length of polar codes is limited to the power of two. In this paper, a family of rate-compatible punctured polar (RCPP) codes is proposed to satisfy the construction with arbitrary code length. We propose a simple quasi-uniform puncturing algorithm to generate the puncturing table. And we prove that this method has better row-weight property than that of the random puncturing. Simulation results under the binary input additive white Gaussian noise channels (BI-AWGNs) show that these RCPP codes outperform the performance of turbo codes in WCDMA (Wideband Code Division Multiple Access) or LTE (Long Term Evolution) wireless communication systems in the large range of code lengths. Especially, the RCPP code with CRC-aided SCL/SCS algorithm can provide over 0.7dB performance gain at the block error rate (BLER) of 10\textsuperscript{-4} with short code length M = 512 and code rate R = 0.5.},
  doi       = {10.1109/ICC.2013.6655078},
  file      = {:pdf/Niu2013 - Beyond Turbo Codes\: Rate-Compatible Punctured Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {1550-3607},
  keywords  = {AWGN channels, concatenated codes, decoding, error statistics, turbo codes, BI-AWGN, BLER, CRC concatenated codes, LTE, Long Term Evolution, RCPP codes, SCL decoding algorithms, SCS decoding algorithms, WCDMA, arbitrary code length, binary input additive white Gaussian noise channels, block error rate, cyclic redundancy check codes, puncturing table, quasi-uniform puncturing algorithm, random puncturing, rate-compatible punctured polar codes, row-weight property, successive cancellation list decoding algorithms, successive cancellation stack decoding algorithms, turbo codes, wideband code division multiple access, wireless communication systems, Decoding, Generators, Multiaccess communication, Spread spectrum communication, Turbo codes, Vectors},
}

@Article{Arikan2009,
  author   = {E. Arikan},
  title    = {Channel Polarization: A Method for Constructing Capacity-Achieving Codes for Symmetric Binary-Input Memoryless Channels},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  year     = {2009},
  volume   = {55},
  number   = {7},
  pages    = {3051--3073},
  month    = jul,
  issn     = {0018-9448},
  abstract = {A method is proposed, called channel polarization, to construct code sequences that achieve the symmetric capacity I(W) of any given binary-input discrete memoryless channel (B-DMC) W. The symmetric capacity is the highest rate achievable subject to using the input letters of the channel with equal probability. Channel polarization refers to the fact that it is possible to synthesize, out of N independent copies of a given B-DMC W, a second set of N binary-input channels {WN(i)1 les i les N} such that, as N becomes large, the fraction of indices i for which I(WN(i)) is near 1 approaches I(W) and the fraction for which I(WN(i)) is near 0 approaches 1-I(W). The polarized channels {WN(i)} are well-conditioned for channel coding: one need only send data at rate 1 through those with capacity near 1 and at rate 0 through the remaining. Codes constructed on the basis of this idea are called polar codes. The paper proves that, given any B-DMC W with I(W) $>$ 0 and any target rate R$<$ I(W) there exists a sequence of polar codes {Cfrn;nges1} such that Cfrn has block-length N=2n , rate ges R, and probability of block error under successive cancellation decoding bounded as Pe(N,R) les O(N-1/4) independently of the code rate. This performance is achievable by encoders and decoders with complexity O(N logN) for each.},
  doi      = {10.1109/TIT.2009.2021379},
  file     = {:pdf/Arikan2009 - Channel Polarization\: A Method for Constructing Capacity-Achieving Codes for Symmetric Binary-Input Memoryless Channels.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {binary codes, channel capacity, channel coding, decoding, memoryless systems, probability, channel capacity, channel coding, channel polarization, code sequence, polar codes, probability, successive cancellation decoding algorithm, symmetric binary-input memoryless channel, Capacity planning, Channel capacity, Channel coding, Codes, Councils, Decoding, Information theory, Memoryless systems, Noise cancellation, Polarization, Capacity-achieving codes, Plotkin construction, Reed-- Muller (RM) codes, channel capacity, channel polarization, polar codes, successive cancellation decoding},
}

@Article{Chase1972,
  author   = {D. Chase},
  title    = {Class of Algorithms for Decoding Block Codes with Channel Measurement Information},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  year     = {1972},
  volume   = {18},
  number   = {1},
  pages    = {170--182},
  month    = jan,
  issn     = {0018-9448},
  abstract = {A class of decoding algorithms that utilizes channel measurement information, in addition to the conventional use of the algebraic properties of the code, is presented. The maximum number of errors that can, with high probability, be corrected is equal to one less thand, the minimum Hamming distance of the code. This two-fold increase over the error-correcting capability of a conventional binary decoder is achieved by using channel measurement (soft-decision) information to provide a measure of the relative reliability of each of the received binary digits. An upper bound on these decoding algorithms is derived, which is proportional to the probability of an error fordth order diversity, an expression that has been evaluated for a wide range of communication channels and modulation techniques. With the aid of a lower bound on these algorithms, which is also a lower bound on a correlation (maximum-likelihood) decoder, we show for both the Gaussian and Rayleigh fading channels, that as the signal-to-noise ratio (SNR) increases, the asymptotic behavior of these decoding algorithms cannot be improved. Computer simulations indicate that even for !ow SNR the performance of a correlation decoder can be approached by relatively simple decoding procedures. In addition, we study the effect on the performance of these decoding algorithms when a threshold is used to simplify the decoding process.},
  doi      = {10.1109/TIT.1972.1054746},
  file     = {:pdf/Chase1972 - Class of Algorithms for Decoding Block Codes with Channel Measurement Information.pdf:PDF},
  groups   = {Error-Correcting Codes (ECC)},
  keywords = {Block codes, Decoding},
}

@Article{Rost2014,
  author   = {P. Rost and C. J. Bernardos and A. D. Domenico and M. D. Girolamo and M. Lalam and A. Maeder and D. Sabella and D. Wübben},
  title    = {Cloud Technologies for Flexible {5G} Radio Access Networks},
  journal  = {IEEE Communications Magazine},
  year     = {2014},
  volume   = {52},
  number   = {5},
  pages    = {68--76},
  month    = may,
  issn     = {0163-6804},
  abstract = {The evolution toward 5G mobile networks will be characterized by an increasing number of wireless devices, increasing device and service complexity, and the requirement to access mobile services ubiquitously. Two key enablers will allow the realization of the vision of 5G: very dense deployments and centralized processing. This article discusses the challenges and requirements in the design of 5G mobile networks based on these two key enablers. It discusses how cloud technologies and flexible functionality assignment in radio access networks enable network densification and centralized operation of the radio access network over heterogeneous backhaul networks. The article describes the fundamental concepts, shows how to evolve the 3GPP LTE a},
  doi      = {10.1109/MCOM.2014.6898939},
  file     = {:pdf/Rost2014 - Cloud Technologies for Flexible 5G Radio Access Networks.pdf:PDF},
  groups   = {5G, Cloud-RAN},
  keywords = {Algorithm design and analysis, Computer architecture, Long Term Evolution, Mobile communication, Mobile computing, Protocols, Radio access networks},
}

@Article{Zhang2017,
  author   = {Q. Zhang and A. Liu and X. Pan and K. Pan},
  title    = {{CRC} Code Design for List Decoding of Polar Codes},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2017},
  volume   = {21},
  number   = {6},
  pages    = {1229--1232},
  month    = jun,
  issn     = {1089-7798},
  abstract = {Cyclic redundancy check (CRC) assisted list successive cancellation decoding (SCLD) makes polar codes competitive with the state-of-art codes. In this letter, we try to find the optimal CRC for polar codes to further improve its performance. We first analyze the undetected error probability of CRC-aided SCLD as well as the characteristics of Hamming weight distribution of polar codes. Based on these characteristics, a multilevel SCLD-based searching strategy with moderate list size is proposed to compute the minimum Hamming weight distribution (MHWD) of different CRC-concatenated polar codes. Using the searched MHWD, the optimal CRC for polar codes are presented in this letter. Simulation results show that the performance of optimal CRC-aided SCLD significantly outperforms the standard one, especially at high code rate.},
  doi      = {10.1109/LCOMM.2017.2672539},
  file     = {:pdf/Zhang2017 - CRC Code Design for List Decoding of Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {Hamming codes, concatenated codes, cyclic redundancy check codes, decoding, probability, search problems, CRC code design, CRC-concatenated polar code, MHWD, cyclic redundancy check code design, list successive cancellation decoding, minimum Hamming weight distribution, multilevel SCLD-based searching strategy, undetected error probability, Cyclic redundancy check codes, Decoding, Error probability, Generators, Hamming weight, Standards, System performance, Polar codes, cyclic redundancy check, list successive cancellation decoder},
}

@Article{Studer2011,
  author   = {C. Studer and C. Benkeser and S. Belfanti and Q. Huang},
  title    = {Design and Implementation of a Parallel Turbo-Decoder {ASIC} for {3GPP}-{LTE}},
  journal  = {IEEE Journal of Solid-State Circuits (JSSC)},
  year     = {2011},
  volume   = {46},
  number   = {1},
  pages    = {8--17},
  month    = jan,
  issn     = {0018-9200},
  abstract = {Turbo-decoding for the 3GPP-LTE (Long Term Evolution) wireless communication standard is among the most challenging tasks in terms of computational complexity and power consumption of corresponding cellular devices. This paper addresses design and implementation aspects of parallel turbo-decoders that reach the 326.4 Mb/s LTE peak data-rate using multiple soft-input soft-output decoders that operate in parallel. To highlight the effectiveness of our design-approach, we realized a 3.57 mm\textsuperscript{2} radix-4based 8$\times$ parallel turbo-decoder ASIC in 0.13 $\mu$m CMOS technology achieving 390 Mb/s. At the more realistic 100 Mb/s LTE milestone targeted by industry today, the turbo-decoder consumes only 69 mW.},
  doi      = {10.1109/JSSC.2010.2075390},
  file     = {:pdf/Studer2011 - Design and Implementation of a Parallel Turbo-Decoder ASIC for 3GPP-LTE.pdf:PDF},
  groups   = {Turbo Codes, 4G},
  keywords = {3G mobile communication, CMOS integrated circuits, Long Term Evolution, application specific integrated circuits, cellular radio, communication complexity, parallel processing, turbo codes, 3GPP-LTE, CMOS technology, LTE wireless communication, Long Term Evolution, bit rate 390 Mbit/s, cellular device, computational complexity, parallel turbo-decoder ASIC, power 69 mW, power consumption, size 0.13 mum, soft-input soft-output decoder, turbo decoding, Decoding, Memory management, Multiaccess communication, Random access memory, Systematics, Throughput, 3G mobile communication, ASIC implementation, LTE, low-power, parallel turbo-decoder, radix-4},
}

@Article{Trifonov2012,
  author   = {P. Trifonov},
  title    = {Efficient Design and Decoding of Polar Codes},
  journal  = {IEEE Transactions on Communications (TCOM)},
  year     = {2012},
  volume   = {60},
  number   = {11},
  pages    = {3221--3227},
  month    = nov,
  issn     = {0090-6778},
  abstract = {Polar codes are shown to be instances of both generalized concatenated codes and multilevel codes. It is shown that the performance of a polar code can be improved by representing it as a multilevel code and applying the multistage decoding algorithm with maximum likelihood decoding of outer codes. Additional performance improvement is obtained by replacing polar outer codes with other ones with better error correction performance. In some cases this also results in complexity reduction. It is shown that Gaussian approximation for density evolution enables one to accurately predict the performance of polar codes and concatenated codes based on them.},
  doi      = {10.1109/TCOMM.2012.081512.110872},
  file     = {:pdf/Trifonov2012 - Efficient Design and Decoding of Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {Gaussian processes, concatenated codes, design, maximum likelihood decoding, Gaussian approximation, concatenated codes, density evolution, design, maximum likelihood decoding, multilevel codes, multistage decoding algorithm, polar codes, Approximation algorithms, Concatenated codes, Constellation diagram, Error probability, Maximum likelihood decoding, Vectors, Polar codes, concatenated codes, multilevel codes},
}

@Article{Hashemi2017,
  author   = {S. A. Hashemi and C. Condo and W. J. Gross},
  title    = {Fast and Flexible Successive-Cancellation List Decoders for Polar Codes},
  journal  = {IEEE Transactions on Signal Processing (TSP)},
  year     = {2017},
  volume   = {65},
  number   = {21},
  pages    = {5756--5769},
  month    = nov,
  issn     = {1053-587X},
  abstract = {Polar codes have gained significant amount of attention during the past few years and have been selected as a coding scheme for the next generation of mobile broadband standard. Among decoding schemes, successive-cancellation list (SCL) decoding provides a reasonable tradeoff between the error-correction performance and hardware implementation complexity when used to decode polar codes, at the cost of limited throughput. The simplified SCL (SSCL) and its extension SSCL-SPC increase the speed of decoding by removing redundant calculations when encountering particular information and frozen bit patterns (rate one and single parity check codes), while keeping the error-correction performance unaltered. In this paper, we improve SSCL and SSCL-SPC by proving that the list size imposes a specific number of path splitting required to decode rate one and single parity check codes. Thus, the number of splitting can be limited while guaranteeing exactly the same error-correction performance as if the paths were forked at each bit estimation. We call the new decoding algorithms Fast-SSCL and Fast-SSCL-SPC. Moreover, we show that the number of path forks in a practical application can be tuned to achieve desirable speed, while keeping the error-correction performance almost unchanged. Hardware architectures implementing both algorithms are then described and implemented: It is shown that our design can achieve $\mathbf {1.86}$ Gb/s throughput, higher than the best state-of-the-art decoders.},
  doi      = {10.1109/TSP.2017.2740204},
  file     = {:pdf/Hashemi2017 - Fast and Flexible Successive-Cancellation List Decoders for Polar Codes.pdf:PDF},
  groups   = {Polar Codes, Hardware Decoders},
  keywords = {block codes, broadband networks, decoding, error correction codes, linear codes, mobile communication, next generation networks, parity check codes, SCL decoding schemes, bit estimation, bit rate 1.86 Gbit/s, coding scheme, error correction performance, fast successive-cancellation list decoder, fast-SSCL, fast-SSCL-SPC, flexible successive-cancellation list decoder, frozen bit patterns, hardware implementation complexity, next generation mobile broadband standard, path splitting, polar code, simplified SCL, single parity check codes, Algorithm design and analysis, Estimation, Hardware, Maximum likelihood decoding, Signal processing algorithms, Throughput, Polar codes, hardware implementation, list decoding, successive-cancellation decoding},
}

@Article{Sarkis2016,
  author   = {G. Sarkis and P. Giard and A. Vardy and C. Thibeault and W. J. Gross},
  title    = {Fast List Decoders for Polar Codes},
  journal  = {IEEE Journal on Selected Areas in Communications (JSAC)},
  year     = {2016},
  volume   = {34},
  number   = {2},
  pages    = {318--328},
  month    = feb,
  issn     = {0733-8716},
  abstract = {Polar codes asymptotically achieve the symmetric capacity of memoryless channels, yet their error-correcting performance under successive-cancellation (SC) decoding for short and moderate length codes is worse than that of other modern codes such as low-density parity-check (LDPC) codes. Of the many methods to improve the error-correction performance of polar codes, list decoding yields the best results, especially when the polar code is concatenated with a cyclic redundancy check (CRC). List decoding involves exploring several decoding paths with SC decoding, and therefore tends to be slower than SC decoding itself, by an order of magnitude in practical implementations. In this paper, we present a new algorithm based on unrolling the decoding tree of the code that improves the speed of list decoding by an order of magnitude when implemented in software. Furthermore, we show that for software-defined radio applications, our proposed algorithm is faster than the fastest software implementations of LDPC decoders in the literature while offering comparable error-correction performance at similar or shorter code lengths.},
  doi      = {10.1109/JSAC.2015.2504299},
  file     = {:pdf/Sarkis2016 - Fast List Decoders for Polar Codes.pdf:PDF},
  groups   = {Polar Codes, Software Decoders, HoF Polar - SCL},
  keywords = {cyclic redundancy check codes, parity check codes, software radio, LDPC codes, cyclic redundancy check, fast list decoders, low-density parity-check codes, memoryless channels, polar codes, software-defined radio, successive-cancellation decoding, symmetric capacity, Complexity theory, Decoding, Hardware, Parity check codes, Reliability, Software, Software algorithms, LDPC, list decoding, polar codes, software decoders, software-defined radio},
}

@Article{Sarkis2014a,
  author   = {G. Sarkis and P. Giard and A. Vardy and C. Thibeault and W. J. Gross},
  title    = {Fast Polar Decoders: Algorithm and Implementation},
  journal  = {IEEE Journal on Selected Areas in Communications (JSAC)},
  year     = {2014},
  volume   = {32},
  number   = {5},
  pages    = {946--957},
  month    = may,
  issn     = {0733-8716},
  abstract = {Polar codes provably achieve the symmetric capacity of a memoryless channel while having an explicit construction. The adoption of polar codes however, has been hampered by the low throughput of their decoding algorithm. This work aims to increase the throughput of polar decoding hardware by an order of magnitude relative to successive-cancellation decoders and is more than 8 times faster than the current fastest polar decoder. We present an algorithm, architecture, and FPGA implementation of a flexible, gigabit-per-second polar decoder.},
  doi      = {10.1109/JSAC.2014.140514},
  file     = {:pdf/Sarkis2014a - Fast Polar Decoders\: Algorithm and Implementation.pdf:PDF;},
  groups   = {Polar Codes, Hardware Decoders},
  keywords = {block codes, decoding, error correction codes, field programmable gate arrays, linear codes, FPGA implementation, fast polar decoders, flexible polar decoder, gigabit-per-second polar decoder, polar codes, polar decoding hardware throughput, successive-cancellation decoders, symmetric memoryless channel capacity, Complexity theory, Maximum likelihood decoding, Parity check codes, Reliability, Systematics, Throughput, polar codes, storage systems, successive-cancellation decoding, node, nodes, spc, rep},
}

@InProceedings{Chinnici2012,
  author    = {S. Chinnici and P. Spallaccini},
  title     = {Fast Simulation of Turbo Codes on {GPU}s},
  booktitle = {International Symposium on Turbo Codes and Iterative Information Processing (ISTC)},
  year      = {2012},
  pages     = {61--65},
  month     = aug,
  publisher = {IEEE},
  abstract  = {Simulation of turbo codes with moderately long block lengths down to bit error rates of the order of 10\textsuperscript{-9} requires long runtimes on conventional CPUs. The approach described in this paper is based on a CPU/GPU co-processing strategy which aims at effectively distributing the processing tasks between the two platforms. In this paper, the most computationally intensive parts of turbo codes simulation are analyzed and their implementation on the GPU parallel architecture is discussed. Results on a case study for a serial concatenated convolutional code are presented, showing a simulation speed-up in excess of 10$\times$. These initial results show that the CPU/GPU approach is a powerful tool that allows the characterization of the high SNR behavior of turbo codes with a short simulation runtime.},
  doi       = {10.1109/ISTC.2012.6325199},
  file      = {:pdf/Chinnici2012 - Fast Simulation of Turbo Codes on GPUs.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  issn      = {2165-4700},
  keywords  = {graphics processing units, parallel architectures, turbo codes, CPU-GPU coprocessing strategy, GPU parallel architecture, bit error rates, conventional CPU, turbo codes simulation, Bit error rate, Computational modeling, Decoding, Graphics processing unit, Iterative decoding, Quadrature amplitude modulation, Turbo codes, GPU, SCCC, turbo codes},
}

@InProceedings{Giard2014,
  author    = {P. Giard and G. Sarkis and C. Thibeault and W. J. Gross},
  title     = {Fast Software Polar Decoders},
  booktitle = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2014},
  pages     = {7555--7559},
  month     = may,
  publisher = {IEEE},
  abstract  = {Among error-correcting codes, polar codes are the first to provably achieve channel capacity with an explicit construction. In this work, we present software implementations of a polar decoder that leverage the capabilities of modern general-purpose processors to achieve an information throughput in excess of 200 Mbps, a throughput well suited for software-defined-radio applications. We also show that, for a similar error-correction performance, the throughput of polar decoders both surpasses that of LDPC decoders targeting general-purpose processors and is competitive with that of state-of-the-art software LDPC decoders running on graphic processing units.},
  doi       = {10.1109/ICASSP.2014.6855069},
  file      = {:pdf/Giard2014 - Fast Software Polar Decoders.pdf:PDF},
  groups    = {Polar Codes, Software Decoders, HoF Polar - SC},
  issn      = {1520-6149},
  keywords  = {channel capacity, codecs, error correction codes, graphics processing units, parity check codes, software radio, channel capacity, error-correcting codes, error-correction performance, general-purpose processors, graphic processing units, information throughput, polar codes, software LDPC decoders, software implementations, software polar decoders, software-defined-radio applications, Decoding, Graphics processing units, Hardware, Parity check codes, Signal processing algorithms, Throughput, Decoding, Error-Correction, Polar Codes, Software-Defined-Radio},
}

@Article{Muller2009,
  author   = {O. Muller and A. Baghdadi and M. Jezequel},
  title    = {From Parallelism Levels to a Multi-{ASIP} Architecture for Turbo Decoding},
  journal  = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  year     = {2009},
  volume   = {17},
  number   = {1},
  pages    = {92--102},
  month    = jan,
  issn     = {1063-8210},
  abstract = {Emerging digital communication applications and the underlying architectures encounter drastically increasing performance and flexibility requirements. In this paper, we present a novel flexible multiprocessor platform for high throughput turbo decoding. The proposed platform enables exploiting all parallelism levels of turbo decoding applications to fulfill performance requirements. In order to fulfill flexibility requirements, the platform is structured around configurable application-specific instruction-set processors (ASIP) combined with an efficient memory and communication interconnect scheme. The designed ASIP has an single instruction multiple data (SIMD) architecture with a specialized and extensible instruction-set and 6-stages pipeline control. The attached memories and communication interfaces enable its integration in multiprocessor architectures. These multiprocessor architectures benefit from the recent shuffled decoding technique introduced in the turbo-decoding field to achieve higher throughput. The major characteristics of the proposed platform are its flexibility and scalability which make it reusable for all simple and double binary turbo codes of existing and emerging standards. Results obtained for double binary WiMAX turbo codes demonstrate around 250 Mb/s throughput using 16-ASIP multiprocessor architecture.},
  doi      = {10.1109/TVLSI.2008.2003164},
  file     = {:pdf/Muller2009 - From Parallelism Levels to a Multi-ASIP Architecture for Turbo Decoding.pdf:PDF},
  groups   = {Turbo Codes, Hardware Decoders},
  keywords = {binary codes, decoding, digital communication, instruction sets, multiprocessor interconnection networks, parallel architectures, turbo codes, application-specific instruction-set processors, binary turbo codes, communication interconnect scheme, digital communication, flexibility requirements, flexible multiprocessor platform, multi-ASIP architecture, multiprocessor architectures, parallelism levels, single instruction multiple data, turbo decoding, Application-specific instruction-set processor (ASIP), Bahl--Cocke--Jelinek--Raviv (BCJR), multiprocessor, parallel processing, turbo decoding},
}

@Article{MacKay1999,
  author   = {D. J. C. MacKay},
  title    = {Good Error-Correcting Codes based on Very Sparse Matrices},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  year     = {1999},
  volume   = {45},
  number   = {2},
  pages    = {399--431},
  month    = mar,
  issn     = {0018-9448},
  abstract = {We study two families of error-correcting codes defined in terms of very sparse matrices. {\textquotedblleft}MN{\textquotedblright} (MacKay-Neal (1995)) codes are recently invented, and {\textquotedblleft}Gallager codes{\textquotedblright} were first investigated in 1962, but appear to have been largely forgotten, in spite of their excellent properties. The decoding of both codes can be tackled with a practical sum-product algorithm. We prove that these codes are {\textquotedblleft}very good{\textquotedblright}, in that sequences of codes exist which, when optimally decoded, achieve information rates up to the Shannon limit. This result holds not only for the binary-symmetric channel but also for any channel with symmetric stationary ergodic noise. We give experimental results for binary-symmetric channels and Gaussian channels demonstrating that practical performance substantially better than that of standard convolutional and concatenated codes can be achieved; indeed, the performance of Gallager codes is almost as close to the Shannon limit as that of turbo codes},
  doi      = {10.1109/18.748992},
  file     = {:pdf/MacKay1999 - Good Error-Correcting Codes based on Very Sparse Matrices.pdf:PDF},
  groups   = {LDPC Codes},
  keywords = {Gaussian channels, channel coding, decoding, error correction codes, noise, sparse matrices, Gallager codes, Gaussian channels, MacKay-Neal codes, Shannon limit, binary-symmetric channel, channel coding, code sequences, concatenated codes, convolutional codes, decoding, error-correcting codes, information rates, optimally decoded codes, performance, sum-product algorithm, symmetric stationary ergodic noise, turbo codes, very sparse matrices, Code standards, Concatenated codes, Convolutional codes, Decoding, Error correction codes, Gaussian channels, Information rates, Sparse matrices, Sum product algorithm, Turbo codes},
}

@InProceedings{Yoge2012,
  author    = {D. R. N. Yoge and N. Chandrachoodan},
  title     = {{GPU} Implementation of a Programmable Turbo Decoder for Software Defined Radio Applications},
  booktitle = {International Conference on VLSI Design},
  year      = {2012},
  pages     = {149--154},
  month     = jan,
  publisher = {IEEE},
  abstract  = {This paper presents the implementation of a 3GPP standards compliant configurable turbo decoder on a GPU. The challenge in implementing a turbo decoder on a GPU is in suitably parallelizing the Log-MAP decoding algorithm and doing an architecture aware mapping of it on to the GPU. The approximations in parallelizing the Log-MAP algorithm come at the cost of reduced BER performance. To mitigate this reduction, different guarding mechanisms of varying computational complexity have been presented. The limited shared memory and registers available on GPUs are carefully allocated to obtain a high real-time decoding rate without requiring several independent data streams in parallel.},
  doi       = {10.1109/VLSID.2012.62},
  file      = {:pdf/Yoge2012 - GPU Implementation of a Programmable Turbo Decoder for Software Defined Radio Applications.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP, Software Defined Radio (SDR)},
  issn      = {1063-9667},
  keywords  = {3G mobile communication, approximation theory, coprocessors, error statistics, graphics processing units, maximum likelihood decoding, software radio, turbo codes, 3GPP standards compliant configurable turbo decoder, GPU implementation, Log-MAP decoding algorithm, approximations, programmable turbo decoder, reduced BER performance, software defined radio applications, Bit error rate, Decoding, Graphics processing unit, Instruction sets, Measurement, Parallel processing, Throughput, CUDA, GPU implementation, Guarding Mechanisms, Parallel Log-MAP, Turbo Decoder},
}

@Article{Leroux2012,
  author    = {C. Leroux and A. J. Raymond and G. Sarkis and I. Tal and A. Vardy and W. J. Gross},
  title     = {Hardware Implementation of Successive-Cancellation Decoders for Polar Codes},
  journal   = {Springer Journal of Signal Processing Systems (JSPS)},
  year      = {2012},
  volume    = {69},
  number    = {3},
  pages     = {305},
  month     = dec,
  abstract  = {The recently-discovered polar codes are seen as a major breakthrough in coding theory; they provably achieve the theoretical capacity of discrete memoryless channels using the low-complexity successive cancellation decoding algorithm. Motivated by recent developments in polar coding theory, we propose a family of efficient hardware implementations for successive cancellation (SC) polar decoders. We show that such decoders can be implemented with O(N) processing elements and O(N) memory elements. Furthermore, we show that SC decoding can be implemented in the logarithmic domain, thereby eliminating costly multiplication and division operations, and reducing the complexity of each processing element greatly. We also present a detailed architecture for an SC decoder and provide logic synthesis results confirming the linear complexity growth of the decoder as the code length increases.},
  date      = {2012-12-01},
  doi       = {10.1007/s11265-012-0685-3},
  file      = {:pdf/Leroux2012 - Hardware Implementation of Successive-Cancellation Decoders for Polar Codes.pdf:PDF},
  groups    = {Polar Codes, Hardware Decoders},
  publisher = {Springer},
  url       = {http://dx.doi.org/10.1007/s11265-012-0685-3},
}

@InProceedings{Wang2013,
  author    = {G. Wang and M. Wu and B. Yin and J. R. Cavallaro},
  title     = {High Throughput Low Latency {LDPC} Decoding on {GPU} for {SDR} Systems},
  booktitle = {Global Conference on Signal and Information Processing (GlobalSIP)},
  year      = {2013},
  pages     = {1258--1261},
  month     = dec,
  publisher = {IEEE},
  abstract  = {In this paper, we present a high throughput and low latency LDPC (low-density parity-check) decoder implementation on GPUs (graphics processing units). The existing GPU-based LDPC decoder implementations suffer from low throughput and long latency, which prevent them from being used in practical SDR (software-defined radio) systems. To overcome this problem, we present optimization techniques for a parallel LDPC decoder including algorithm optimization, fully coalesced memory access, asynchronous data transfer and multi-stream concurrent kernel execution for modern GPU architectures. Experimental results demonstrate that the proposed LDPC decoder achieves 316 Mbps (at 10 iterations) peak throughput on a single GPU. The decoding latency, which is much lower than that of the state of the art, varies from 0.207 ms to 1.266 ms for different throughput requirements from 62.5 Mbps to 304.16 Mbps. When using four GPUs concurrently, we achieve an aggregate peak throughput of 1.25 Gbps (at 10 iterations).},
  doi       = {10.1109/GlobalSIP.2013.6737137},
  file      = {:pdf/Wang2013 - High Throughput Low Latency LDPC Decoding on GPU for SDR Systems.pdf:PDF},
  groups    = {LDPC Codes, Software Decoders, HoF LDPC - BP},
  keywords  = {codecs, data communication, decoding, graphics processing units, parity check codes, software radio, GPU architectures, GPU systems, GPU-based LDPC decoder, LDPC decoder, SDR systems, algorithm optimization, asynchronous data transfer, bit rate 1.25 Gbit/s, bit rate 316 Mbit/s, bit rate 62.5 Mbit/s to 304.16 Mbit/s, coalesced memory access, decoding latency, graphics processing units, high throughput low latency LDPC decoding, multistream concurrent kernel execution, optimization techniques, parallel LDPC decoder, software-defined radio, Decoding, Graphics processing units, Kernel, Message systems, Parity check codes, Throughput, WiMAX, GPU, LDPC codes, high throughput, low latency, software-defined radio},
}

@InProceedings{Zhang2012,
  author    = {Suiping Zhang and Rongrong Qian and Tao Peng and Ran Duan and Kuilin Chen},
  title     = {High Throughput Turbo Decoder Design for {GPP} Platform},
  booktitle = {International Conference on Communications and Networking in China (CHINACOM)},
  year      = {2012},
  pages     = {817--821},
  month     = aug,
  publisher = {IEEE},
  abstract  = {The great development of general-purpose processor technology makes the GPP possess the efficiently massive data processing abilities. In the solution(s) of 3GPP LTE system based on GPP platform, Turbo decoding algorithms play a vital role for high decoding complexity. This paper presents the design and implementation of SIMD technique based parallel turbo decoder suitable for GPP architecture. We improve the throughput through a) adopting fix-point design, b) taking full advantage of SIMD technique, c) using multiple soft-input soft-output decoders that operate in parallel, d) introducing multi-core technique to parallelize workload across cores. With 4 threads, the throughput of our SIMD and multi-threading technology based turbo decoder for GPP platform achieves 150 Mbps which can meet the requirement of LTE system.},
  doi       = {10.1109/ChinaCom.2012.6417597},
  file      = {:pdf/Zhang2012 - High Throughput Turbo Decoder Design for GPP Platform.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  keywords  = {3G mobile communication, Long Term Evolution, codecs, multi-threading, parallel processing, turbo codes, 3GPP LTE system, GPP platform, SIMD technique, general-purpose processor technology, massive data processing, multi-threading technology, turbo decoder design, turbo decoding algorithms, Approximation algorithms, Complexity theory, Computer architecture, Decoding, Signal processing, Throughput, Wireless communication, GPP, LTE, SIMD, multi-thread, turbo decoder},
}

@Article{LeGal2015,
  author   = {B. {Le Gal} and C. J\'ego},
  title    = {High-Throughput {LDPC} Decoder on Low-Power Embedded Processors},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2015},
  volume   = {19},
  number   = {11},
  pages    = {1861--1864},
  month    = nov,
  issn     = {1089-7798},
  abstract = {Real-time efficient implementations of LDPC decoders have long been considered exclusively reachable using dedicated hardware architectures. Attempts to implement LDPC decoders on CPU and GPU devices have lead to high power consumptions as well as high processing latencies that are incompatible with most embedded and mobile transmission systems. In this letter, we propose ARM-based decoders that go from 50 to 100 Mbps while executing 10 layered-decoding iterations. We hereby demonstrate that efficient LDPC decoders can be implemented on a low-power programmable architecture. The proposed decoders are competitive with recent GPU related works. Therefore, software LDPC decoders constitute a response to software defined radio constraints.},
  doi      = {10.1109/LCOMM.2015.2477081},
  file     = {:pdf/LeGal2015 - High-Throughput LDPC Decoder on Low-Power Embedded Processors.pdf:PDF},
  groups   = {LDPC Codes, Software Decoders},
  keywords = {decoding, embedded systems, graphics processing units, low-power electronics, microcontrollers, parity check codes, 10-layered-decoding iterations, ARM-based decoders, CPU devices, GPU devices, hardware architectures, high-throughput LDPC decoder, latencies, low-density parity check code, low-power embedded processors, low-power programmable architecture, Decoding, Graphics processing units, Parity check codes, Power demand, Throughput, ARM processor, LDPC, SIMD, multi-core},
}

@Article{LeGal2016,
  author   = {B. {Le Gal} and C. J\'ego},
  title    = {High-Throughput Multi-Core {LDPC} Decoders Based on x86 Processor},
  journal  = {IEEE Transactions on Parallel and Distributed Systems (TPDS)},
  year     = {2016},
  volume   = {27},
  number   = {5},
  pages    = {1373--1386},
  month    = may,
  issn     = {1045-9219},
  abstract = {Low-Density Parity-Check (LDPC) codes are an efficient way to correct transmission errors in digital communication systems. Although initially targeting strictly to ASICs due to computation complexity, LDPC decoders have been recently ported to multicore and many-core systems. Most works focused on taking advantage of GPU devices. In this paper, we propose an alternative solution based on a layered OMS/NMS LDPC decoding algorithm that can be efficiently implemented on a multi-core device using Single Instruction Multiple Data (SIMD) and Single Program Multiple Data (SPMD) programming models. Several experimentations were performed on a x86 processor target. Throughputs up to 170 Mbps were achieved on a single core of an INTEL Core i7 processor when executing 20 layered-based decoding iterations. Throughputs reaches up to 560 Mbps on four INTEL Core-i7 cores. Experimentation results show that the proposed implementations achieved similar BER correction performance than previous works. Moreover, much higher throughputs have been achieved by comparison with all previous GPU and CPU works. They range from x1.4 to x8 by comparison with recent GPU works.},
  doi      = {10.1109/TPDS.2015.2435787},
  file     = {:pdf/LeGal2016 - High-Throughput Multi-Core LDPC Decoders Based on x86 Processor.pdf:PDF},
  groups   = {LDPC Codes, Software Decoders, HoF LDPC - BP},
  keywords = {application specific integrated circuits, decoding, error statistics, microprocessor chips, multiprocessing systems, parity check codes, 20 layered-based decoding iterations, ASIC, BER correction, INTEL Core i7 processor, SIMD, SPMD, computation complexity, digital communication systems, high-throughput multicore LDPC decoders, layered OMS-NMS LDPC decoding, low-density parity-check codes, many-core systems, multicore device, normalized min-sum, offset min-sum, single instruction multiple data, single program multiple data, transmission errors, x86 processor, Decoding, Graphics processing units, Iterative decoding, Processor scheduling, Throughput, GPU, LDPC decoding, SIMD architecture, Software Define Radio (SDR), software decoders, software define radio (SDR), throughput, x86 processor},
}

@InProceedings{Debbabi2016,
  author    = {I. Debbabi and N. Khouja and F. Tlili and B. {Le Gal} and C. J\'ego},
  title     = {Multicore implementation of {LDPC} decoders based on ADMM algorithm},
  booktitle = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2016},
  pages     = {971--975},
  month     = mar,
  publisher = {IEEE},
  abstract  = {Alternate direction method of multipliers (ADMM) technique has recently been proposed for LDPC decoding. Even though it improves the error rate performance compared with traditional message passing (MP) techniques, it shows a higher computation complexity. In this article, the ADMM decoding algorithm is first described. Then, its computation complexity is analyzed. Finally, an optimized version which benefits from the multi-core processors architecture as well as the ADMM algorithm's parallelism is presented. The optimized version of the ADMM decoder can achieve up to 30 Mbps for standardized LDPC codes on a laptop x86 processor. Therefore, it could guide an efficient GPU implementation for real-time and high-throughput decoding systems requiring correction performances beyond MP-Sum Product Algorithm (SPA) capabilities.},
  doi       = {10.1109/ICASSP.2016.7471820},
  file      = {:pdf/Debbabi2016a - Real Time LP Decoding of LDPC Codes for High Correction Performance Applications.pdf:PDF;:pdf/Debbabi2016a - Real Time LP Decoding of {LDPC} Codes for High Correction Performance Applications.pdf:PDF},
  groups    = {LDPC Codes, Software Decoders, HoF LDPC - LP},
  keywords  = {computational complexity, decoding, graphics processing units, multiprocessing systems, parity check codes, ADMM technique, GPU implementation, LDPC decoding, MP-SPA capabilities, MP-sum product algorithm capabilities, alternate direction method of multipliers technique, computation complexity, error rate performance, laptop x86 processor, message passing techniques, multicore processors architecture, standardized LDPC codes, Complexity theory, Decoding, Iterative decoding, Kernel, Signal processing algorithms, Software algorithms, ADMM algorithm, Forward Error Correction codes, LDPC decoding, muti-core, software optimization},
}

@Article{Tal2013,
  author   = {I. Tal and A. Vardy},
  title    = {How to Construct Polar Codes},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  year     = {2013},
  volume   = {59},
  number   = {10},
  pages    = {6562--6582},
  month    = oct,
  issn     = {0018-9448},
  abstract = {A method for efficiently constructing polar codes is presented and analyzed. Although polar codes are explicitly defined, straightforward construction is intractable since the resulting polar bit-channels have an output alphabet that grows exponentially with the code length. Thus, the core problem that needs to be solved is that of faithfully approximating a bit-channel with an intractably large alphabet by another channel having a manageable alphabet size. We devise two approximation methods which {\textquotedblleft}sandwich{\textquotedblright} the original bit-channel between a degraded and an upgraded version thereof. Both approximations can be efficiently computed and turn out to be extremely close in practice. We also provide theoretical analysis of our construction algorithms, proving that for any fixed $\epsilon$ $>$ 0 and all sufficiently large code lengths n, polar codes whose rate is within $\epsilon$ of channel capacity can be constructed in time and space that are both linear in n.},
  doi      = {10.1109/TIT.2013.2272694},
  file     = {:pdf/Tal2013 - How to Construct Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {approximation theory, codes, alphabet size, approximation methods, channel capacity, construction algorithms, polar bit-channels, polar codes, Approximation algorithms, Approximation methods, Complexity theory, Convolutional codes, Decoding, Kernel, Quantization (signal), Channel degrading and upgrading, channel polarization, construction algorithms, polar codes},
}

@InProceedings{Wu2013,
  author    = {M. Wu and G. Wang and B. Yin and C. Studer and J. R. Cavallaro},
  title     = {{HSPA+/LTE-A} Turbo Decoder on {GPU} and Multicore {CPU}},
  booktitle = {Asilomar Conference on Signals, Systems, and Computers (ACSSC)},
  year      = {2013},
  pages     = {824--828},
  month     = nov,
  publisher = {IEEE},
  abstract  = {This paper compares two implementations of reconfigurable and high-throughput turbo decoders. The first implementation is optimized for an NVIDIA Kepler graphics processing unit (GPU), whereas the second implementation is for an Intel Ivy Bridge processor. Both implementations support max-log-MAP and log-MAP turbo decoding algorithms, various code rates, different interleaver types, and all block-lengths, as specified by HSPA; and LTE-Advanced. In order to ensure a fair comparison between both implementations, we perform device-specific optimizations to improve the decoding throughput and error-rate performance. Our results show that the Intel Ivy Bridge processor implementation achieves up to 2$\times$ higher decoding throughput than our GPU implementation. In addition our CPU implementation requires roughly 4$\times$ fewer codewords to be processed in parallel to achieve its peak throughput.},
  doi       = {10.1109/ACSSC.2013.6810402},
  file      = {:pdf/Wu2013 - HSPA+ LTE-A Turbo Decoder on GPU and Multicore CPU.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  keywords  = {Long Term Evolution, graphics processing units, maximum likelihood decoding, multiprocessing systems, turbo codes, GPU, HSPA, Intel Ivy Bridge processor, LTE-Advanced, NVIDIA Kepler graphics processing unit, block-lengths, code rates, decoding throughput, device-specific optimizations, error-rate performance, high-throughput turbo decoders, interleaver types, log-MAP turbo decoding algorithms, max-log-MAP algorithms, multicore CPU, reconfigurable turbo decoders, Approximation methods, Decoding, Graphics processing units, Measurement, Throughput, Turbo codes, Vectors},
}

@InProceedings{Paul2015,
  author    = {H. Paul and D. Wübben and P. Rost},
  title     = {Implementation and Analysis of Forward Error Correction Decoding for Cloud-{RAN} Systems},
  booktitle = {International Conference on Communications Workshop (ICCW)},
  year      = {2015},
  pages     = {2708--2713},
  month     = jun,
  publisher = {IEEE},
  abstract  = {In future 5G mobile networks, radio access network functions will be virtualized and implemented on centralized cloud platforms. In principle, this allows for more advanced algorithms of joint processing and offers the ability to balance the computational load. However, the shift of functionality on a cloud-platform also imposes challenges on the design of the applied algorithms. In this paper, we analyse the implementation of forward error correction decoding algorithms on general purpose hardware, which draws the main computational burden of signal processing in the uplink. Although the protocol stack introduces strict timing requirements we demonstrate by numerical results the feasibility of such implementation.},
  doi       = {10.1109/ICCW.2015.7247588},
  file      = {:pdf/Paul2015 - Implementation and Analysis of Forward Error Correction Decoding for Cloud-RAN Systems.pdf:PDF},
  groups    = {Cloud-RAN},
  issn      = {2164-7038},
  keywords  = {5G mobile communication, cloud computing, decoding, forward error correction, protocols, radio access networks, signal processing, virtualisation, 5G mobile networks, centralized cloud platforms, cloud-RAN systems, computational load, forward error correction decoding algorithms, protocol stack, radio access network functions, signal processing, Decoding, Forward error correction, Hardware, Mobile communication, Radio access networks, Signal processing algorithms, Signal to noise ratio},
}

@InProceedings{Wu2010,
  author    = {M. Wu and Y. Sun and J. R. Cavallaro},
  title     = {Implementation of a {3GPP} {LTE} Turbo Decoder Accelerator on {GPU}},
  booktitle = {International Workshop on Signal Processing Systems (SiPS)},
  year      = {2010},
  pages     = {192--197},
  month     = oct,
  publisher = {IEEE},
  abstract  = {This paper presents a 3GPP LTE compliant turbo decoder accelerator on GPU. The challenge of implementing a turbo decoder is finding an efficient mapping of the decoder algorithm on GPU, e.g. finding a good way to parallelize workload across cores and allocate and use fast on-die memory to improve throughput. In our implementation, we increase throughput through 1) distributing the decoding workload for a codeword across multiple cores, 2) decoding multiple codewords simultaneously to increase concurrency and 3) employing memory optimization techniques to reduce memory bandwidth requirements. In addition, we analyze how different MAP algorithm approximations affect both throughput and bit error rate (BER) performance of this decoder.},
  doi       = {10.1109/SIPS.2010.5624788},
  file      = {:pdf/Wu2010 - Implementation of a 3GPP LTE Turbo Decoder Accelerator on GPU.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  issn      = {2162-3562},
  keywords  = {3G mobile communication, computer graphic equipment, coprocessors, error statistics, maximum likelihood decoding, turbo codes, 3 GPP, GPU, LTE, MAP algorithm, bit error rate, codeword, decoding, memory optimization techniques, turbo decoder accelerator, Bit error rate, Decoding, Graphics processing unit, Instruction sets, Iterative decoding, Kernel, Throughput},
}

@Article{Bang2014,
  author    = {S. Bang and C. Ahn and Y. Jin and S. Choi and J. Glossner and S. Ahn},
  title     = {Implementation of {LTE} System on an {SDR} Platform using {CUDA} and {UHD}},
  journal   = {Springer Journal of Analog Integrated Circuits and Signal Processing (AICSP)},
  year      = {2014},
  volume    = {78},
  number    = {3},
  pages     = {599},
  month     = mar,
  abstract  = {In this paper, we present an implementation of a long term evolution (LTE) system on a software defined radio (SDR) platform using a conventional personal computer that adopts a graphic processing unit (GPU) and a universal software radio peripheral2 (USRP2) with a URSP hardware driver (UHD) to implement an SDR software modem and a radio frequency transceiver, respectively. The central processing unit executes C++ control code that can access the USRP2 via the UHD. We have adopted the Ettus Research UHD due to its high degree of flexibility in the design of the transceiver chain. By taking advantage of this benefit, a simple cognitive radio engine has been implemented using libraries provided by the UHD. We have implemented the software modem on a GPU that is suitable for parallel computing due to its powerful arithmetic and logic units. A parallel programming method is proposed that exploits the single instruction multiple data architecture of the GPU. We focus on the implementation of the Turbo decoder due to its high computational requirements and difficulty in parallelizing the algorithm. The implemented system is analyzed primarily in terms of computation time using the compute unified device architecture profiler. From our experimental tests using the implemented system, we have measured the total processing time for a single frame of both transmit and receive LTE data. We find that it takes 5.00 and 8.58 ms for transmit and receive, respectively. This confirms that the implemented system is capable of real-time processing of all the baseband signal processing algorithms required for LTE systems.},
  date      = {2014-03-01},
  doi       = {10.1007/s10470-013-0229-1},
  file      = {:pdf/Bang2014 - Implementation of LTE System on an SDR Platform using CUDA and UHD.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders},
  publisher = {Springer},
  url       = {http://dx.doi.org/10.1007/s10470-013-0229-1},
}

@Article{Wu2011,
  author    = {M. Wu and Y. Sun and G. Wang and J. R. Cavallaro},
  title     = {Implementation of a High Throughput {3GPP} Turbo Decoder on {GPU}},
  journal   = {Springer Journal of Signal Processing Systems (JSPS)},
  year      = {2011},
  volume    = {65},
  number    = {2},
  pages     = {171},
  month     = sep,
  abstract  = {Turbo code is a computationally intensive channel code that is widely used in current and upcoming wireless standards. General-purpose graphics processor unit (GPGPU) is a programmable commodity processor that achieves high performance computation power by using many simple cores. In this paper, we present a 3GPP LTE compliant Turbo decoder accelerator that takes advantage of the processing power of GPU to offer fast Turbo decoding throughput. Several techniques are used to improve the performance of the decoder. To fully utilize the computational resources on GPU, our decoder can decode multiple codewords simultaneously, divide the workload for a single codeword across multiple cores, and pack multiple codewords to fit the single instruction multiple data (SIMD) instruction width. In addition, we use shared memory judiciously to enable hundreds of concurrent multiple threads while keeping frequently used data local to keep memory access fast. To improve efficiency of the decoder in the high SNR regime, we also present a low complexity early termination scheme based on average extrinsic LLR statistics. Finally, we examine how different workload partitioning choices affect the error correction performance and the decoder throughput.},
  date      = {2011-09-10},
  doi       = {10.1007/s11265-011-0617-7},
  file      = {:pdf/Wu2011 - Implementation of a High Throughput 3GPP Turbo Decoder on GPU.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  publisher = {Springer},
  url       = {http://dx.doi.org/10.1007/s11265-011-0617-7},
}

@Article{Vogt2000,
  author   = {J. Vogt and A. Finger},
  title    = {Improving the max-log-{MAP} Turbo Decoder},
  journal  = {IET Electronics Letters},
  year     = {2000},
  volume   = {36},
  number   = {23},
  pages    = {1937--1939},
  month    = nov,
  issn     = {0013-5194},
  abstract = {Decoding turbo codes with the max-log-MAP algorithm is a good compromise between performance and complexity. The decoding quality of the max-log-MAP decoder is improved by using a scaling factor within the extrinsic calculation. Simulations using the 1MT-2000/3GPP parameters demonstrate that this method gives ~0.2 to 0.4 dB performance gain compared to the standard max-log-MAP algorithm},
  doi      = {10.1049/el:20001357},
  file     = {:pdf/Vogt2000 - Improving the max-log-MAP Turbo Decoder.pdf:PDF},
  groups   = {Turbo Codes},
  keywords = {decoding, turbo codes, 1MT-2000 parameters, 3GPP parameters, decoding quality improvement, max-log-MAP turbo decoder, scaling factor, turbo codes},
}

@InProceedings{Sarkis2014b,
  author    = {G. Sarkis and P. Giard and A. Vardy and C. Thibeault and W. J. Gross},
  title     = {Increasing the Speed of Polar List Decoders},
  booktitle = {International Workshop on Signal Processing Systems (SiPS)},
  year      = {2014},
  pages     = {1--6},
  month     = oct,
  publisher = {IEEE},
  abstract  = {In this work, we present a simplified successive cancellation list decoder that uses a Chase-like decoding process to achieve a six time improvement in speed compared to successive cancellation list decoding while maintaining the same error-correction performance advantage over standard successive-cancellation polar decoders. We discuss the algorithm and detail the data structures and methods used to obtain this speed-up. We also propose an adaptive decoding algorithm that significantly improves the throughput while retaining the error-correction performance. Simulation results over the additive white Gaussian noise channel are provided and show that the proposed system is up to 16 times faster than an LDPC decoder of the same frame size, code rate, and similar error-correction performance, making it more suitable for use as a software decoding solution.},
  doi       = {10.1109/SiPS.2014.6986089},
  file      = {:pdf/Sarkis2014b - Increasing the Speed of Polar List Decoders.pdf:PDF},
  groups    = {Polar Codes, Software Decoders, HoF Polar - SCL},
  issn      = {2162-3562},
  keywords  = {AWGN channels, adaptive decoding, data structures, error correction, parity check codes, software engineering, Chase-like decoding process, LDPC decoder, adaptive decoding algorithm, additive white Gaussian noise channel, cancellation list decoding, data structures, error-correction performance advantage, polar list decoders, simplified successive cancellation list decoder, software decoding solution, standard successive-cancellation polar decoders, Maximum likelihood decoding, Parity check codes, Reliability, Simulation, Software, Throughput},
}

@Article{Tal2015,
  author   = {I. Tal and A. Vardy},
  title    = {List Decoding of Polar Codes},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  year     = {2015},
  volume   = {61},
  number   = {5},
  pages    = {2213--2226},
  month    = may,
  issn     = {0018-9448},
  abstract = {We describe a successive-cancellation list decoder for polar codes, which is a generalization of the classic successive-cancellation decoder of Ar{\i}kan. In the proposed list decoder, L decoding paths are considered concurrently at each decoding stage, where L is an integer parameter. At the end of the decoding process, the most likely among the L paths is selected as the single codeword at the decoder output. Simulations show that the resulting performance is very close to that of maximum-likelihood decoding, even for moderate values of L. Alternatively, if a genie is allowed to pick the transmitted codeword from the list, the results are comparable with the performance of current state-of-the-art LDPC codes. We show that such a genie can be easily implemented using simple CRC precoding. The specific list-decoding algorithm that achieves this performance doubles the number of decoding paths for each information bit, and then uses a pruning procedure to discard all but the L most likely paths. However, straightforward implementation of this algorithm requires {{$\Omega$}}(Ln\textsuperscript{2}) time, which is in stark contrast with the O(n log n) complexity of the original successive-cancellation decoder. In this paper, we utilize the structure of polar codes along with certain algorithmic transformations in order to overcome this problem: we devise an efficient, numerically stable, implementation of the proposed list decoder that takes only O(Ln logn) time and O(Ln) space.},
  doi      = {10.1109/TIT.2015.2410251},
  file     = {:pdf/Tal2015 - List Decoding of Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {cyclic redundancy check codes, maximum likelihood decoding, parity check codes, precoding, CRC precoding, LDPC codes, algorithmic transformations, codeword, decoder output, decoding path, decoding process, decoding stage, information bit, integer parameter, list decoding, maximum-likelihood decoding, polar codes, pruning procedure, specific list-decoding algorithm, stark contrast, successive-cancellation list decoder, Arrays, Bit error rate, Complexity theory, Maximum likelihood decoding, Parity check codes, Vectors, List decoding, polar codes, successive cancellation decoding},
}

@InProceedings{Tal2011,
  author    = {I. Tal and A. Vardy},
  title     = {List Decoding of Polar Codes},
  booktitle = {International Symposium on Information Theory (ISIT)},
  year      = {2011},
  pages     = {1--5},
  month     = jul,
  publisher = {IEEE},
  abstract  = {We describe a successive-cancellation list decoder for polar codes, which is a generalization of the classic successive-cancellation decoder of Arikan. In the proposed list decoder, up to L decoding paths are considered concurrently at each decoding stage. Simulation results show that the resulting performance is very close to that of a maximum-likelihood decoder, even for moderate values of L. Thus it appears that the proposed list decoder bridges the gap between successive-cancellation and maximum-likelihood decoding of polar codes. The specific list-decoding algorithm that achieves this performance doubles the number of decoding paths at each decoding step, and then uses a pruning procedure to discard all but the L {\textquotedblleft}best{\textquotedblright} paths. In order to implement this algorithm, we introduce a natural pruning criterion that can be easily evaluated. Nevertheless, straightforward implementation still requires O(L $\cdot$ n\textsuperscript{2}) time, which is in stark contrast with the O(n log n) complexity of the original successive-cancellation decoder. We utilize the structure of polar codes to overcome this problem. Specifically, we devise an efficient, numerically stable, implementation taking only O(L $\cdot$ n log n) time and O(L $\cdot$ n) space.},
  doi       = {10.1109/ISIT.2011.6033904},
  file      = {:pdf/Tal2011 - List Decoding of Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {2157-8095},
  keywords  = {maximum likelihood decoding, Arikan, decoding paths, maximum likelihood decoder, polar codes, successive-cancellation list decoder, Arrays, Complexity theory, Equations, Error analysis, Mathematical model, Maximum likelihood decoding},
}

@Article{Balatsoukas-Stimming2015,
  author   = {A. Balatsoukas-Stimming and M. B. Parizi and A. Burg},
  title    = {{LLR}-Based Successive Cancellation List Decoding of Polar Codes},
  journal  = {IEEE Transactions on Signal Processing (TSP)},
  year     = {2015},
  volume   = {63},
  number   = {19},
  pages    = {5165--5179},
  month    = oct,
  issn     = {1053-587X},
  abstract = {We show that successive cancellation list decoding can be formulated exclusively using log-likelihood ratios. In addition to numerical stability, the log-likelihood ratio based formulation has useful properties that simplify the sorting step involved in successive cancellation list decoding. We propose a hardware architecture of the successive cancellation list decoder in the log-likelihood ratio domain which, compared with a log-likelihood domain implementation, requires less irregular and smaller memories. This simplification, together with the gains in the metric sorter, lead to 56\% to 137\% higher throughput per unit area than other recently proposed architectures. We then evaluate the empirical performance of the CRC-aided successive cancellation list decoder at different list sizes using different CRCs and conclude that it is important to adapt the CRC length to the list size in order to achieve the best error-rate performance of concatenated polar codes. Finally, we synthesize conventional successive cancellation decoders at large block-lengths with the same block-error probability as our proposed CRC-aided successive cancellation list decoders to demonstrate that, while our decoders have slightly lower throughput and larger area, they have a significantly smaller decoding latency.},
  doi      = {10.1109/TSP.2015.2439211},
  file     = {:pdf/Balatsoukas-Stimming2015a - On Metric Sorting for Successive Cancellation List Decoding of Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {concatenated codes, cyclic redundancy check codes, error statistics, statistical analysis, CRC-aided successive cancellation list decoder, LLR-based successive cancellation list decoding, block-error probability, concatenated polar codes, log-likelihood ratio based formulation, metric sorter, numerical stability, sorting step, Decoding, Hardware, Indexes, Receivers, Signal processing algorithms, Sorting, Throughput, CRC-aided successive cancellation list decoder, hardware implementation, polar codes, successive cancellation decoder, successive cancellation list decoder},
}

@InProceedings{Balatsoukas-Stimming2014,
  author    = {A. Balatsoukas-Stimming and M. Bastani Parizi and A. Burg},
  title     = {{LLR}-Based Successive Cancellation List Decoding of Polar Codes},
  booktitle = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2014},
  pages     = {3903--3907},
  month     = may,
  publisher = {IEEE},
  abstract  = {We present an LLR-based implementation of the successive cancellation list (SCL) decoder. To this end, we associate each decoding path with a metric which (i) is a monotone function of the path's likelihood and (ii) can be computed efficiently from the channel LLRs. The LLR-based formulation leads to a more efficient hardware implementation of the decoder compared to the known log-likelihood based implementation. Synthesis results for an SCL decoder with block-length of N = 1024 and list sizes of L = 2 and L = 4 confirm that the LLR-based decoder has considerable area and operating frequency advantages in the orders of 50\% and 30\%, respectively.},
  doi       = {10.1109/ICASSP.2014.6854333},
  file      = {:pdf/Balatsoukas-Stimming2014 - LLR-Based Successive Cancellation List Decoding of Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {1520-6149},
  keywords  = {channel coding, decoding, LLR-based successive cancellation list decoding, SCL decoder, block-length, channel LLR, log-likelihood based implementation, log-likelihood ratios, monotone function, path likelihood, polar codes, Clocks, Decoding, Hardware, Memory management, Signal processing, Hardware Implementation, Polar Codes, Successive Cancellation List Decoder},
}

@InProceedings{Fan2015,
  author    = {Y. Fan and J. Chen and C. Xia and C. y. Tsui and J. Jin and H. Shen and B. Li},
  title     = {Low-latency List Decoding of Polar Codes with Double Thresholding},
  booktitle = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2015},
  pages     = {1042--1046},
  month     = apr,
  publisher = {IEEE},
  abstract  = {For polar codes with short-to-medium code length, list successive cancellation decoding is used to achieve a good error-correcting performance. However, list pruning in the current list decoding is based on the sorting strategy and its timing complexity is high. This results in a long decoding latency for large list size. In this work, aiming at a low-latency list decoding implementation, a double thresholding algorithm is proposed for a fast list pruning. As a result, with a negligible performance degradation, the list pruning delay is greatly reduced. Based on the double thresholding, a low-latency list decoding architecture is proposed and implemented using a UMC 90nm CMOS technology. Synthesis results show that, even for a large list size of 16, the proposed low-latency architecture achieves a decoding throughput of 220 Mbps at a frequency of 641 MHz.},
  doi       = {10.1109/ICASSP.2015.7178128},
  file      = {:pdf/Fan2015 - Low-latency List Decoding of Polar Codes with Double Thresholding.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {1520-6149},
  keywords  = {CMOS logic circuits, decoding, UMC CMOS technology, double thresholding, error-correcting performance, frequency 641 MHz, list pruning, list successive cancellation decoding, long decoding latency, low-latency list decoding, polar codes, short-to-medium code length, size 90 nm, sorting strategy, timing complexity, Clocks, Computer architecture, Decoding, Delays, Phasor measurement units, Sorting, Polar codes, VLSI implementation, list decoding, low latency, successive cancellation decoding},
}

@Article{Fan2016,
  author   = {Y. Fan and C. Xia and J. Chen and C. Y. Tsui and J. Jin and H. Shen and B. Li},
  title    = {A Low-Latency List Successive-Cancellation Decoding Implementation for Polar Codes},
  journal  = {IEEE Journal on Selected Areas in Communications (JSAC)},
  year     = {2016},
  volume   = {34},
  number   = {2},
  pages    = {303--317},
  month    = feb,
  issn     = {0733-8716},
  abstract = {Due to their provably capacity-achieving performance, polar codes have attracted a lot of research interest recently. For a good error-correcting performance, list successive-cancellation decoding (LSCD) with large list size is used to decode polar codes. However, as the complexity and delay of the list management operation rapidly increase with the list size, the overall latency of LSCD becomes large and limits the applicability of polar codes in high-throughput and latency-sensitive applications. Therefore, in this work, the low-latency implementation for LSCD with large list size is studied. Specifically, at the system level, a selective expansion method is proposed such that some of the reliable bits are not expanded to reduce the computation and latency. At the algorithmic level, a double thresholding scheme is proposed as a fast approximate-sorting method for the list management operation to reduce the LSCD latency for large list size. A VLSI architecture of the LSCD implementing the selective expansion and double thresholding scheme is then developed, and implemented using a UMC 90 nm CMOS technology. Experimental results show that, even for a large list size of 16, the proposed LSCD achieves a decoding throughput of 460 Mbps at a clock frequency of 658 MHz.},
  doi      = {10.1109/JSAC.2015.2504318},
  file     = {:pdf/Fan2016 - A Low-Latency List Successive-Cancellation Decoding Implementation for Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {CMOS integrated circuits, VLSI, computational complexity, decoding, error correction codes, integrated circuit reliability, LSCD VLSI architecture, UMC CMOS technology, error-correcting performance, high-throughput applications, latency-sensitive applications, list management operation complexity, list management operation delay, low-latency list successive-cancellation decoding implementation, polar code reliability, selective expansion method, Complexity theory, Computer architecture, Delays, Maximum likelihood decoding, Reliability, Polar codes, VLSI decoder architectures, double thresholding, list decoding, selective expansion, successive-cancellation decoding},
}

@InProceedings{Li2014a,
  author    = {B. Li and H. Shen and D. Tse and W. Tong},
  title     = {Low-Latency Polar Codes via Hybrid Decoding},
  booktitle = {International Symposium on Turbo Codes and Iterative Information Processing (ISTC)},
  year      = {2014},
  pages     = {223--227},
  month     = aug,
  publisher = {IEEE},
  abstract  = {In this paper, we propose a family of hybrid decoders for polar codes. By decomposing the overall polar code into an inner code and an outer code, a hybrid decoder in the family uses successive cancellation (SC) to decode the inner code and maximum-likelihood (ML) to decode the outer code. At one extreme in the family is the ML decoder, when the entire polar code is viewed as the outer code; at the other extreme is the SC decoder, when the entire polar code is viewed as the inner code. Since ML decoding has lower latency than SC decoding, a hybrid decoder can achieve lower latency than the conventional SC decoder, at the expense of higher complexity due to the ML decoding of the outer code. We propose a reduction in the complexity of ML decoding by exploiting the structure of polar codes.},
  doi       = {10.1109/ISTC.2014.6955118},
  file      = {:pdf/Li2014a - Low-Latency Polar Codes via Hybrid Decoding.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {2165-4700},
  keywords  = {codes, computational complexity, maximum likelihood decoding, ML decoder, SC decoder, hybrid decoding, inner code, low-latency polar codes, maximum-likelihood decoder, outer code, successive cancellation decoder, Complexity theory, Indexes, Information processing, Iterative decoding, Maximum likelihood decoding, Turbo codes},
}

@Article{Giard2016b,
  author    = {P. Giard and G. Sarkis and C. Leroux and C. Thibeault and W. J. Gross},
  title     = {Low-Latency Software Polar Decoders},
  journal   = {Springer Journal of Signal Processing Systems (JSPS)},
  year      = {2016},
  pages     = {1},
  month     = jul,
  abstract  = {Polar codes are a new class of capacity-achieving error-correcting codes with low encoding and decoding complexity. Their low-complexity decoding algorithms rendering them attractive for use in software-defined radio applications where computational resources are limited. In this work, we present low-latency software polar decoders that exploit modern processor capabilities. We show how adapting the algorithm at various levels can lead to significant improvements in latency and throughput, yielding polar decoders that are suitable for high-performance software-defined radio applications on modern desktop processors and embedded-platform processors. These proposed decoders have an order of magnitude lower latency and memory footprint compared to state-of-the-art decoders, while maintaining comparable throughput. In addition, we present strategies and results for implementing polar decoders on graphical processing units. Finally, we show that the energy efficiency of the proposed decoders is comparable to state-of-the-art software polar decoders.},
  date      = {2016-07-11},
  doi       = {10.1007/s11265-016-1157-y},
  file      = {:pdf/Giard2016b - Low-Latency Software Polar Decoders.pdf:PDF},
  groups    = {Polar Codes, Software Decoders, HoF Polar - SC},
  publisher = {Springer},
  url       = {http://dx.doi.org/10.1007/s11265-016-1157-y},
}

@InProceedings{Shen2016,
  author    = {Y. Shen and C. Zhang and J. Yang and S. Zhang and X. You},
  title     = {Low-Latency Software Successive Cancellation List Polar Decoder using Stage-Located Copy},
  booktitle = {International Conference on Digital Signal Processing (DSP)},
  year      = {2016},
  pages     = {84--88},
  month     = oct,
  publisher = {IEEE},
  abstract  = {Successive cancellation list (SCL) decoding for polar codes is promising in data communication. However, in addition to L times complexity of conventional SC, both path selecting and updating result in extra complexity. In detail, the copy of intermediate values suffers from a long latency, especially when list size L is large. In this paper, a stage-located copy algorithm is proposed to avoid copying the same contents in candidate paths, which significantly reduces the processing latency. Furthermore, the resulting data processing speedup increases with code length. For (2048, 1723) polar codes, experimental results have shown that by employing the proposed stage-located copy, throughput of software-based SCL decoder with L = 32 achieves up to 1.1 Mbps throughput with 45\% increase compared to the state-of-the-art software SCL decoders.},
  doi       = {10.1109/ICDSP.2016.7868521},
  file      = {:pdf/Shen2016 - Low-Latency Software Successive Cancellation List Polar Decoder using Stage-Located Copy.pdf:PDF},
  groups    = {Polar Codes, Software Decoders, HoF Polar - SCL},
  keywords  = {channel coding, computational complexity, data communication, software radio, L times complexity, data communication, low-latency software successive cancellation list polar decoder, polar codes, software defined radio, software-based SCL decoder, stage-located copy algorithm, Complexity theory, Decoding, Indexes, Software, Software algorithms, Sorting, Upper bound, Polar codes, reference matrix, stage-located copy, successive cancellation list (SCL) decoder},
}

@Article{Yuan2014,
  author   = {B. Yuan and K. K. Parhi},
  title    = {Low-Latency Successive-Cancellation Polar Decoder Architectures Using 2\mbox{-}Bit Decoding},
  journal  = {IEEE Transactions on Circuits and Systems (TCS)},
  year     = {2014},
  volume   = {61},
  number   = {4},
  pages    = {1241--1254},
  month    = apr,
  issn     = {1549-8328},
  abstract = {Polar codes have emerged as important error correction codes due to their capacity-achieving property. Successive cancellation (SC) algorithm is viewed as a good candidate for hardware design of polar decoders due to its low complexity. However, for (n, k) polar codes, the long latency of SC algorithm of (2n-2) is a bottleneck for designing high-throughput polar decoder. In this paper, we present a novel reformulation for the last stage of SC decoding. The proposed reformulation leads to two benefits. First, critical path and hardware complexity in the last stage of SC algorithm is significantly reduced. Second, 2 bits can be decoded simultaneously instead of 1 bit. As a result, this new decoder, referred to as 2b-SC decoder, reduces latency from (2n-2) to (1.5n-2) without performance loss. Additionally, overlapped-scheduling, precomputation and look-ahead techniques are used to design two additional decoders referred to as 2b-SC-Overlapped-scheduling decoder and 2b-SC-Precomputation decoder, respectively. All three architectures offer significant advantages with respect to throughput and hardware efficiency. Compared to known prior least-latency SC decoder, the 2b-SC-Precomputation decoder has 25\% less latency. Synthesis results show that the proposed (1024, 512) 2b-SC-Precomputation decoder can achieve at least 4 times increase in throughput and 40\% increase in hardware efficiency.},
  doi      = {10.1109/TCSI.2013.2283779},
  file     = {:pdf/Yuan2014 - Low-Latency Successive-Cancellation Polar Decoder Architectures Using 2-Bit Decoding.pdf:PDF},
  groups   = {Polar Codes, Hardware Decoders},
  keywords = {decoding, 2 bit decoding, SC algorithm, hardware complexity, hardware design, hardware efficiency, least-latency SC decoder, low latency successive cancellation polar decoder architectures, successive cancellation, Algorithm design and analysis, Complexity theory, Computer architecture, Decoding, Encoding, Equations, Hardware, 2-bit decoder, Look-ahead, overlapped scheduling, polar codes, precomputation, successive cancellation},
}

@Article{LeGal2015a,
  author   = {B. {Le Gal} and C. Leroux and C. J\'ego},
  title    = {Multi-{G}b/s Software Decoding of Polar Codes},
  journal  = {IEEE Transactions on Signal Processing (TSP)},
  year     = {2015},
  volume   = {63},
  number   = {2},
  pages    = {349--359},
  month    = jan,
  issn     = {1053-587X},
  abstract = {This paper presents an optimized software implementation of a Successive Cancellation (SC) decoder for polar codes. Despite the strong data dependencies in SC decoding, a highly parallel software polar decoder is devised for x86 processor target. A high level of performance is achieved by exploiting the parallelism inherent in today's processor architectures (SIMD, multicore, etc.). Some optimizations that were originally thought for hardware implementation (memory reduction techniques and algorithmic simplifications) were also applied to enhance the throughput of the software implementation. Finally, some low level optimizations such as explicit assembly description or data packing are used to improve the throughput even more. The resulting decoder description is implemented on different x86 processor targets. An analysis of the decoder in terms of latency and throughput is proposed. The influence of several parameters on the throughput and the latency is investigated: the selected target, the code rate, the code length, the SIMD mode (SSE/AVX), the multithreading mode, etc. The energy per decoded bit is also estimated. The proposed software decoder compares favorably with state of the art software polar decoders. Extensive experimentations demonstrate that the proposed software polar decoder exceeds 1 Gb/s for code lengths N $\leq$ 2\textsuperscript{17} on a single core and reaches multi-Gb/s throughputs when using four cores in parallel in AVX mode.},
  doi      = {10.1109/TSP.2014.2371781},
  file     = {:pdf/LeGal2015a - Multi-Gbps Software Decoding of Polar Codes.pdf:PDF},
  groups   = {Polar Codes, Software Decoders, HoF Polar - SC, AFF3CT},
  keywords = {decoding, optimisation, AVX mode, SC decoder, SIMD mode, algorithmic simplifications, code length, code rate, data packing, energy per decoded bit, explicit assembly description, low level optimizations, memory reduction techniques, multiGb/s software decoding, multithreading mode, parallel software polar decoder, polar codes, processor architectures, selected target, software polar decoders, successive cancellation decoder, x86 processor target, Decoding, Optimization, Signal processing algorithms, Software, Systematics, Throughput, Vectors, Polar codes, SIMD, software optimizations, successive cancellation decoding, x86 processor},
}

@Article{Giard2016a,
  author   = {P. Giard and G. Sarkis and C. Thibeault and W. J. Gross},
  title    = {Multi-Mode Unrolled Architectures for Polar Decoders},
  journal  = {IEEE Transactions on Circuits and Systems (TCS)},
  year     = {2016},
  volume   = {63},
  number   = {9},
  pages    = {1443--1453},
  month    = sep,
  issn     = {1549-8328},
  abstract = {In this work, we present a family of architectures for polar decoders using a reduced-complexity successive-cancellation decoding algorithm that employs unrolling to achieve extremely high throughput values while retaining moderate implementation complexity. The resulting fully-unrolled, deeply-pipelined architecture is capable of achieving a coded throughput in excess of 1 Tbps on a 65 nm ASIC at 500 MHz-three orders of magnitude greater than current state-of-the-art polar decoders. However, unrolled decoders are built for a specific, fixed code. Therefore we also present a new method to enable the use of multiple code lengths and rates in a fully-unrolled polar decoder architecture. This method leads to a length- and rate-flexible decoder while retaining the very high speed typical to unrolled decoders. The resulting decoders can decode a master polar code of a given rate and length, and several shorter codes of different rates and lengths. We present results for two versions of a multi-mode decoder supporting eight and ten different polar codes, respectively. Both are capable of a peak throughput of 25.6 Gbps. For each decoder, the energy efficiency for the longest supported polar code is shown to be of 14.8 pJ/bit at 250 MHz and of 8.8 pJ/bit at 500 MHz.},
  doi      = {10.1109/TCSI.2016.2586218},
  file     = {:pdf/Giard2016a - Multi-Mode Unrolled Architectures for Polar Decoders.pdf:PDF},
  groups   = {Polar Codes, Hardware Decoders},
  keywords = {application specific integrated circuits, decoding, energy conservation, low-power electronics, ASIC, code lengths, coded throughput, energy efficiency, frequency 250 MHz, frequency 500 MHz, fully-unrolled polar decoder architecture, length-flexible decoder, master polar code, multimode decoder, polar decoders, rate-flexible decoder, reduced-complexity successive-cancellation decoding algorithm, size 65 nm, unrolled decoders, Computer architecture, Encoding, Hardware, Maximum likelihood decoding, Systematics, Throughput, ASIC, high throughput, multi-mode, polar codes, unrolled architecture},
}

@InProceedings{Berrou1993,
  author    = {C. Berrou and A. Glavieux and P. Thitimajshima},
  title     = {Near {Shannon} Limit Error-Correcting Coding and Decoding: Turbo-Codes},
  booktitle = {International Conference on Communications (ICC)},
  year      = {1993},
  volume    = {2},
  pages     = {1064--1070 vol.2},
  month     = may,
  publisher = {IEEE},
  abstract  = {A new class of convolutional codes called turbo-codes, whose performances in terms of bit error rate (BER) are close to the Shannon limit, is discussed. The turbo-code encoder is built using a parallel concatenation of two recursive systematic convolutional codes, and the associated decoder, using a feedback decoding rule, is implemented as P pipelined identical elementary decoders},
  doi       = {10.1109/ICC.1993.397441},
  file      = {:pdf/Berrou1993 - Near Shannon Limit Error-Correcting Coding and Decoding\: Turbo-Codes.pdf:PDF},
  groups    = {Turbo Codes},
  keywords  = {codecs, concatenated codes, convolutional codes, decoding, error correction codes, error statistics, feedback, pipeline processing, Shannon limit, bit error rate, decoder, encoder, feedback decoding rule, parallel concatenation, pipelined identical elementary decoders, recursive systematic convolutional codes, turbo-codes, Bit error rate, Convolutional codes, Decoding, Digital communication, Digital integrated circuits, Equations, Europe, Feedback, Laboratories, Turbo codes},
}

@InProceedings{Balatsoukas-Stimming2015a,
  author    = {A. Balatsoukas-Stimming and M. Bastani Parizi and A. Burg},
  title     = {On Metric Sorting for Successive Cancellation List Decoding of Polar Codes},
  booktitle = {International Symposium on Circuits and Systems (ISCAS)},
  year      = {2015},
  pages     = {1993--1996},
  month     = may,
  publisher = {IEEE},
  abstract  = {We focus on the metric sorter unit of successive cancellation list decoders for polar codes, which lies on the critical path in all current hardware implementations of the decoder. We review existing metric sorter architectures and we propose two new architectures that exploit the structure of the path metrics in a log-likelihood ratio based formulation of successive cancellation list decoding. Our synthesis results show that, for the list size of L = 32, our first proposed sorter is 14\% faster and 45\% smaller than existing sorters, while for smaller list sizes, our second sorter has a higher delay in return for up to 36\% reduction in the area.},
  doi       = {10.1109/ISCAS.2015.7169066},
  file      = {:pdf/Balatsoukas-Stimming2015a - On Metric Sorting for Successive Cancellation List Decoding of Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {0271-4302},
  keywords  = {channel coding, maximum likelihood decoding, log-likelihood ratio, metric sorter unit architecture, path metrics, polar codes, successive cancellation list decoder, successive cancellation list decoding, Computer architecture, Decoding, Delays, Hardware, Indexes, Sorting},
}

@Article{Bahl1974,
  author   = {L. Bahl and J. Cocke and F. Jelinek and J. Raviv},
  title    = {Optimal Decoding of Linear Codes for Minimizing Symbol Error Rate (Corresp.)},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  year     = {1974},
  volume   = {20},
  number   = {2},
  pages    = {284--287},
  month    = mar,
  issn     = {0018-9448},
  abstract = {The general problem of estimating the a posteriori probabilities of the states and transitions of a Markov source observed through a discrete memoryless channel is considered. The decoding of linear block and convolutional codes to minimize symbol error probability is shown to be a special case of this problem. An optimal decoding algorithm is derived.},
  doi      = {10.1109/TIT.1974.1055186},
  file     = {:pdf/Bahl1974 - Optimal Decoding of Linear Codes for Minimizing Symbol Error Rate (Corresp.).pdf:PDF},
  keywords = {Convolutional codes, Decoding, Estimation, Linear codes, Markov processes, Convolutional codes, Error analysis, Error probability, Feedback, Linear code, Maximum likelihood decoding, Memoryless systems, Reflective binary codes, State estimation, Viterbi algorithm},
}

@Article{Miloslavskaya2015,
  author   = {V. Miloslavskaya},
  title    = {Shortened Polar Codes},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  year     = {2015},
  volume   = {61},
  number   = {9},
  pages    = {4852--4865},
  month    = sep,
  issn     = {0018-9448},
  abstract = {An optimization algorithm for finding a shortening pattern and a set of frozen symbols for polar codes is proposed. The structure of polar codes is exploited to eliminate many equivalent shortening patterns, thus reducing the search space. A reduced-complexity suboptimal algorithm is proposed for finding shortening patterns for long polar codes. Shortened codes obtained with the proposed method are shown to outperform low-density parity-check (LDPC) codes.},
  doi      = {10.1109/TIT.2015.2453312},
  file     = {:pdf/Miloslavskaya2015 - Shortened Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {decoding, optimisation, low-density parity-check codes, optimization algorithm, polar codes, reduced-complexity suboptimal algorithm, sequential decoding, successive cancellation decoding, Arrays, Decoding, Error probability, Heuristic algorithms, Iron, Linear codes, Optimization, Polar codes, sequential decoding, shortening, successive cancellation decoding},
}

@InProceedings{LeGal2014,
  author    = {B. {Le Gal} and C. Leroux and C. J\'ego},
  title     = {Software Polar Decoder on an Embedded Processor},
  booktitle = {International Workshop on Signal Processing Systems (SiPS)},
  year      = {2014},
  pages     = {1--6},
  month     = oct,
  publisher = {IEEE},
  abstract  = {This paper presents the software implementation of a Polar Codes decoder on an embedded processor. An efficient use of computation and memory resource is made in order to devise a fast polar decoder on an embedded ARM processor. Memory footprint reduction and algorithmic simplifications are applied in order to increase the throughput of the decoder. The NEON instruction set of ARM processors is used to exploit the parallelism of the algorithm. The resulting decoder description is implemented on a Cortex A9 ARM processor. The throughput of the resulting decoder is reported and discussed for several parameters : the code rate, the code length and the multithreading mode. To the best of our knowledge, this is the first reported implementation of a polar decoder on an embedded processor core. The proposed software decoder reaches $>$100Mbps for a codelength of 16K. Moreover, it compares favorably with state of the art LDPC decoders implemented on embedded processors.},
  doi       = {10.1109/SiPS.2014.6986083},
  file      = {:pdf/LeGal2014 - Software Polar Decoder on an Embedded Processor.pdf:PDF},
  groups    = {Polar Codes, Software Decoders, HoF Polar - SC},
  issn      = {2162-3562},
  keywords  = {codecs, embedded systems, information theory, instruction sets, parity check codes, Cortex A9 ARM processor, LDPC decoders, NEON instruction set, algorithmic simplifications, code length, code rate, embedded ARM processor, memory footprint reduction, multithreading mode, polar codes decoder, software implementation, software polar decoder, Decoding, Multicore processing, Parallel processing, Parity check codes, Software, Software algorithms, Throughput},
}

@Article{Puschel2005,
  author   = {M. Puschel and J. M. F. Moura and J. R. Johnson and D. Padua and M. M. Veloso and B. W. Singer and Jianxin Xiong and F. Franchetti and A. Gacic and Y. Voronenko and K. Chen and R. W. Johnson and N. Rizzolo},
  title    = {{SPIRAL}: Code Generation for {DSP} Transforms},
  journal  = {Proceedings of the IEEE},
  year     = {2005},
  volume   = {93},
  number   = {2},
  pages    = {232--275},
  month    = feb,
  issn     = {0018-9219},
  abstract = {Fast changing, increasingly complex, and diverse computing platforms pose central problems in scientific computing: How to achieve, with reasonable effort, portable optimal performance? We present SPIRAL, which considers this problem for the performance-critical domain of linear digital signal processing (DSP) transforms. For a specified transform, SPIRAL automatically generates high-performance code that is tuned to the given platform. SPIRAL formulates the tuning as an optimization problem and exploits the domain-specific mathematical structure of transform algorithms to implement a feedback-driven optimizer. Similar to a human expert, for a specified transform, SPIRAL "intelligently" generates and explores algorithmic and implementation choices to find the best match to the computer's microarchitecture. The "intelligence" is provided by search and learning techniques that exploit the structure of the algorithm and implementation space to guide the exploration and optimization. SPIRAL generates high-performance code for a broad set of DSP transforms, including the discrete Fourier transform, other trigonometric transforms, filter transforms, and discrete wavelet transforms. Experimental results show that the code generated by SPIRAL competes with, and sometimes outperforms, the best available human tuned transform library code.},
  doi      = {10.1109/JPROC.2004.840306},
  file     = {:pdf/Puschel2005 - SPIRAL\: Code Generation for DSP Transforms.pdf:PDF},
  keywords = {discrete Fourier transforms, discrete wavelet transforms, mathematics computing, optimising compilers, signal processing, DSP transforms, SPIRAL, code generation, computer microarchitecture, discrete Fourier transform, discrete wavelet transforms, diverse computing platforms, domain specific mathematical structure, feedback driven optimizer, filter transforms, human tuned transform library code, learning methods, linear digital signal processing transforms, optimization, scientific computing, search methods, transform algorithms, trigonometric transforms, Digital signal processing, Discrete Fourier transforms, Discrete wavelet transforms, Fourier transforms, Humans, Microarchitecture, Portable computers, Scientific computing, Signal processing algorithms, Spirals, Adaptation, Markov decision process, automatic performance tuning, code optimization, discrete Fourier transform (DFT), discrete cosine transform (DCT), fast Fourier transform (FFT), filter, genetic and evolutionary algorithm, high-performance computing, learning, library generation, linear signal transform, search, wavelet},
}

@Article{Arikan2011,
  author   = {E. Arikan},
  title    = {Systematic Polar Coding},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2011},
  volume   = {15},
  number   = {8},
  pages    = {860--862},
  month    = aug,
  issn     = {1089-7798},
  abstract = {Polar codes were originally introduced as a class of non-systematic linear block codes. This paper gives encoding and decoding methods for systematic polar coding that preserve the low-complexity nature of non-systematic polar coding while guaranteeing the same frame error rate. Simulation results are given to show that systematic polar coding offers significant advantages in terms of bit error rate performance.},
  doi      = {10.1109/LCOMM.2011.061611.110862},
  file     = {:pdf/Arikan2011 - Systematic Polar Coding.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {binary codes, decoding, error statistics, linear codes, bit error rate performance, decoding methods, encoding methods, frame error rate, nonsystematic linear block codes, nonsystematic polar coding, systematic polar coding, Binary phase shift keying, Bit error rate, Channel coding, Complexity theory, Decoding, Systematics, Polar codes, error propagation, successive cancellation decoding, systematic polar codes},
}

@InProceedings{Zhang2014,
  author    = {Y. Zhang and Z. Xing and L. Yuan and C. Liu and Q. Wang},
  title     = {The Acceleration of Turbo Decoder on the Newest {GPGPU} of Kepler Architecture},
  booktitle = {International Symposium on Communications and Information Technologies (ISCIT)},
  year      = {2014},
  pages     = {199--203},
  month     = sep,
  publisher = {IEEE},
  abstract  = {In the paper, a new implementation of a 3GPP LTE standards compliant turbo decoder based on GPGPU is proposed. It uses the newest GPU-Tesla K20c, which is based on the Kepler GK110 architecture. The new architecture has more powerful parallel computing capability and we use it to fully exploit the parallelism in the turbo decoding algorithm in novel ways. Meanwhile, we use various memory hierarchies to meet various kinds of data demands on speed and capacity. Simulation shows that our implementation is practical and it gets 76\% improvement on throughput over the latest GPU implementation. The result demonstrates that the newest Kepler architecture is suitable for turbo decoding and it can be a promising reconfigurable platform for the communication system.},
  doi       = {10.1109/ISCIT.2014.7011900},
  file      = {:pdf/Zhang2014 - The Acceleration of Turbo Decoder on the Newest GPGPU of Kepler Architecture.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  keywords  = {3G mobile communication, Long Term Evolution, codecs, decoding, graphics processing units, parallel architectures, turbo codes, 3GPP LTE standards, GPGPU, GPU-Tesla K20c, Kepler GK110 architecture, Kepler architecture, general purpose graphic processing units, turbo decoder acceleration, turbo decoding algorithm, Bit error rate, Computer architecture, Decoding, Graphics processing units, Kernel, Parallel processing, Throughput, GPGPU, Kepler, Max-Log-Map Algorithm, Parallel, Turbo Decoder},
}

@Article{Frigo2005,
  author   = {M. Frigo and S. G. Johnson},
  title    = {The Design and Implementation of {FFTW3}},
  journal  = {Proceedings of the IEEE},
  year     = {2005},
  volume   = {93},
  number   = {2},
  pages    = {216--231},
  month    = feb,
  issn     = {0018-9219},
  abstract = {FFTW is an implementation of the discrete Fourier transform (DFT) that adapts to the hardware in order to maximize performance. This paper shows that such an approach can yield an implementation that is competitive with hand-optimized libraries, and describes the software structure that makes our current FFTW3 version flexible and adaptive. We further discuss a new algorithm for real-data DFTs of prime size, a new way of implementing DFTs by means of machine-specific single-instruction, multiple-data (SIMD) instructions, and how a special-purpose compiler can derive optimized implementations of the discrete cosine and sine transforms automatically from a DFT algorithm.},
  doi      = {10.1109/JPROC.2004.840301},
  file     = {:pdf/Frigo2005 - The Design and Implementation of FFTW3.pdf:PDF},
  groups   = {Single Instruction Multiple Data (SIMD)},
  keywords = {discrete Fourier transforms, discrete cosine transforms, mathematics computing, optimising compilers, parallel programming, software libraries, DFT algorithm, FFTW3 design, FFTW3 version, cosine transforms, discrete Fourier transform, hand optimized libraries, machine specific single instruction, multiple data instructions, sine transforms, software structure, Data structures, Discrete Fourier transforms, Discrete cosine transforms, Discrete transforms, Fast Fourier transforms, Fourier transforms, Hardware, Multidimensional systems, Optimizing compilers, Software libraries, Adaptive software, Fourier transform, Hartley transform, I/O tensor, cosine transform, fast Fourier transform (FFT)},
}

@InProceedings{Rodriguez2017,
  author    = {V. Q. Rodriguez and F. Guillemin},
  title     = {Towards the Deployment of a Fully Centralized Cloud-{RAN} Architecture},
  booktitle = {International Wireless Communications and Mobile Computing Conference (IWCMC)},
  year      = {2017},
  pages     = {1055--1060},
  month     = jun,
  publisher = {IEEE},
  abstract  = {In the framework of Network Function Virtualization (NFV), we address in this work the design and sizing of Cloud-RAN architectures. We concretely investigate the execution time of software-based Base Band Units (BBUs) on multi-core systems. Since Cloud-RAN requires a real-time behavior, we use parallel programming techniques in order to minimize the runtime of BBU functions. For an efficient utilization of computing resources, we investigate the relevance of resource pooling where a global scheduling algorithm allocates processing units to runnable BBU-jobs. We specifically examine the gain that can be obtained when applying data parallelism on the channel decoding BBU-function which is the most expensive one in terms of processing time. Performance results show a significant reduction in the runtime of PHY functions which enables the deployment of a fully centralized Cloud-RAN architecture.},
  doi       = {10.1109/IWCMC.2017.7986431},
  file      = {:pdf/Rodriguez2017 - Towards the Deployment of a Fully Centralized Cloud-RAN Architecture.pdf:PDF},
  groups    = {Cloud-RAN},
  keywords  = {channel coding, cloud computing, multiprocessing systems, parallel programming, processor scheduling, radio access networks, telecommunication computing, virtualisation, NFV, PHY functions, channel decoding BBU-function, cloud-RAN architecture, global scheduling algorithm, multicore systems, network function virtualization, parallel programming techniques, software-based base band units, Antennas, Cloud computing, Computer architecture, Decoding, Long Term Evolution, Parallel processing, Runtime, BBU, Cloud-RAN, NFV, VNF, channel decoding, global scheduling, multi-core},
}

@Article{Leonardon2017,
  author      = {M. L\'eonardon and A. Cassagne and C. Leroux and C. J\'ego and L-P. Hamelin and Y. Savaria},
  title       = {Fast and Flexible Software Polar List Decoders},
  journal     = {Springer Journal of Signal Processing Systems (JSPS)},
  abstract    = {Flexibility is one mandatory aspect of channel coding in modern wireless communication systems. Among other things, the channel decoder has to support several code lengths and code rates. This need for flexibility applies to polar codes that are considered for control channels in the future 5G standard. This paper presents a new generic and flexible implementation of a software Successive Cancellation List (SCL) decoder. A large set of parameters can be fine-tuned dynamically without re-compiling the software source code: the code length, the code rate, the frozen bits set, the puncturing patterns, the cyclic redundancy check, the list size, the type of decoding algorithm, the tree-pruning strategy and the data quantization. This generic and flexible SCL decoder enables to explore tradeoffs between throughput, latency and decoding performance. Several optimizations are proposed to achieve a competitive decoding speed despite the constraints induced by the genericity and the flexibility. The resulting polar list decoder is about 4 times faster than a generic software decoder and only 2 times slower than a non-flexible unrolled decoder. Thanks to the flexibility of the decoder, the fully adaptive SCL algorithm can be easily implemented and achieves higher throughput than any other similar decoder in the literature (up to 425 Mb/s on a single processor core for N = 2048 and K = 1723 at 4.5 dB).},
  date        = {2017-10-23},
  eprint      = {1710.08314v1},
  eprintclass = {cs.IT},
  eprinttype  = {arXiv},
  file        = {:pdf/Leonardon2017 - Fast and Flexible Software Polar List Decoders.pdf:PDF;:pdf/Leonardon2017 - Fast and Flexible Software Polar List Decoders [JSPS].pdf:PDF},
  groups      = {Polar Codes, Software Decoders, HoF Polar - SCL, AFF3CT},
  keywords    = {cs.IT, math.IT},
  url         = {https://arxiv.org/abs/1710.08314},
}

@Article{LeGal2017a,
  author   = {B. {Le Gal} and C. Leroux and C. J\'ego},
  title    = {High-Performance Software Implementation of {SCAN} Decoders for Polar codes},
  journal  = {Springer Annals of Telecommunications},
  year     = {2018},
  volume   = {73},
  number   = {5},
  pages    = {401--412},
  month    = {Jun},
  issn     = {1958-9395},
  abstract = {This paper presents the first optimized software implementation of a SCAN decoder for Polar codes. Unlike SC and SC-List decoding algorithms, the SCAN decoding algorithm provides soft outputs (useful for, e.g., parallel concatenated decoders Zhang et al. IEEE Trans Commun 64(2):456--466 2016). Despite the strong data dependencies in the SCAN decoding, two highly parallel software implementations are devised for x86 processor target. Different parallelization strategies, algorithmic improvements, and source code optimizations were applied in order to enhance the throughput of the decoders. The impact of the parallelization approach, the code rate, and the code length on the throughput and the latency is investigated. Extensive experimentations demonstrate that the proposed software polar decoder can exceed 600 Mb/s on a single core and reaches multi-Gb/s when using four cores simultaneously. These decoders can then achieve real-time performance required in many applications such as software defined radio or cloud-RAN systems where network physical layer is implemented in software.},
  day      = {01},
  doi      = {10.1007/s12243-018-0634-7},
  file     = {:pdf/legal2018 - High-Performance Software Implementation of SCAN Decoders for Polar Codes.pdf:PDF},
  groups   = {HoF Polar - SCAN, Software Decoders, Polar Codes},
}

@Article{Falcao2008,
  author   = {G. Falcao and V. Silva and L. Sousa and J. Marinho},
  title    = {High Coded Data Rate and Multicodeword {WiMAX} {LDPC} Decoding on Cell/BE},
  journal  = {IET Electronics Letters},
  year     = {2008},
  volume   = {44},
  number   = {24},
  pages    = {1415--1416},
  month    = nov,
  issn     = {0013-5194},
  abstract = {A novel, flexible and scalable parallel LDPC decoding approach for the WiMAX wireless broadband standard (IEEE 802.16e) in the multicore Cell broadband engine architecture is proposed. A multicodeword LDPC decoder performing the simultaneous decoding of 96 codewords is presented. The coded data rate achieved a range of 72-80-Mbit/s, which compares well with VLSI-based decoders and is superior to the maximum coded data rate required by the WiMAX standard performing in worst case conditions. The 8-bit precision arithmetic adopted shows additional advantages over traditional 6-bit precision dedicated VLSI-based solutions, allowing better error floors and BER performance.},
  doi      = {10.1049/el:20081927},
  file     = {:pdf/Falcao2008 - High Coded Data Rate and Multicodeword WiMAX LDPC Decoding on Cell BE.pdf:PDF},
  groups   = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords = {WiMax, broadband networks, decoding, error statistics, microprocessor chips, parity check codes, 8-bit precision arithmetic, BER performance, WiMAX wireless broadband standard, multicodeword LDPC decoding, multicore Cell broadband engine architecture},
}

@InProceedings{Wang2008,
  author    = {S. Wang and S. Cheng and Q. Wu},
  title     = {A Parallel Decoding Algorithm of {LDPC} Codes Using {CUDA}},
  booktitle = {Asilomar Conference on Signals, Systems, and Computers (ACSSC)},
  year      = {2008},
  pages     = {171--175},
  month     = oct,
  publisher = {IEEE},
  abstract  = {A parallel belief propagation algorithm for decoding low-density parity-check (LDPC) codes is presented in this paper based on Compute Unified Device Architecture (CUDA). As a new hardware and software architecture for addressing and managing computations, CUDA offers parallel data computing using the highly multithreaded coprocessor driven by very high memory bandwidth GPU. The parallel decoding algorithm, based on CUDA, allows that all bit-nodes or check-nodes work simultaneously, thus provides an efficient and fast way for implementing the decoder.},
  doi       = {10.1109/ACSSC.2008.5074385},
  file      = {:pdf/Wang2008 - A Parallel Decoding Algorithm of LDPC Codes Using CUDA.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  issn      = {1058-6393},
  keywords  = {belief networks, coprocessors, decoding, multi-threading, parity check codes, telecommunication computing, CUDA, Compute Unified Device Architecture, LDPC codes, bit-nodes, check-nodes, graphics processor unit, high memory bandwidth GPU, low-density parity-check codes, multithreaded coprocessor, parallel belief propagation algorithm, parallel data computing, parallel decoding algorithm, Bandwidth, Belief propagation, Computer architecture, Concurrent computing, Coprocessors, Decoding, Hardware, Memory management, Parity check codes, Software architecture},
}

@Article{Falcao2009,
  author    = {G. Falcão and S. Yamagiwa and V. Silva and L. Sousa},
  title     = {Parallel {LDPC} Decoding on {GPUs} Using a Stream-Based Computing Approach},
  journal   = {Springer Journal of Computer Science and Technology (JCST)},
  year      = {2009},
  volume    = {24},
  number    = {5},
  pages     = {913},
  month     = sep,
  issn      = {1860-4749},
  abstract  = {Low-Density Parity-Check (LDPC) codes are powerful error correcting codes adopted by recent communication standards. LDPC decoders are based on belief propagation algorithms, which make use of a Tanner graph and very intensive message-passing computation, and usually require hardware-based dedicated solutions. With the exponential increase of the computational power of commodity graphics processing units (GPUs), new opportunities have arisen to develop general purpose processing on GPUs. This paper proposes the use of GPUs for implementing flexible and programmable LDPC decoders. A new stream-based approach is proposed, based on compact data structures to represent the Tanner graph. It is shown that such a challenging application for stream-based computing, because of irregular memory access patterns, memory bandwidth and recursive flow control constraints, can be efficiently implemented on GPUs. The proposal was experimentally evaluated by programming LDPC decoders on GPUs using the Caravela platform, a generic interface tool for managing the kernels' execution regardless of the GPU manufacturer and operating system. Moreover, to relatively assess the obtained results, we have also implemented LDPC decoders on general purpose processors with Streaming Single Instruction Multiple Data (SIMD) Extensions. Experimental results show that the solution proposed here efficiently decodes several codewords simultaneously, reducing the processing time by one order of magnitude.},
  date      = {2009-09-01},
  doi       = {10.1007/s11390-009-9266-8},
  file      = {:pdf/Falcao2009 - Parallel LDPC Decoding on GPUs Using a Stream-Based Computing Approach.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  publisher = {Springer},
  url       = {http://dx.doi.org/10.1007/s11390-009-9266-8},
}

@InProceedings{Zhao2011,
  author    = {J. Zhao and M. Zhao and H. Yang and J. Chen and X. Chen and J. Wang},
  title     = {High Performance {LDPC} Decoder on CELL BE for {WiMAX} System},
  booktitle = {International Conference on Communications and Mobile Computing (CMC)},
  year      = {2011},
  pages     = {278--281},
  month     = apr,
  publisher = {IEEE},
  abstract  = {Low-density parity-check (LDPC) codes are widely used in many radio systems due to its superior performance and Software Defined Radio (SDR) is an emerging paradigm of the wireless communication system design due to its good flexibility and adaptability. However, LDPC decoding is computationally intensive and the implementation of it on software in a parallel way is quite challenging due to the characteristics of LDPC decoding and structures of software. In this paper, we presented an efficient software implementation of the LDPC decoder using the Parallel Code Block Decoding mode for the WiMAX SDR base band system on IBM CELL Broadband Engine (BE). With a single Synergistic Processor Element (SPE) running at 3.2GHz, the implemented LDPC decoder can achieve a throughput up to 1.71 Mbps. With eight SPEs working in parallel, the decoder can obtain the throughput more than 13Mbps, which can meet the WiMAX system requirement at 5MHz bandwidth mode.},
  doi       = {10.1109/CMC.2011.117},
  file      = {:pdf/Zhao2011 - High Performance LDPC Decoder on CELL BE for WiMAX System.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords  = {WiMax, block codes, codecs, microprocessor chips, parity check codes, software radio, CELL BE, IBM CELL, LDPC decoder, WiMAX System, bandwidth 5 MHz, broadband engine, frequency 3.2 GHz, low density parity check codes, parallel code block decoding mode, software defined radio, software implementation, synergistic processor element, Baseband, Computer architecture, Decoding, Microprocessors, Parity check codes, Throughput, WiMAX, CELL BE, LDPC decoder, Software Defined Radio, WiMAX},
}

@Article{Chang2011,
  author   = {C. C. Chang and Y. L. Chang and M. Y. Huang and B. Huang},
  title    = {Accelerating Regular {LDPC} Code Decoders on {GPUs}},
  journal  = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (J-STARS)},
  year     = {2011},
  volume   = {4},
  number   = {3},
  pages    = {653--659},
  month    = sep,
  issn     = {1939-1404},
  abstract = {Modern active and passive satellite and airborne sensors with higher temporal, spectral and spatial resolutions for Earth remote sensing result in a significant increase in data volume. This poses a challenge for data transmission over error-prone wireless links to a ground receiving station. Low-density parity-check (LDPC) codes have been adopted in modern communication systems for robust error correction. Demands for LDPC decoders at a ground receiving station for efficient and flexible data communication links have inspired the usage of a cost-effective high-performance computing device. In this paper we propose a graphic-processing-unit (GPU)-based regular LDPC decoders with the log sum-product iterative decoding algorithm (log-SPA). The GPU code was written to run NVIDIA GPUs using the compute unified device architecture (CUDA) language with a novel implementation of asynchronous data transfer for LDPC decoding. Experimental results showed that the proposed GPU-based high-throughput regular LDPC decoder achieved a significant 271x speedup compared to its CPU-based single-threaded counterpart written in the C language.},
  doi      = {10.1109/JSTARS.2011.2142295},
  file     = {:pdf/Chang2011 - Accelerating Regular LDPC Code Decoders on GPUs.pdf:PDF},
  groups   = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords = {computer graphic equipment, coprocessors, iterative decoding, parallel architectures, parity check codes, remote sensing, satellite communication, LDPC code decoders, NVIDIA GPU, airborne sensors, compute unified device architecture, earth remote sensing, error prone wireless links, flexible data communication links, graphic processing unit, ground receiving station, log sum product iterative decoding algorithm, low density parity check codes, satellite sensors, Computer architecture, Decoding, Encoding, Graphics processing unit, Instruction sets, Iterative decoding, Compute unified device architecture (CUDA), graphic processing unit (GPU), log sum-product algorithm (log-SPA), low-density parity-check (LDPC), regular LDPC, remote sensing, satellite communication links},
}

@Article{Falcao2011,
  author   = {G. Falcao and J. Andrade and V. Silva and L. Sousa},
  title    = {{GPU}-Based {DVB-S}2 {LDPC} Decoder with High Throughput and Fast Error Floor Detection},
  journal  = {IET Electronics Letters},
  year     = {2011},
  volume   = {47},
  number   = {9},
  pages    = {542--543},
  month    = apr,
  issn     = {0013-5194},
  abstract = {A new strategy is proposed for implementing computationally intensive high-throughput decoders based on the long length irregular LDPC codes adopted in the DVB-S2 standard. It is supported on manycore graphics processing unit (GPU) architectures, for performing parallel multi-threaded decoding of multiple codewords with reduced accesses to global memory. This novel approach is flexible and scalable, and achieves throughputs superior to the 90 Mbit/s required by the DVB-S2 standard, while at the same time it improves error-correcting performances such as BER and error floors regarding conventional VLSI-based decoders.},
  doi      = {10.1049/el.2011.0201},
  file     = {:pdf/Falcao2011 - GPU-Based DVB-S2 LDPC Decoder with High Throughput and Fast Error Floor Detection.pdf:PDF;:pdf/Falcao2011a - Massively LDPC Decoding on Multicore Architectures.pdf:PDF},
  groups   = {HoF LDPC - BP, Software Decoders, LDPC Codes, DVB},
  keywords = {decoding, digital video broadcasting, direct broadcasting by satellite, error correction codes, error statistics, parity check codes, GPU-based DVB-S2 LDPC decoder, VLSI-based decoders, bit error rate, error correcting codes, error floor detection, global memory, graphics processing unit, low density parity check codes, parallel multi-threaded decoding},
}

@Article{Falcao2011a,
  author   = {G. Falcao and L. Sousa and V. Silva},
  title    = {Massively {LDPC} Decoding on Multicore Architectures},
  journal  = {IEEE Transactions on Parallel and Distributed Systems (TPDS)},
  year     = {2011},
  volume   = {22},
  number   = {2},
  pages    = {309--322},
  month    = feb,
  issn     = {1045-9219},
  abstract = {Unlike usual VLSI approaches necessary for the computation of intensive Low-Density Parity-Check (LDPC) code decoders, this paper presents flexible software-based LDPC decoders. Algorithms and data structures suitable for parallel computing are proposed in this paper to perform LDPC decoding on multicore architectures. To evaluate the efficiency of the proposed parallel algorithms, LDPC decoders were developed on recent multicores, such as off-the-shelf general-purpose x86 processors, Graphics Processing Units (GPUs), and the CELL Broadband Engine (CELL/B.E.). Challenging restrictions, such as memory access conflicts, latency, coalescence, or unknown behavior of thread and block schedulers, were unraveled and worked out. Experimental results for different code lengths show throughputs in the order of 1 ~ 2 Mbps on the general-purpose multicores, and ranging from 40 Mbps on the GPU to nearly 70 Mbps on the CELL/B.E. The analysis of the obtained results allows to conclude that the CELL/B.E. performs better for short to medium length codes, while the GPU achieves superior throughputs with larger codes. They achieve throughputs that in some cases approach very well those obtained with VLSI decoders. From the analysis of the results, we can predict a throughput increase with the rise of the number of cores.},
  doi      = {10.1109/TPDS.2010.66},
  file     = {:pdf/Falcao2011a - Massively LDPC Decoding on Multicore Architectures.pdf:PDF},
  groups   = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords = {computer graphic equipment, coprocessors, multiprocessing systems, parallel algorithms, Cell broadband engine, graphics processing units, low-density parity-check code, multicore architectures, parallel algorithms, software-based LDPC decoders, x86 processors, Computer architecture, Data structures, Decoding, Graphics, Multicore processing, Parallel algorithms, Parallel processing, Parity check codes, Throughput, Very large scale integration, CELL, CUDA, GPU, LDPC, OpenMP., data-parallel computing, graphics processing units, multicore},
}

@Article{Ji2011,
  author    = {H. Ji and J. Cho and W. Sung},
  title     = {Memory Access Optimized Implementation of Cyclic and Quasi-Cyclic {LDPC} Codes on a {GPGPU}},
  journal   = {Springer Journal of Signal Processing Systems (JSPS)},
  year      = {2011},
  volume    = {64},
  number    = {1},
  pages     = {149},
  month     = jul,
  abstract  = {Software based decoding of low-density parity-check (LDPC) codes frequently takes very long time, thus the general purpose graphics processing units (GPGPUs) that support massively parallel processing can be very useful for speeding up the simulation. In LDPC decoding, the parity-check matrix H needs to be accessed at every node updating process, and the size of the matrix is often larger than that of GPU on-chip memory especially when the code length is long or the weight is high. In this work, the parity-check matrix of cyclic or quasi-cyclic (QC) LDPC codes is greatly compressed by exploiting the periodic property of the matrix. Also, vacant elements are eliminated from the sparse message arrays to utilize the coalesced access of global memory supported by GPGPUs. Regular projective geometry (PG) and irregular QC LDPC codes are used for sum-product algorithm based decoding with the GTX-285 NVIDIA graphics processing unit (GPU), and considerable speed-up results are obtained.},
  date      = {2011-07-01},
  doi       = {10.1007/s11265-010-0547-9},
  file      = {:pdf/Ji2011 - Memory Access Optimized Implementation of Cyclic and Quasi-Cyclic LDPC Codes on a GPGPU.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  publisher = {Springer},
  url       = {http://dx.doi.org/10.1007/s11265-010-0547-9},
}

@InProceedings{Wang2011,
  author    = {G. Wang and M. Wu and Y. Sun and J. R. Cavallaro},
  title     = {A Massively Parallel Implementation of {QC-LDPC} Decoder on {GPU}},
  booktitle = {Symposium on Application Specific Processors (SASP)},
  year      = {2011},
  pages     = {82--85},
  month     = jun,
  publisher = {IEEE},
  abstract  = {The graphics processor unit (GPU) is able to provide a low-cost and flexible software-based multi-core architecture for high performance computing. However, it is still very challenging to efficiently map the real-world applications to GPU and fully utilize the computational power of GPU. As a case study, we present a GPU-based implementation of a real-world digital signal processing (DSP) application: low-density parity-check (LDPC) decoder. The paper shows the efforts we made to map the algorithm onto the massively parallel architecture of GPU and fully utilize GPU's computational resources to significantly boost the performance. Moreover, several efficient data structures have been proposed to reduce the memory access latency and the memory bandwidth requirement. Experimental results show that the proposed GPU-based LDPC decoding accelerator can take advantage of the multi-core computational power provided by GPU and achieve high throughput up to 100.3Mbps.},
  doi       = {10.1109/SASP.2011.5941084},
  file      = {:pdf/Wang2011 - A Massively Parallel Implementation of QC-LDPC Decoder on GPU.pdf:PDF;:pdf/Wang2011a - GPU Accelerated Scalable Parallel Decoding of LDPC Codes.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords  = {computer graphic equipment, coprocessors, data structures, decoding, multiprocessing systems, parity check codes, signal processing, QC-LDPC decoder, data structures, digital signal processing application, graphics processor unit, high performance computing, low density parity check decoder, massively parallel implementation, memory access latency, memory bandwidth requirement, software based multicore architecture, Decoding, Graphics processing unit, IEEE 802.11n Standard, Instruction sets, Message systems, Parity check codes, Throughput, CUDA, GPU, LDPC decoder, parallel computing},
}

@InProceedings{Wang2011a,
  author    = {G. Wang and M. Wu and Y. Sun and J. R. Cavallaro},
  title     = {{GPU} Accelerated Scalable Parallel Decoding of {LDPC} Codes},
  booktitle = {Asilomar Conference on Signals, Systems, and Computers (ACSSC)},
  year      = {2011},
  pages     = {2053--2057},
  month     = nov,
  publisher = {IEEE},
  abstract  = {This paper proposes a flexible low-density parity-check (LDPC) decoder which leverages graphic processor units (GPU) to provide high decoding throughput. LDPC codes are widely adopted by the new emerging standards for wireless communication systems and storage applications due to their near-capacity error correcting performance. To achieve high decoding throughput on GPU, we leverage the parallelism embedded in the check-node computation and variable-node computation and propose a parallel strategy of partitioning the decoding jobs among multi-processors in GPU. In addition, we propose a scalable multi-codeword decoding scheme to fully utilize the computation resources of GPU. Furthermore, we developed a novel adaptive performance-tuning method to make our decoder implementation more flexible and scalable. The experimental results show that our LDPC decoder is scalable and flexible, and the adaptive performance-tuning method can deliver the peak performance based on the GPU architecture.},
  doi       = {10.1109/ACSSC.2011.6190388},
  file      = {:pdf/Wang2011a - GPU Accelerated Scalable Parallel Decoding of LDPC Codes.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  issn      = {1058-6393},
  keywords  = {decoding, error correction codes, graphics processing units, parity check codes, GPU architecture, LDPC codes, accelerated scalable parallel decoding, check-node computation, graphic processor units, low-density parity-check decoder, multiprocessors, near-capacity error correcting performance, scalable multicodeword decoding, variable-node computation, wireless communication, Decoding, Graphics processing unit, Instruction sets, Kernel, Parity check codes, Performance evaluation, Throughput, GPGPU, adaptive performance-tuning, parallel LDPC decoder, reconfigurable and scalable algorithms},
}

@Article{Falcao2012,
  author   = {G. Falcao and V. Silva and L. Sousa and J. Andrade},
  title    = {Portable {LDPC} Decoding on Multicores Using OpenCL},
  journal  = {IEEE Signal Processing Magazine},
  year     = {2012},
  volume   = {29},
  number   = {4},
  pages    = {81--109},
  month    = jul,
  issn     = {1053-5888},
  abstract = {This article proposes to address, in a tutorial style, the benefits of using Open Computing Language [1] (OpenCL) as a quick way to allow programmers to express and exploit parallelism in signal processing algorithms, such as those used in error-correcting code systems. In particular, we will show how multiplatform kernels can be developed straightforwardly using OpenCL to perform computationally intensive low-density parity-check (LDPC) decoding, targeting them to run on a large set of worldwide disseminated multicore architectures, such as x86 general purpose multicore central processing units (CPUs) and graphics processing units (GPUs). Moreover, devices with different architectures can be orchestrated to cooperatively execute these signal processing applications programmed in OpenCL. Experimental evaluation of the parallel kernels programmed with the OpenCL framework shows that high-performance can be achieved for distinct parallel computing architectures with low programming effort.},
  doi      = {10.1109/MSP.2012.2192212},
  file     = {:pdf/Falcao2012 - Portable LDPC Decoding on Multicores Using OpenCL.pdf:PDF},
  groups   = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords = {decoding, parallel architectures, parity check codes, signal processing, CPU, GPU, Open Computing Language, OpenCL framework, error-correcting code systems, graphics processing units, low density parity check decoding, multicore central processing unit, multicores, multiplatform kernels, parallel computing architecture, parallel kernels, portable LDPC decoding, signal processing, Error correction codes, Multicore processing, Parallel processing, Signal processing algorithms, Tutorials},
}

@InProceedings{Kang2012,
  author    = {S. Kang and J. Moon},
  title     = {Parallel {LDPC} Decoder Implementation on {GPU} Based on Unbalanced Memory Coalescing},
  booktitle = {International Conference on Communications (ICC)},
  year      = {2012},
  pages     = {3692--3697},
  month     = jun,
  publisher = {IEEE},
  abstract  = {We consider flexible decoder implementation of low density parity check (LDPC) codes via compute-unified-device-architecture (CUDA) programming on graphics processing unit (GPU), a research subject of considerable recent interest. An important issue in LDPC decoder design based on CUDA-GPU is realizing coalesced memory access, a technique that reduces memory transaction time considerably. In previous works along this direction, it has not been possible to achieve coalesced memory access in both the read and write operations due to the asymmetric nature of the bipartite graph describing the LDPC code structure. In this paper, a new algorithm is proposed that enables coalesced memory access in both the read and write operations for one half of the decoding process - either the bit-to-check or the check-to-bit message passing. For the remaining half of the decoding step our scheme requires address transformation in both the read and write operations but one translating array is sufficient. We also describe the use of on-chip shared memory and texture cache. Overall, experimental results show that proposed GPU-based LDPC decoder achieves more than 234$\times$-speedup compared to CPU-based LDPC decoders and also outperforms existing GPU-based decoders by a significant margin.},
  doi       = {10.1109/ICC.2012.6363991},
  file      = {:pdf/Kang2012 - Parallel LDPC Decoder Implementation on GPU Based on Unbalanced Memory Coalescing.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  issn      = {1550-3607},
  keywords  = {decoding, graph theory, graphics processing units, message passing, parallel architectures, parity check codes, CUDA-GPU decoder, LDPC code structure, bipartite graph, bit-to-check message passing, check-to-bit message passing, coalesced memory access, compute-unified-device-architecture programming, decoding process, graphics processing unit, low density parity check codes, memory transaction time reduction, on-chip shared memory, parallel LDPC decoder design, texture cache, unbalanced memory coalescing, Arrays, Decoding, Error analysis, Graphics processing units, Memory management, Message passing, Parity check codes},
}

@Article{Groenroos2012,
  author    = {S. Gr\"onroos and K. Nybom and J. Bj\"orkqvist},
  title     = {Efficient {GPU} and {CPU}-Based {LDPC} Decoders for Long Codewords},
  journal   = {Springer Journal of Analog Integrated Circuits and Signal Processing (AICSP)},
  year      = {2012},
  volume    = {73},
  number    = {2},
  pages     = {583},
  month     = nov,
  abstract  = {The next generation DVB-T2, DVB-S2, and DVB-C2 standards for digital television broadcasting specify the use of low-density parity-check (LDPC) codes with codeword lengths of up to 64800 bits. The real-time decoding of these codes on general purpose computing hardware is useful for completely software defined receivers, as well as for testing and simulation purposes. Modern graphics processing units (GPUs) are capable of massively parallel computation, and can in some cases, given carefully designed algorithms, outperform general purpose CPUs (central processing units) by an order of magnitude or more. The main problem in decoding LDPC codes on GPU hardware is that LDPC decoding generates irregular memory accesses, which tend to carry heavy performance penalties (in terms of efficiency) on GPUs. Memory accesses can be efficiently parallelized by decoding several codewords in parallel, as well as by using appropriate data structures. In this article we present the algorithms and data structures used to make log-domain decoding of the long LDPC codes specified by the DVB-T2 standard—at the high data rates required for television broadcasting—possible on a modern GPU. Furthermore, we also describe a similar decoder implemented on a general purpose CPU, and show that high performance LDPC decoders are also possible on modern multi-core CPUs.},
  date      = {2012-11-01},
  doi       = {10.1007/s10470-012-9895-7},
  file      = {:pdf/Groenroos2012 - Efficient GPU and CPU-Based LDPC Decoders for Long Codewords.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes, DVB},
  publisher = {Springer},
  url       = {http://dx.doi.org/10.1007/s10470-012-9895-7},
}

@InProceedings{Pan2013,
  author    = {Xia Pan and Xiao-fan Lu and Ming-qi Li and Rong-fang Song},
  title     = {A High Throughput {LDPC} Decoder in {CMMB} Based on Virtual Radio},
  booktitle = {Wireless Communications and Networking Conference Workshops (WCNCW)},
  year      = {2013},
  pages     = {95--99},
  month     = apr,
  publisher = {IEEE},
  abstract  = {LDPC (Low Density Parity Check) is widely used in many telecommunication systems due to its excellent performance. However, real-time LDPC decoder in virtual radio system is difficult to realize. Taking CMMB (China Mobile Multimedia Broadcasting) for instance, this paper proposes a method to achieve high throughput decoder based on x86 processors which support SIMD (Single Instruction Multiple Data) instructions. By utilizing Normalized Min Sum decoding algorithm, normalized parameter as well as bit width of input variables and intermediate variables are determined. Then taking advantages of SIMD instructions, updating progress of variable nodes and check nodes in LDPC decoding algorithm is parallelized. Meanwhile, memory access operations are optimized as well. Tested on Intel Core i7-3960X, the throughput of the LDPC decoder using multithread processing can reach 92Mbps ~ 722Mbps.},
  doi       = {10.1109/WCNCW.2013.6533323},
  file      = {:pdf/Pan2013 - A High Throughput LDPC Decoder in CMMB Based on Virtual Radio.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords  = {decoding, mobile radio, multimedia communication, parity check codes, radio broadcasting, CMMB, China mobile multimedia broadcasting, Intel Core i7-3960X, SIMD instructions, check nodes, high throughput LDPC decoder, high throughput low density parity check decoder, memory access operations, multithread processing, normalized min sum decoding algorithm, real-time LDPC decoder, single instruction multiple data instructions, telecommunication systems, variable nodes, virtual radio, x86 processors, Broadcasting, Decoding, Instruction sets, Iterative decoding, Signal processing algorithms, Throughput, CMMB, LDPC, Multithreading, SIMD, Virtual Radio, fixed-point},
}

@InProceedings{Han2013,
  author    = {X. Han and K. Niu and Z. He},
  title     = {Implementation of {IEEE} 802.11n {LDPC} Codes Based on General Purpose Processors},
  booktitle = {International Conference on Communication Technology (ICCT)},
  year      = {2013},
  pages     = {218--222},
  month     = nov,
  publisher = {IEEE},
  abstract  = {Recently, General-purpose processor (GPP) soft defined radio (SDR) platforms have drawn great attention for their programmability and flexibility, and some high-speed wireless protocol stacks (e.g., IEEE 802.11a/b/g) have been implemented on them using commodity general-purpose PCs. Low-density parity-check (LDPC) codes are optionally used in IEEE 802.11n high throughput (HT) system as a high-performance error correcting code instead of convolutional codes for the near Shannon limit performance. In order to complete the implementation of IEEE 802.11n on SDR platforms, this paper presents the encoding and decoding of IEEE 802.11n LDPC codes on GPPs. We extensively use the features of contemporary processor architectures to accelerate data processing, including large low-latency caches to store lookup tables and SIMD processing on GPPs. Layered decoding is used in this paper, which can significantly reduce the number of iterations and is well suited to using SIMD instructions. The implementation results show that the throughput can meet the protocol timing requirement under the performance premise.},
  doi       = {10.1109/ICCT.2013.6820375},
  file      = {:pdf/Han2013 - Implementation of IEEE 802.11n LDPC Codes Based on General Purpose Processors.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords  = {cache storage, microprocessor chips, parallel processing, parity check codes, software radio, table lookup, wireless LAN, IEEE 802.11n, LDPC codes, SIMD processing, data processing acceleration, general purpose processors, lookup table cache, low density parity check codes, processor architecture, soft defined radio platform, Decoding, IEEE 802.11n Standard, Iterative decoding, Parallel processing, Throughput, Vectors, General-purpose processor (GPP), IEEE 802.11n, Low-density parity-check (LDPC) codes, layered decoding},
}

@InProceedings{Groenroos2013,
  author    = {S. Grönroos and J. Björkqvist},
  title     = {Performance Evaluation of {LDPC} Decoding on a General Purpose Mobile {CPU}},
  booktitle = {Global Conference on Signal and Information Processing (GlobalSIP)},
  year      = {2013},
  pages     = {1278--1281},
  month     = dec,
  abstract  = {This paper explores using a mobile platform for performing the calculations required for the building blocks of telecommunication systems. The building block analyzed in this paper is LDPC (low-density parity-check) channel decoding, performed on the LDPC design used in the DVB-T2 standard. Implementation details are given, and a performance analysis on a mobile CPU is performed. The implementation is compared against a very similar implementation running on a desktop computer CPU, as well as a GPU (graphics processing unit) implementation. The results give indications of the current state of typical mobile platforms of today.},
  doi       = {10.1109/GlobalSIP.2013.6737142},
  file      = {:pdf/Groenroos2013 - Performance Evaluation of LDPC Decoding on a General Purpose Mobile CPU.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes, DVB},
  keywords  = {channel coding, graphics processing units, mobile computing, parity check codes, performance evaluation, LDPC decoding, building block, channel decoding, general purpose mobile CPU, graphics processing unit, low density parity check, mobile CPU, mobile platform, performance evaluation, telecommunication systems, Decoding, Digital video broadcasting, Graphics processing units, Mobile communication, Parity check codes, Standards, Throughput, ARM, LDPC, NEON, SDR},
}

@InProceedings{Li2013,
  author    = {R. Li and J. Zhou and Y. Dou and S. Guo and D. Zou and S. Wang},
  title     = {A Multi-Standard Efficient Column-Layered {LDPC} Decoder for Software Defined Radio on {GPUs}},
  booktitle = {International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)},
  year      = {2013},
  pages     = {724--728},
  month     = jun,
  publisher = {IEEE},
  abstract  = {In this paper, we propose a multi-standard high-throughput column-layered (CL) low-density parity-check (LDPC) decoder for Software-Defined Radio (SDR) on a Graphics Processing Unit (GPU) platform. Multiple columns in the sub-matrix of quasi-cyclic LDPC (QC-LDPC) code are parallel performed inside a block, while multiple codewords are simultaneously decoded among many blocks on the GPU. Several optimization methods are employed to enhance the throughput, such as the compressed matrix structure, memory optimization, codeword packing scheme, two-dimension thread configuration and asynchronous data transfer. The experiment shows that our decoder has low bit error ratio and the peak throughput is 712Mbps, which is about two orders of magnitude faster than that of CPU implementation and comparable to the dedicated hardware solutions. Compared to the existing fastest GPU-based implementation, the presented decoder can achieve a performance improvement of 3.0x times.},
  doi       = {10.1109/SPAWC.2013.6612145},
  file      = {:pdf/Li2013 - A Multi-Standard Efficient Column-Layered LDPC Decoder for Software Defined Radio on GPUs.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  issn      = {1948-3244},
  keywords  = {cyclic codes, error statistics, graphics processing units, optimisation, parity check codes, software radio, GPU, LDPC decoder, QC-LDPC code, SDR, asynchronous data transfer, bit error ratio, codeword packing scheme, column-layered decoder, compressed matrix structure, graphics processing unit, low-density parity-check decoder, memory optimization, multiple codewords, multistandard decoder, quasicyclic LDPC code, software defined radio, two-dimension thread configuration, Decoding, Graphics processing units, Iterative decoding, Message systems, Registers, Throughput, GPU, LDPC Decoder, SDR, column-layered decoding},
}

@Article{Lin2014a,
  author   = {Y. Lin and W. Niu},
  title    = {High Throughput {LDPC} Decoder on {GPU}},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2014},
  volume   = {18},
  number   = {2},
  pages    = {344--347},
  month    = feb,
  issn     = {1089-7798},
  abstract = {The available Lower Density Parity Check (LDPC) decoders on Graphics Processing Unit (GPU) do not simultaneously read and write contiguous data blocks in memory because of the random nature of LDPC codes. One of these two operations has to be performed using noncontiguous accesses, resulting in long access time. To overcome this issue, we designed a multi-codeword parallel decoder with fully coalesced memory access. To test the performance of the method, we applied the method using an 8-bit compact data. The experimental results demonstrated that the method achieved more than 550Mbps throughput on Compute Unified Device Architecture (CUDA) enabled GPU.},
  doi      = {10.1109/LCOMM.2014.010214.132406},
  file     = {:pdf/Lin2014a - High Throughput LDPC Decoder on GPU.pdf:PDF},
  groups   = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords = {decoding, graphics processing units, parity check codes, GPU, compute unified device architecture, contiguous data blocks, graphics processing unit, high throughput LDPC decoder, long access time, lower density parity check decoders, Decoding, Graphics processing units, Message systems, Parity check codes, Performance evaluation, Sparse matrices, Throughput, CUDA, GPU, LDPC code, decoding, parallel processing},
}

@Article{LeGal2014a,
  author   = {B. {Le Gal} and C. J\'ego and J. Crenne},
  title    = {A High Throughput Efficient Approach for Decoding {LDPC} Codes onto {GPU} Devices},
  journal  = {IEEE Embedded Systems Letters (ESL)},
  year     = {2014},
  volume   = {6},
  number   = {2},
  pages    = {29--32},
  month    = jun,
  issn     = {1943-0663},
  abstract = {Low density parity check (LDPC) decoding process is known as compute intensive. This kind of digital communication applications was recently implemented onto graphic processing unit (GPU) devices for LDPC code performance estimation and/or for real-time measurements. Overall previous studies about LDPC decoding on GPU were based on the implementation of the flooding-based decoding algorithm that provides massive computation parallelism. More efficient layered schedules were proposed in literature because decoder iteration can be split into sublayer iterations. These schedules seem to badly fit onto GPU devices due to restricted computation parallelism and complex memory access patterns. However, the layered schedules enable the decoding convergence to speed up by two. In this letter, we show that: 1) layered schedule can be efficiently implemented onto a GPU device; and 2) this approach-implemented onto a low-cost GPU device-provides higher throughputs with identical correction performances (BER) compared to previously published results.},
  doi      = {10.1109/LES.2014.2311317},
  file     = {:pdf/LeGal2014a - A High Throughput Efficient Approach for Decoding LDPC Codes onto GPU Devices.pdf:PDF},
  groups   = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords = {graphics processing units, parity check codes, BER, GPU devices, LDPC code performance estimation, LDPC decoding process, bit error rate, computation parallelism, correction performance, decoder iteration, digital communication applications, flooding-based decoding algorithm, graphics processing unit, high throughput efficient approach, layered schedules, low density parity check codes, memory access patterns, Decoding, Graphics processing units, Iterative decoding, Kernel, Performance evaluation, Throughput, Graphic processing unit (GPU), layered-based algorithm, low density parity check (LDPC), throughput optimized},
}

@Article{Lai2016,
  author   = {B. C. C. Lai and C. Y. Lee and T. H. Chiu and H. K. Kuo and C. K. Chang},
  title    = {Unified Designs for High Performance {LDPC} Decoding on {GPGPU}},
  journal  = {IEEE Transactions on Computers (TC)},
  year     = {2016},
  volume   = {65},
  number   = {12},
  pages    = {3754--3765},
  month    = dec,
  issn     = {0018-9340},
  abstract = {Modern GPGPU's have enabled massively parallel computing with programmability that can exploit the highly parallel nature of LDPC decoding. Previous works customized the design on a GPGPU towards specific execution attributes of a particular LDPC decoding matrix. Supporting different LDPC decoding matrices requires either substantial rework on the current program, or a brand new parallel design. This paper proposes two unified designs that can achieve high performance for both regular and irregular LDPC decoding on a GPGPU. The first design introduces a node-based scheme with a versatile translation array mechanism that can efficiently handle the complex data access patterns of different LDPC decoding matrices. The second design proposes an edge-based parallel paradigm that uses more intuitive data layout. More edges than nodes in a Tanner graph also give the edge-based design higher computation parallelism when there are limited concurrent codewords. With the proposed unified designs, designers can be ignorant of the types of LDPC matrices and achieve high performance LDPC decoding. The experiments on a GTX 470 GPGPU have demonstrated up to 134.56x runtime improvement, when compared with designs on a high-end CPU. The maximum throughput can reach 80.25 Mbps. When compared with the previous customized designs, the proposed systematic designs can reach better performance while relieving the effort of customization.},
  doi      = {10.1109/TC.2016.2547379},
  file     = {:pdf/Lai2016 - Unified Designs for High Performance LDPC Decoding on GPGPU.pdf:PDF},
  groups   = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords = {decoding, graphics processing units, matrix algebra, parity check codes, GTX 470 GPGPU, complex data access patterns, decoding matrices, edge-based design, edge-based parallel paradigm, general purpose graphic processing units, high performance LDPC decoding, irregular decoding, low-density parity-check decoding, node-based scheme, parallel design, regular decoding, versatile translation array mechanism, Computer architecture, Decoding, Iterative decoding, Message passing, Message systems, Parallel processing, C.4 performance of systems, D.2.2 design tools and techniques},
}

@Article{Debbabi2016a,
  author   = {I. Debbabi and {B. Le Gal} and N. Khouja and F. Tlili and C. J\'ego},
  title    = {Real Time LP Decoding of {LDPC} Codes for High Correction Performance Applications},
  journal  = {IEEE Wireless Communications Letters (WCL)},
  year     = {2016},
  volume   = {5},
  number   = {6},
  pages    = {676--679},
  month    = dec,
  issn     = {2162-2337},
  abstract = {The alternate direction method of multipliers is a recent linear programming error correcting approach which improves the decoding performance of LDPC codes compared with the best BP decoding techniques. In this letter, an efficient implementation of the ADMM LP decoding algorithm on a multicore architecture is presented. Its throughput performance level is about one order of magnitude higher than related works on the same multicore target. The proposed decoder's throughput reaches up to 100 Mb/s which makes it viable for real time applications with tough error correction requirements.},
  doi      = {10.1109/LWC.2016.2615304},
  file     = {:pdf/Debbabi2016a - Real Time LP Decoding of LDPC Codes for High Correction Performance Applications.pdf:PDF;},
  groups   = {HoF LDPC - LP, Software Decoders, LDPC Codes},
  keywords = {decoding, error correction codes, linear programming, parity check codes, ADMM LP decoding algorithm, LDPC code, alternate direction method of multiplier, high correction performance application, linear programming error correcting approach, multicore architecture, real time LP decoding, throughput performance level, Complexity theory, Decoding, Iterative decoding, Software algorithms, Throughput, ADMM, LDPC code, LP, SIMD, SIMT, layered scheduling, multicore, software optimization, throughput},
}

@InProceedings{Esterie2012,
  author    = {P. Est\'erie and M. Gaunard and J. Falcou and J. T. Laprest\'e and B. Rozoy},
  title     = {Boost.SIMD: Generic programming for portable SIMDization},
  booktitle = {International Conference on Parallel Architectures and Compilation Techniques (PACT)},
  year      = {2012},
  pages     = {431--432},
  month     = sep,
  publisher = {IEEE},
  abstract  = {SIMD extensions have been a feature of choice for processor manufacturers for a couple of decades. Designed to exploit data parallelism in applications at the instruction level and provide significant accelerations, these extensions still require a high level of expertise or the use of potentially fragile compiler support or vendor-specific libraries. In this poster, we present Boost.SIMD, a C++ template library that simplifies the exploitation of SIMD hardware within a standard C++ programming model.},
  file      = {:pdf/Esterie2012 - Boost.SIMD\: Generic programming for portable SIMDization.pdf:PDF;:pdf/Esterie2012a - Exploiting Multimedia Extensions in C++\: A Portable Approach.pdf:PDF},
  groups    = {C++, Single Instruction Multiple Data (SIMD), Wrapper},
  keywords  = {C++ language, parallel processing, software libraries, Boost.SIMD, C++ programming model, C++ template library, SIMD hardware, data parallelism, portable SIMDization, C++ languages, Computer architecture, Libraries, Parallel processing, Programming, Registers, Standards, C++, Generic Programming, SIMD, Template Metaprogramming},
}

@Article{Esterie2012a,
  author   = {P. Est\'erie and M. Gaunard and J. Falcou and J. T. Laprest\'e},
  title    = {Exploiting Multimedia Extensions in {C++}: A Portable Approach},
  journal  = {IEEE Computing in Science \& Engineering (CS\&E)},
  year     = {2012},
  volume   = {14},
  number   = {5},
  pages    = {72--77},
  month    = sep,
  issn     = {1521-9615},
  abstract = {Single instruction, multiple data (SIMD) extensions have been a feature of choice for processor manufacturers for a couple of decades. Designed to provide significant accelerations, they require expertise, the use of potentially fragile compiler support, or vendor-specific libraries. Here, a C++ template library called Boost.SIMD is presented that simplifies the exploitation of SIMD hardware within a standing C++ programming model.},
  doi      = {10.1109/MCSE.2012.96},
  file     = {:pdf/Esterie2012a - Exploiting Multimedia Extensions in C++\: A Portable Approach.pdf:PDF},
  groups   = {C++, Single Instruction Multiple Data (SIMD), Wrapper},
  keywords = {C++ language, multimedia systems, parallel processing, program compilers, software libraries, Boost, C++ programming model, C++ template library, SIMD extensions, SIMD hardware, compiler support, multimedia extensions, single instruction multiple data extensions, vendor-specific libraries, Computational modeling, Hardware, Instruction sets, Programming, Scientific computing, C\&+\&+, SIMD hardware, computational science, scientific computing, scientific programming},
}

@Article{Inoue2015,
  author     = {H. Inoue and K. Taura},
  title      = {{SIMD-} and Cache-friendly Algorithm for Sorting an Array of Structures},
  journal    = {Proceedings of the VLDB Endowment (PVLDB)},
  year       = {2015},
  volume     = {8},
  number     = {11},
  pages      = {1274--1285},
  month      = jul,
  issn       = {2150-8097},
  acmid      = {2809988},
  doi        = {10.14778/2809974.2809988},
  file       = {:pdf/Inoue2015 - SIMD- and Cache-friendly Algorithm for Sorting an Array of Structures.pdf:PDF},
  groups     = {Single Instruction Multiple Data (SIMD), Sort},
  issue_date = {July 2015},
  numpages   = {12},
  publisher  = {VLDB Endowment},
  url        = {http://dx.doi.org/10.14778/2809974.2809988},
}

@Article{Chhugani2008,
  author     = {J. Chhugani and A. D. Nguyen and V. W. Lee and W. Macy and M. Hagog and Y-K. Chen and A. Baransi and S. Kumar and P. Dubey},
  title      = {Efficient Implementation of Sorting on Multi-core {SIMD} {CPU} Architecture},
  journal    = {Proceedings of the VLDB Endowment (PVLDB)},
  year       = {2008},
  volume     = {1},
  number     = {2},
  pages      = {1313--1324},
  month      = aug,
  issn       = {2150-8097},
  acmid      = {1454171},
  doi        = {10.14778/1454159.1454171},
  file       = {:pdf/Chhugani2008 - Efficient Implementation of Sorting on Multi-core SIMD CPU Architecture.pdf:PDF},
  groups     = {Single Instruction Multiple Data (SIMD), Sort},
  issue_date = {August 2008},
  numpages   = {12},
  publisher  = {VLDB Endowment},
  url        = {http://dx.doi.org/10.14778/1454159.1454171},
}

@Article{Kschischang2001,
  author   = {F. R. Kschischang and B. J. Frey and H. A. Loeliger},
  title    = {Factor Graphs and the Sum-Product Algorithm},
  journal  = {IEEE Transactions on Information Theory},
  year     = {2001},
  volume   = {47},
  number   = {2},
  pages    = {498--519},
  month    = feb,
  issn     = {0018-9448},
  abstract = {Algorithms that must deal with complicated global functions of many variables often exploit the manner in which the given functions factor as a product of {\textquotedblleft}local{\textquotedblright} functions, each of which depends on a subset of the variables. Such a factorization can be visualized with a bipartite graph that we call a factor graph, In this tutorial paper, we present a generic message-passing algorithm, the sum-product algorithm, that operates in a factor graph. Following a single, simple computational rule, the sum-product algorithm computes-either exactly or approximately-various marginal functions derived from the global function. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the sum-product algorithm, including the forward/backward algorithm, the Viterbi algorithm, the iterative {\textquotedblleft}turbo{\textquotedblright} decoding algorithm, Pearl's (1988) belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform (FFT) algorithms},
  doi      = {10.1109/18.910572},
  file     = {:pdf/Kschischang2001 - Factor Graphs and the Sum-Product Algorithm.pdf:PDF},
  groups   = {Factor Graphs},
  keywords = {Kalman filters, Viterbi decoding, artificial intelligence, belief networks, digital communication, fast Fourier transforms, functional analysis, graph theory, hidden Markov models, iterative decoding, message passing, signal processing, turbo codes, Bayesian networks, FFT algorithms, HMM, Kalman filter, Viterbi algorithm, artificial intelligence, belief propagation algorithm, bipartite graph, computational rule, digital communications, factor graphs, factorization, fast Fourier transform, forward/backward algorithm, generic message-passing algorithm, global function, global functions, iterative turbo decoding algorithm, local functions, marginal functions, signal processing, sum-product algorithm, Artificial intelligence, Bipartite graph, Digital communication, Digital signal processing, Iterative algorithms, Iterative decoding, Signal processing algorithms, Sum product algorithm, Visualization, Viterbi algorithm},
}

@Article{Loeliger2004,
  author   = {H. A. Loeliger},
  title    = {An Introduction to Factor Graphs},
  journal  = {IEEE Signal Processing Magazine},
  year     = {2004},
  volume   = {21},
  number   = {1},
  pages    = {28--41},
  month    = jan,
  issn     = {1053-5888},
  abstract = {Graphical models such as factor graphs allow a unified approach to a number of key topics in coding and signal processing such as the iterative decoding of turbo codes, LDPC codes and similar codes, joint decoding, equalization, parameter estimation, hidden-Markov models, Kalman filtering, and recursive least squares. Graphical models can represent complex real-world systems, and such representations help to derive practical detection/estimation algorithms in a wide area of applications. Most known signal processing techniques -including gradient methods, Kalman filtering, and particle methods -can be used as components of such algorithms. Other than most of the previous literature, we have used Forney-style factor graphs, which support hierarchical modeling and are compatible with standard block diagrams.},
  doi      = {10.1109/MSP.2004.1267047},
  file     = {:pdf/Loeliger2004 - An Introduction to Factor Graphs (Colors Edition).pdf:PDF;:pdf/Loeliger2004 - An Introduction to Factor Graphs.pdf:PDF},
  groups   = {Factor Graphs},
  keywords = {Kalman filters, error correction codes, gradient methods, graph theory, hidden Markov models, iterative decoding, least squares approximations, parity check codes, recursive estimation, signal processing, turbo codes, Forney-style factor graphs, Kalman filtering, LDPC codes, error-correcting codes, gradient methods, hidden-Markov models, joint decoding, parameter estimation, particle methods, recursive least squares, signal processing techniques, turbo codes, Filtering, Graphical models, Iterative algorithms, Iterative decoding, Kalman filters, Least squares approximation, Parameter estimation, Parity check codes, Signal processing algorithms, Turbo codes},
}

@Article{Fayyaz2014,
  author   = {U. U. Fayyaz and J. R. Barry},
  title    = {Low-Complexity Soft-Output Decoding of Polar Codes},
  journal  = {IEEE Journal on Selected Areas in Communications (JSAC)},
  year     = {2014},
  volume   = {32},
  number   = {5},
  pages    = {958--966},
  month    = may,
  issn     = {0733-8716},
  abstract = {The state-of-the-art soft-output decoder for polar codes is a message-passing algorithm based on belief propagation, which performs well at the cost of high processing and storage requirements. In this paper, we propose a low-complexity alternative for soft-output decoding of polar codes that offers better performance but with significantly reduced processing and storage requirements. In particular we show that the complexity of the proposed decoder is only 4\% of the total complexity of the belief propagation decoder for a rate one-half polar code of dimension 4096 in the dicode channel, while achieving comparable error-rate performance. Furthermore, we show that the proposed decoder requires about 39\% of the memory required by the belief propagation decoder for a block length of 32768.},
  doi      = {10.1109/JSAC.2014.140515},
  file     = {:pdf/Fayyaz2014 - Low-Complexity Soft-Output Decoding of Polar Codes.pdf:PDF},
  groups   = {Factor Graphs, Polar Codes},
  keywords = {decoding, error statistics, message passing, belief propagation decoder, dicode channel, error rate performance, low-complexity soft-output decoding, message passing algorithm, polar codes, Belief propagation, Complexity theory, Decoding, Iterative decoding, Memory management, Receivers, Polar codes, soft-output decoding, turbo equalization, SCAN},
}

@PhdThesis{Benaddi2015,
  author   = {T. Benaddi},
  title    = {Sparse Graph-Based Coding Schemes for Continuous Phase Modulations},
  school   = {Institut National Polytechnique de Toulouse},
  year     = {2015},
  month    = {December},
  abstract = {The use of the continuous phase modulation (CPM) is interesting when the channel represents a strong non-linearity and in the case of limited spectral support; particularly for the uplink, where the satellite holds an amplifier per carrier, and for downlinks where the terminal equipment works very close to the saturation region. Numerous studies have been conducted on this issue but the proposed solutions use iterative CPM demodulation/decoding concatenated with convolutional or block error correcting codes. The use of LDPC codes has not yet been introduced. Particularly, no works, to our knowledge, have been done on the optimization of sparse graph-based codes adapted for the context described here. In this study, we propose to perform the asymptotic analysis and the design of turbo-CPM systems based on the optimization of sparse graph-based codes. Moreover, an analysis on the corresponding receiver will be done.},
  file     = {:pdf/Benaddi2015 - Sparse Graph-Based Coding Schemes for Continuous Phase Modulations.pdf:PDF},
  groups   = {Turbo Codes, LDPC Codes},
  keywords = {CPM,Code design,Based codes,Turbo detection,Sparse graph,ML/MAP receivers,EXIT chart,Tarik},
  url      = {http://oatao.univ-toulouse.fr/16037/},
}

@Misc{Terboven2014,
  author = {C. Terboven and M. Klemm and E. Stotzer and B. R. {de Supinski}},
  title  = {Advanced OpenMP Tutorial},
  year   = {2014},
  note   = {International Supercomputing Conference},
  file   = {:pdf/Terboven2014 - Advanced OpenMP Tutorial.pdf:PDF},
}

@Article{Kretz2012,
  author    = {M. Kretz and V. Lindenstruth},
  title     = {Vc: A {C++} Library for Explicit Vectorization},
  journal   = {Software: Practice and Experience},
  year      = {2012},
  volume    = {42},
  number    = {11},
  pages     = {1409--1430},
  doi       = {10.1002/spe.1149},
  file      = {:pdf/Kretz2012 - Vc\: A C++ library for explicit vectorization.pdf:PDF},
  groups    = {Single Instruction Multiple Data (SIMD), C++, Wrapper},
  publisher = {Wiley Online Library},
}

@MastersThesis{Kretz2009,
  author = {M. Kretz},
  title  = {Efficient Use of Multi- and Many-Core Systems with Vectorization and Multithreading},
  school = {University of Heidelberg},
  year   = {2009},
  file   = {:pdf/Kretz2009 - Efficient Use of Multi- and Many-Core Systems with Vectorization and Multithreading.pdf:PDF},
  groups = {Single Instruction Multiple Data (SIMD), C++, Wrapper},
}

@PhdThesis{Kretz2015,
  author   = {M. Kretz},
  title    = {Extending {C++} for Explicit Data-Parallel Programming via {SIMD} Vector Types},
  school   = {Goethe University Frankfurt},
  year     = {2015},
  abstract = {Data-parallel programming is more important than ever since serial performance is stagnating. All mainstream computing architectures have been and are still enhancing their support for general purpose computing with explicitly data-parallel execution. For CPUs, data-parallel execution is implemented via SIMD instructions and registers. GPU hardware works very similar allowing very efficient parallel processing of wide data streams with a common instruction stream.
These advances in parallel hardware have not been accompanied by the necessary advances in established programming languages. Developers have thus not been enabled to explicitly state the data-parallelism inherent in their algorithms. Some approaches of GPU and CPU vendors have introduced new programming languages, language extensions, or dialects enabling explicit data-parallel programming. However, it is arguable whether the programming models introduced by these approaches deliver the best solution. In addition, some of these approaches have shortcomings from a hardware-specific focus of the language design. There are several programming problems for which the aforementioned language approaches are not expressive and flexible enough.
This thesis presents a solution tailored to the C++ programming language. The concepts and interfaces are presented specifically for C++ but as abstract as possible facilitating adoption by other programming languages as well. The approach builds upon the observation that C++ is very expressive in terms of types. Types communicate intention and semantics to developers as well as compilers. It allows developers to clearly state their intentions and allows compilers to optimize via explicitly defined semantics of the type system.
Since data-parallelism affects data structures and algorithms, it is not sufficient to enhance the language's expressivity in only one area. The definition of types whose operators express data-parallel execution automatically enhances the possibilities for building data structures. This thesis therefore defines low-level, but fully portable, arithmetic and mask types required to build a flexible and portable abstraction for data-parallel programming. On top of these, it presents higher-level abstractions such as fixed-width vectors and masks, abstractions for interfacing with containers of scalar types, and an approach for automated vectorization of structured types.
The Vc library is an implementation of these types. I developed the Vc library for researching data-parallel types and as a solution for explicitly data-parallel programming. This thesis discusses a few example applications using the Vc library showing the real-world relevance of the library. The Vc types enable parallelization of search algorithms and data structures in a way unique to this solution. It shows the importance of using the type system for expressing data-parallelism. Vc has also become an important building block in the high energy physics community. Their reliance on Vc shows that the library and its interfaces were developed to production quality.},
  file     = {:pdf/Kretz2015 - Extending C++ for Explicit Data-Parallel Programming via SIMD Vector Types.pdf:PDF},
  groups   = {C++, Single Instruction Multiple Data (SIMD), Wrapper},
  keywords = {C++, SIMD, data parallel, parallel programming, vectorization},
}

@Misc{Kanapickas,
  author   = {P. Kanapickas},
  title    = {libsimdpp},
  year     = {2017},
  abstract = {libsimdpp is a portable header-only zero-overhead C++ wrapper around single-instruction multiple-data (SIMD) intrinsics found in many compilers. The library presents a single interface over several instruction sets in such a way that the same source code may be compiled for different instruction sets. The resulting object files then may be hooked into internal dynamic dispatch mechanism.

The library resolves differences between instruction sets by implementing the missing functionality as a combination of several intrinsics. Moreover, the library supplies a lot of additional, commonly used functionality, such as various variants of matrix transpositions, interleaving loads/stores, optimized compile-time shuffling instructions, etc. Each of these are implemented in the most efficient manner for the target instruction set. Finally, it's possible to fall back to native intrinsics when necessary, without compromising maintanability.

The library sits somewhere in the middle between programming directly in intrinsics and even higher-level SIMD libraries. As much control as possible is given to the developer, so that it's possible to exactly predict what code the compiler will generate.},
  groups   = {Single Instruction Multiple Data (SIMD), C++, Wrapper},
  url      = {https://github.com/p12tic/libsimdpp},
}

@Misc{Mabille,
  author   = {J. Mabille},
  title    = {xsimd},
  year     = {2017},
  abstract = {SIMD (Single Instruction, Multiple Data) is a feature of microprocessors that has been available for many years. SIMD instructions perform a single operation on a batch of values at once, and thus provide a way to significantly accelerate code execution. However, these instructions differ between microprocessor vendors and compilers.

xsimd provides a unified means for using these features for library authors. Namely, it enables manipulation of batches of numbers with the same arithmetic operators as for single values. It also provides accelerated implementation of common mathematical functions operating on batches.

You can find out more about this implementation of C++ wrappers for SIMD intrinsics at the The C++ Scientist. The mathematical functions are a lightweight implementation of the algorithms used in boost.SIMD.},
  groups   = {Single Instruction Multiple Data (SIMD), C++, Wrapper},
  url      = {https://github.com/JohanMabille/xsimd},
}

@Misc{Cassagne,
  author   = {A. Cassagne},
  title    = {My Intrinsics++ (MIPP)},
  year     = {2017},
  abstract = {MIPP is a portable and Open-source wrapper (MIT license) for vector intrinsic functions (SIMD) written in C++11. It works for SSE, AVX, AVX512 and ARM NEON instructions. MIPP wrapper supports simple/double precision floating-point numbers and also signed integer arithmetic (32-bit, 16-bit and 8-bit).

With the MIPP wrapper you do not need to write a specific intrinsic code anymore. Just use provided functions and the wrapper will automatically generates the right intrisic calls for your specific architecture.},
  groups   = {Single Instruction Multiple Data (SIMD), Wrapper, C++},
  url      = {https://github.com/aff3ct/MIPP},
}

@Misc{Fog,
  author   = {A. Fog},
  title    = {C++ Vector Class Library ({VCL})},
  year     = {2017},
  abstract = {This is a collection of C++ classes, functions and operators that makes it easier to use the the vector instructions (Single Instruction Multiple Data instructions) of modern CPUs without using assembly language. Supports the SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, AVX, AVX2, AVX512, FMA, and XOP instruction sets. Includes standard mathematical functions. Can compile for different instruction sets from the same source code.},
  file     = {:pdf/Fog - C++ Vector Class library (VCL).pdf:PDF},
  groups   = {Wrapper, C++},
  url      = {http://www.agner.org/optimize/#vectorclass},
}

@Misc{West,
  author   = {N. West},
  title    = {The Vector Optimized Library of Kernels ({VOLK})},
  month    = {Jul},
  year     = {2016},
  abstract = {VOLK is the Vector-Optimized Library of Kernels. It is a free library, currently offered under the GPLv3, that contains kernels of hand-written SIMD code for different mathematical operations. Since each SIMD architecture can be very different and no compiler has yet come along to handle vectorization properly or highly efficiently, VOLK approaches the problem differently.

For each architecture or platform that a developer wishes to vectorize for, a new proto-kernel is added to VOLK. At runtime, VOLK will select the correct proto-kernel. In this way, the users of VOLK call a kernel for performing the operation that is platform/architecture agnostic. This allows us to write portable SIMD code that is optimized for a variety of platforms.},
  groups   = {Kernel},
  url      = {http://libvolk.org/},
}

@Article{Parri2011,
  author     = {J. Parri and D. Shapiro and M. Bolic and V. Groza},
  title      = {Returning Control to the Programmer: {SIMD} Intrinsics for Virtual Machines},
  journal    = {Communications of the ACM},
  year       = {2011},
  volume     = {54},
  number     = {4},
  pages      = {38--43},
  month      = apr,
  issn       = {0001-0782},
  abstract   = {Exposing SIMD units within interpreted languages could simplify programs and unleash floods of untapped processor power.

Server an d workstatio n hardware architecture is continually improving, yet interpreted languages—most importantly, Java—have failed to keep pace with the proper utilization of modern processors. SIMD (single instruction, multiple data) units are available.},
  acmid      = {1924437},
  address    = {New York, NY, USA},
  doi        = {10.1145/1924421.1924437},
  file       = {:pdf/Parri2011 - Returning Control to the Programmer\: SIMD Intrinsics for Virtual Machines.pdf:PDF},
  groups     = {Single Instruction Multiple Data (SIMD)},
  issue_date = {April 2011},
  numpages   = {6},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/1924421.1924437},
}

@PhdThesis{Marchand2010,
  author   = {C. Marchand},
  title    = {Implementation of an {LDPC} Decoder for the {DVB-S2}, {-T2} and {-C2} Standards},
  school   = {University of Bretagne Sud},
  year     = {2010},
  month    = {Nov},
  abstract = {LDPC codes are, like turbo-codes, able to achieve decoding performance close to the Shannon limit. The performance associated with relatively easy implementation makes this solution very attractive to the digital communication systems. This is the case for the Digital video broadcasting by satellite in the DVB-S2 standard that was the first standard including an LDPC.
This thesis subject is about the optimization of the implementation of an LDPC decoder for the DVB-S2, -T2 and -C2 standards. After a state-of-the-art overview, the layered decoder is chosen as the basis architecture for the decoder implementation. We had to deal with the memory conflicts due to the matrix structure specific to the DVB-S2, -T2, -C2 standards. Two new contributions have been studied to solve the problem. The first is based on the construction of an equivalent matrix and the other relies on the repetition of layers. The conflicts inherent to the pipelined architecture are solved by an efficient scheduling found with the help of graph theories.
Memory size is a major point in term of area and consumption, therefore the reduction to a minimum of this memory is studied. A well defined saturation and an optimum partitioning of memory bank lead to a significant reduction compared to the state-of-the-art. Moreover, the use of single port RAM instead of dual port RAM is studied to reduce memory cost.
In the last chapter we answer to the need of a decoder able to decode in parallel x streams with a reduced cost compared to the use of x decoders.},
  file     = {:pdf/Marchand2010 - Implementation of an LDPC Decoder for the DVB-S2, -T2 and -C2 Standards.pdf:PDF},
  groups   = {LDPC Codes, DVB},
  keywords = {LDPC codes, implementation, memory conflicts, DVB-S2},
  url      = {https://hal.archives-ouvertes.fr/tel-01151985},
}

@Article{Sarkis2016a,
  author   = {G. Sarkis and I. Tal and P. Giard and A. Vardy and C. Thibeault and W. J. Gross},
  title    = {Flexible and Low-Complexity Encoding and Decoding of Systematic Polar Codes},
  journal  = {IEEE Transactions on Communications (TCOM)},
  year     = {2016},
  volume   = {64},
  number   = {7},
  pages    = {2732--2745},
  month    = jul,
  issn     = {0090-6778},
  abstract = {In this paper, we present hardware and software implementations of flexible polar systematic encoders and decoders. The proposed implementations operate on polar codes of any length less than a maximum and of any rate. We describe the low-complexity, highly parallel, and flexible systematic-encoding algorithm that we use and prove its correctness. Our hardware implementation results show that the overhead of adding code rate and length flexibility is little, and the impact on operation latency minor compared with code-specific versions. Finally, the flexible software encoder and decoder implementations are also shown to be able to maintain high throughput and low latency.},
  doi      = {10.1109/TCOMM.2016.2574996},
  file     = {:pdf/Sarkis2016a - Flexible and Low-Complexity Encoding and Decoding of Systematic Polar Codes.pdf:PDF},
  groups   = {Polar Codes, Software Decoders, Hardware Decoders},
  keywords = {codes, decoding, code rate, decoding, length flexibility, low-complexity encoding, software decoder, software encoder, systematic encoding algorithm, systematic polar code, Decoding, Encoding, Hardware, Software, Software algorithms, Systematics, Throughput, Polar codes, multi-code decoders, multi-code encoders, systematic encoding},
}

@InProceedings{Pohl2016,
  author    = {A. Pohl and B. Cosenza and M. A. Mesa and C. C. Chi and B. Juurlink},
  title     = {An Evaluation of Current SIMD Programming Models for {C++}},
  booktitle = {Workshop on Programming Models for SIMD/Vector Processing (WPMVP)},
  year      = {2016},
  pages     = {3:1--3:8},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {SIMD extensions were added to microprocessors in the mid '90s to speed-up data-parallel code by vectorization. Unfortunately, the SIMD programming model has barely evolved and the most efficient utilization is still obtained with elaborate intrinsics coding. As a consequence, several approaches to write efficient and portable SIMD code have been proposed. In this work, we evaluate current programming models for the C++ language, which claim to simplify SIMD programming while maintaining high performance.

The proposals were assessed by implementing two kernels: one standard floating-point benchmark and one real-world integer-based application, both highly data parallel. Results show that the proposed solutions perform well for the floating point kernel, achieving close to the maximum possible speed-up. For the real-world application, the programming models exhibit significant performance gaps due to data type issues, missing template support and other problems discussed in this paper.},
  acmid     = {2870653},
  articleno = {3},
  doi       = {10.1145/2870650.2870653},
  file      = {:pdf/Pohl2016 - An Evaluation of Current SIMD Programming Models for C++.pdf:PDF},
  groups    = {Wrapper},
  isbn      = {978-1-4503-4060-1},
  keywords  = {C++, SIMD, parallel programming, programming model, vectorization},
  location  = {Barcelona, Spain},
  numpages  = {8},
  url       = {http://doi.acm.org/10.1145/2870650.2870653},
}

@Misc{OpenMP2013,
  author = {{OpenMP Architecture Review Board}},
  title  = {OpenMP Application Program Interface},
  year   = {2013},
  file   = {:pdf/OpenMP2013 - OpenMP Application Program Interface.pdf:PDF},
  groups = {Single Instruction Multiple Data (SIMD)},
  url    = {http://www.openmp.org/mp-documents/OpenMP4.0.0.pdf},
}

@InProceedings{Pharr2012,
  author    = {M. Pharr and W. R. Mark},
  title     = {ispc: A SPMD Compiler for High-Performance {CPU} Programming},
  booktitle = {Innovative Parallel Computing (InPar)},
  year      = {2012},
  pages     = {1--13},
  month     = may,
  publisher = {IEEE},
  abstract  = {SIMD parallelism has become an increasingly important mechanism for delivering performance in modern CPUs, due its power efficiency and relatively low cost in die area compared to other forms of parallelism. Unfortunately, languages and compilers for CPUs have not kept up with the hardware's capabilities. Existing CPU parallel programming models focus primarily on multi-core parallelism, neglecting the substantial computational capabilities that are available in CPU SIMD vector units. GPU-oriented languages like OpenCL support SIMD but lack capabilities needed to achieve maximum efficiency on CPUs and suffer from GPU-driven constraints that impair ease of use on CPUs. We have developed a compiler, the Intel R{\textregistered} SPMD Program Compiler (ispc), that delivers very high performance on CPUs thanks to effective use of both multiple processor cores and SIMD vector units. ispc draws from GPU programming languages, which have shown that for many applications the easiest way to program SIMD units is to use a single-program, multiple-data (SPMD) model, with each instance of the program mapped to one SIMD lane. We discuss language features that make ispc easy to adopt and use productively with existing software systems and show that ispc delivers up to 35x speedups on a 4-core system and up to 240$\times$ speedups on a 40-core system for complex workloads (compared to serial C++ code).},
  doi       = {10.1109/InPar.2012.6339601},
  file      = {:pdf/Pharr2012 - ispc\: A SPMD Compiler for High-Performance CPU Programming.pdf:PDF},
  groups    = {Single Instruction Multiple Data (SIMD)},
  keywords  = {graphics processing units, multiprocessing systems, parallel programming, program compilers, CPU SIMD vector units, GPU programming languages, GPU-oriented languages, ISPC, Intel R SPMD program compiler, OpenCL, SIMD parallelism, high-performance CPU parallel programming, multicore parallelism, multiple processor cores, single-program multiple-data model, Graphics processing unit, Hardware, Parallel processing, Productivity, Programming, Vectors},
}

@Article{Robison2013,
  author   = {A. D. Robison},
  title    = {Composable Parallel Patterns with Intel Cilk Plus},
  journal  = {IEEE Computing in Science \& Engineering (CS\&E)},
  year     = {2013},
  volume   = {15},
  number   = {2},
  pages    = {66--71},
  month    = mar,
  issn     = {1521-9615},
  abstract = {Intel Cilk Plus extends C and C++ to enable writing composable deterministic parallel software that can exploit both the thread and vector parallelism commonly available in modern hardware.},
  doi      = {10.1109/MCSE.2013.21},
  file     = {:pdf/Robison2013 - Composable Parallel Patterns with Intel Cilk Plus.pdf:PDF},
  groups   = {Single Instruction Multiple Data (SIMD)},
  keywords = {C++ language, multi-threading, C++, Intel Cilk Plus, composable deterministic parallel software, composable parallel pattern, parallel thread, parallel vector, Message systems, Parallel processing, Program processors, Programming, Scientific computing, Vectors, Intel Cilk Plus, OpenMP, parallel programming, scientific programming, thread parallelism, vector parallelism},
}

@Misc{Howes2015,
  author    = {L. Howes},
  title     = {The OpenCL Specification},
  year      = {2015},
  note      = {Version 2.1, Revision 23},
  file      = {:pdf/Howes2015 - The OpenCL Specification.pdf:PDF},
  groups    = {Single Instruction Multiple Data (SIMD)},
  publisher = {Khronos Group},
  url       = {https://www.khronos.org/registry/OpenCL/specs/opencl-2.1.pdf},
}

@TechReport{Moller2016,
  author      = {R. M\"oller},
  title       = {Design of a Low-Level {C++} Template {SIMD} Library},
  institution = {Bielefeld University, Faculty of Technology, Computer Engineering Group},
  year        = {2016},
  abstract    = {T-SIMD is a low-level C++ template SIMD library which wraps built-in vector data types and built-in vector intrincics in template classes and template functions or overloaded functions, respectively. Templates parameters are the element data type of the vectors and the vector width in bytes (e.g. 16 for SSE* and NEON, 32 for AVX/AVX2). This makes it possible to flexibly change the data type and the vector instruction set for entire portions of the code. SSE*, AVX/AVX2, and ARM NEON vector instruction sets are currently supported.},
  file        = {:pdf/Moller2016 - Design of a Low-Level C++ Template SIMD Library.pdf:PDF},
  groups      = {Wrapper},
  url         = {http://www.ti.uni-bielefeld.de/html/people/moeller/tsimd_warpingsimd.html},
}

@Misc{IntelIntrinsicsGuide,
  author   = {{Intel Corp}},
  title    = {Intel Intrinsics Guide},
  year     = {2017},
  abstract = {The Intel Intrinsics Guide is an interactive reference tool for Intel intrinsic instructions, which are C style functions that provide access to many Intel instructions - including Intel® SSE, AVX, AVX-512, and more - without the need to write assembly code.},
  groups   = {Single Instruction Multiple Data (SIMD)},
  url      = {https://software.intel.com/sites/landingpage/IntrinsicsGuide/},
}

@Misc{NeonIntrinsicsGuide,
  author   = {{ARM Ltd}},
  title    = {{NEON} Intrinsics Reference},
  year     = {2017},
  abstract = {The NEON Intrinsics Reference is an interactive reference tool for NEON intrinsic instructions, which are C style functions that provide access to many NEON instructions - including NEON, NEONv2, NEON 64-bit, NEONv2 64-bit, and more - without the need to write assembly code.},
  groups   = {Single Instruction Multiple Data (SIMD)},
  url      = {https://developer.arm.com/technologies/neon/intrinsics},
}

@Misc{AltivecManual,
  author   = {{Freescale Semiconductor}},
  title    = {{AltiVec} Technology Programming Interface Manual},
  year     = {1999},
  abstract = {The primary objective of this manual is to help programmers to provide software that is
compatible across the family of PowerPC processors using AltiVec technology.
To locate any published errata or updates for this document, refer to the website at
http://www.mot.com/SPS/PowerPC/.
This book is one of two that discuss the AltiVec architecture, the two books are:
- AltiVec: The Programming Interface Manual (AltiVec PIM) is used as a reference
guide for high-level programmers. The AltiVec PIM provides a mechanism for
programmers to access AltiVec functionality from programming languages such as
C and C++. The AltiVec PIM defines a programming model for use with the AltiVec
instruction set extension to the PowerPC architecture.
- AltiVec: The Programming Environments Manual (AltiVec PEM) is used as a
reference guide for assembler programmers (or intrinsics). The AltiVec PEM provides a
description for each instruction that includes the instruction format, an
individualized legend that provides such information as the level(s) of the PowerPC
architecture in which the instruction may be found, the privilege level of the
instruction, and figures to help in understanding how the instruction works.
It is beyond the scope of this manual to describe individual AltiVec technology
implementations on PowerPC processors. It must be kept in mind that each PowerPC
processor is unique in its implementation of the AltiVec technology.
The information in this book is subject to change without notice, as described in the
disclaimers on the title page of this book. As with any technical documentation, it is the
readers’ responsibility to be sure they are using the most recent version of the
documentation. For more information, contact your sales representative or visit our website
at: http://www.mot.com/SPS/PowerPC/. },
  groups   = {Single Instruction Multiple Data (SIMD)},
  url      = {https://www.nxp.com/docs/en/reference-manual/ALTIVECPIM.pdf},
}

@Article{Stephens2017,
  author   = {N. Stephens and S. Biles and M. Boettcher and J. Eapen and M. Eyole and G. Gabrielli and M. Horsnell and G. Magklis and A. Martinez and N. Premillieu and A. Reid and A. Rico and P. Walker},
  title    = {The {ARM} Scalable Vector Extension},
  journal  = {IEEE Micro},
  year     = {2017},
  volume   = {37},
  number   = {2},
  pages    = {26--39},
  month    = mar,
  issn     = {0272-1732},
  abstract = {This article describes the ARM Scalable Vector Extension (SVE). Several goals guided the design of the architecture. First was the need to extend the vector processing capability associated with the ARM AArch64 execution state to better address the computational requirements in domains such as high-performance computing, data analytics, computer vision, and machine learning. Second was the desire to introduce an extension that can scale across multiple implementations, both now and into the future, allowing CPU designers to choose the vector length most suitable for their power, performance, and area targets. Finally, the architecture should avoid imposing a software development cost as the vector length changes and where possible reduce it by improving the reach of compiler auto-vectorization technologies. SVE achieves these goals. It allows implementations to choose a vector register length between 128 and 2,048 bits. It supports a vector-length agnostic programming model that lets code run and scale automatically across all vector lengths without recompilation. Finally, it introduces several innovative features that begin to overcome some of the traditional barriers to autovectorization.},
  comment  = {Ce papier parle de la nouvelle extension SIMD pou ARM (SVE).

Il est important de noter que cette extension adresse des registres SIMD de 128 bits à 2048 bits.
Le code assembleur est le même quel que soit la taille des registres (il ne contient pas d'information sur la taille des instructions SIMD).

Comme pour AVX-512, il y a des registres de masquage (nommés "predicate").
C'est un peu plus puissant que AVX-512 dans le masquages car il y a des instructions "while" spécifiques qui utilisent des registres predicates (il y a aussi plus de registres predicates en SVE qu'en AVX-512: 16 contre 8).
Ca permet du coup d'améliorer l'auto-vectorisation des codes.
Il y a aussi des mécanismes pour s'arreter si on déborde en mémoire, c'est nottament utile pour vectoriser des codes comme "strlen" où l'on ne connait pas la fin du tableau.

Les performances sont pour le moment assez moyenne mais le taux de vectorisation (auto) du code est très bon par rapport à NEON et c'est prometteur pour la suite.},
  doi      = {10.1109/MM.2017.35},
  file     = {:pdf/Stephens2017 - The ARM Scalable Vector Extension (bis).pdf:PDF},
  groups   = {Error-Correcting Codes (ECC), Single Instruction Multiple Data (SIMD)},
  keywords = {program compilers, programming, vectors, ARM AArch64 execution state, ARM scalable vector extension, CPU designers, SVE, area targets, compiler autovectorization technologies, computational requirements, computer vision, data analytics, high-performance computing, machine learning, performance targets, power targets, vector length, vector processing capability, vector-length agnostic programming model, Computer architecture, Encoding, High performance computing, Programming, Scalability, Software engineering, Vectors, ARM, HPC, SIMD, SVE, Scalable Vector Extension, VLA, Vector Length Agnostic, autovectorization, data parallelism, high-performance computing, instruction set architecture, predication, scalable vector architecture, vector length agnostic},
}

@Article{Chen2005,
  author   = {J. Chen and A. Dholakia and E. Eleftheriou and M. P. C. Fossorier and X.-Y. Hu},
  title    = {Reduced-Complexity Decoding of {LDPC} Codes},
  journal  = {IEEE Transactions on Communications (TCOM)},
  year     = {2005},
  volume   = {53},
  number   = {8},
  pages    = {1288--1299},
  month    = aug,
  issn     = {0090-6778},
  abstract = {Various log-likelihood-ratio-based belief-propagation (LLR-BP) decoding algorithms and their reduced-complexity derivatives for low-density parity-check (LDPC) codes are presented. Numerically accurate representations of the check-node update computation used in LLR-BP decoding are described. Furthermore, approximate representations of the decoding computations are shown to achieve a reduction in complexity by simplifying the check-node update, or symbol-node update, or both. In particular, two main approaches for simplified check-node updates are presented that are based on the so-called min-sum approximation coupled with either a normalization term or an additive offset term. Density evolution is used to analyze the performance of these decoding algorithms, to determine the optimum values of the key parameters, and to evaluate finite quantization effects. Simulation results show that these reduced-complexity decoding algorithms for LDPC codes achieve a performance very close to that of the BP algorithm. The unified treatment of decoding techniques for LDPC codes presented here provides flexibility in selecting the appropriate scheme from performance, latency, computational-complexity, and memory-requirement perspectives.},
  doi      = {10.1109/TCOMM.2005.852852},
  file     = {:pdf/Chen2005 - Reduced-Complexity Decoding of LDPC Codes.pdf:PDF},
  groups   = {LDPC Codes},
  keywords = {approximation theory, computational complexity, iterative decoding, maximum likelihood decoding, parity check codes, quantisation (signal), LDPC codes, LLR-BP decoding, check-node update, computational-complexity, finite quantization effects, log-likelihood-ratio-based belief-propagation decoding, low-density parity-check codes, memory-requirement perspective, min-sum approximation, reduced-complexity decoding, symbol-node update, Algorithm design and analysis, Approximation algorithms, Computational modeling, Delay, Iterative algorithms, Iterative decoding, Parity check codes, Performance analysis, Quantization, Sum product algorithm, Belief-propagation (BP) decoding, density evolution (DE), iterative decoding, low-density parity-check (LDPC) codes, reduced-complexity decoding, complexity},
}

@InProceedings{Leonardon2018,
  author    = {M. L\'eonardon and C. Leroux and D. Binet and J.M. P. Langlois and C. J\'ego and Y. Savaria},
  title     = {Custom Low Power Processor for Polar Decoding},
  booktitle = {International Symposium on Circuits and Systems (ISCAS)},
  year      = {2018},
  pages     = {1--5},
  month     = may,
  publisher = {IEEE},
  abstract  = {Cloud Radio Access Network is foreseen as one of the key features of the future 5G mobile communication standard. In this context, all the baseband processing is intended to be performed on CPUs in order to keep a high level of flexibility. The challenge is then to propose efficient software implementations of baseband processing algorithms that guarantee a sufficient throughput, while limiting the energy consumption. In this paper, as an alternative to general purpose processors, we propose an implementation of an Application Specific Instruction set Processor customized for the Successive Cancellation decoding of polar codes. The resulting software decoder achieves throughputs similar to state-of-the-art ARM processor implementations, while reducing the energy consumption by a factor 10.},
  doi       = {10.1109/ISCAS.2018.8351739},
  file      = {:pdf/Leonardon2018 - Custom Low Power Processor for Polar Decoding.pdf:PDF},
  groups    = {Polar Codes, Hardware Decoders, AFF3CT},
  issn      = {0271-4302},
  keywords  = {5G mobile communication;Decoding;Energy consumption;Hardware;Registers;Software;Throughput},
}

@Misc{ETSI2017,
  author = {ETSI},
  title  = {{3GPP} - {TS} 38.212 - {Multiplexing} and channel coding ({R.} 15)},
  month  = sep,
  year   = {2017},
  groups = {5G},
}

@InProceedings{Cassagne2018,
  author    = {A. Cassagne and O. Aumage and D. Barthou and C. Leroux and C. J\'ego},
  title     = {{MIPP}: a Portable {C++} {SIMD} Wrapper and its use for Error Correction Coding in {5G} Standard},
  booktitle = {Workshop on Programming Models for SIMD/Vector Processing (WPMVP)},
  year      = {2018},
  address   = {V\"osendorf/Wien, Austria},
  month     = feb,
  publisher = {ACM},
  abstract  = {Error correction code (ECC) processing has so far been performed on dedicated hardware for previous generations of mobile communication standards, to meet latency and bandwidth constraints.
As the 5G mobile standard, and its associated channel coding algorithms, are now being specified, modern CPUs are progressing to the point where software channel decoders can viably be contemplated. A key aspect in reaching this transition point is to get the most of CPUs SIMD units on the decoding algorithms being pondered for 5G mobile standards. The nature and diversity of such algorithms requires highly versatile programming tools. This paper demonstrates the virtues and versatility of our MIPP SIMD wrapper in implementing a high performance portfolio of key ECC decoding algorithms.},
  doi       = {10.1145/3178433.3178435},
  file      = {:pdf/Cassagne2018 - MIPP\: a Portable C++ SIMD Wrapper and its use for Error Correction Coding in 5G Standard.pdf:PDF;:pdf/Cassagne2018 - MIPP\: a Portable C++ SIMD Wrapper and its use for Error Correction Coding in 5G Standard [slides].pdf:PDF},
  groups    = {Wrapper, Polar Codes, LDPC Codes, Software Decoders, Pseudo-Random Number Generator (PRNG), 5G, AFF3CT},
  keywords  = {SIMD, wrapper, C++, channel code, SSE, AVX, AVX-512, NEON},
}

@InProceedings{Cassagne2015,
  author       = {A. Cassagne and J-F. Boussuge and G. Puigt and N. Villedieu and I. {D'Ast} and A. Genot},
  title        = {{JAGUAR}: a New {CFD} Code Dedicated to Massively Parallel High-Order {LES} Computations on Complex Geometry},
  booktitle    = {International Conference on Applied Aerodynamics},
  year         = {2015},
  address      = {Toulouse, France},
  month        = apr,
  organization = {3AF},
  abstract     = {LES of industrial flows is associated with geometrical complexity and requires high order schemes to minimize dissipation and dispersion. To tackle these two issues it is necessary to use unstructured grids and High Performance Computing algorithms. In this context, CERFACS initiated two years ago the development of a new CFD code called JAGUAR based on a mathematical framework leading to high-level capability for LES. In this paper, many topics for HPC are introduced and solved in order to obtain the best code performance.},
  file         = {:pdf/Cassagne2015 - JAGUAR\: a New CFD Code Dedicated to Massively Parallel High-Order LES Computations on Complex Geometry.pdf:PDF},
  groups       = {Computational Fluid Dynamics (CFD)},
  url          = {http://www.cerfacs.fr/~cfdbib/repository/WN_CFD_14_5.pdf},
}

@TechReport{Cassagne2015a,
  author      = {A. Cassagne and J-F. Boussuge and G. Puigt},
  title       = {High-order Method for a New Generation of Large Eddy Simulation Solver},
  institution = {PRACE},
  year        = {2015},
  abstract    = {We enabled hybrid OpenMP/MPI computations for a new generation of CFD code based on a new high-order method (Spectral Difference method) dedicated to Large Eddy Simulation (LES). The code is written in Fortran 90 with MPI library and OpenMP directives for the parallelization. This white-paper is focused on achieving good performances with the OpenMP shared memory model on standard environment (bi-socket nodes and multi-core x86 processors). The goal was to reduce the number of MPI communications by considering MPI communications between nodes and OpenMP approach for all cores on any node. Three different approaches are compared: full MPI, full OpenMP and hybrid OpenMP/MPI. We observed that hybrid and full MPI computations took nearly the same time for a small number of cores.},
  file        = {:pdf/Cassagne2015a - High-order Method for a New Generation of Large Eddy Simulation Solver.pdf:PDF},
  groups      = {Computational Fluid Dynamics (CFD)},
  keywords    = {jaguar,cfd,openmp,mpi},
  url         = {http://www.cerfacs.fr/~cfdbib/repository/TR_CFD_15_9.pdf},
}

@TechReport{Cassagne2017,
  author      = {A. Cassagne and M. L\'eonardon and O. Hartmann and T. Tonnellier and G. Delbergue and V. Giraud and C. Leroux and R. Tajan and B. {Le Gal} and C. J\'ego and O. Aumage and D. Barthou},
  title       = {{AFF3CT} : Un environnement de simulation pour le codage de canal},
  institution = {GdR SoC2},
  year        = {2017},
  month       = jun,
  abstract    = {Dans cet article nous présentons un environnement de simulation de Monte Carlo pour les systèmes de communications numériques. Nous nous focalisons en particulier sur les fonctions associées au codage de
canal. Après avoir présenté les enjeux liés à la simulation, nous identifions trois problèmes inhérents à ce type de simulation. Puis nous présentons les principales caractéristiques de l’environnement AFF3CT.},
  file        = {:pdf/Cassagne2017 - AFF3CT \: Un environnement de simulation pour le codage de canal [poster].pdf:PDF;:pdf/Cassagne2017 - AFF3CT \: Un environnement de simulation pour le codage de canal.pdf:PDF},
  groups      = {Software Decoders, AFF3CT},
}

@Conference{Cassagne2017a,
  author    = {A. Cassagne and O. Hartmann and M. L\'eonardon and T. Tonnellier and G. Delbergue and C. Leroux and R. Tajan and B. {Le Gal} and C. J\'ego and O. Aumage and D. Barthou},
  title     = {Fast Simulation and Prototyping with {AFF3CT}},
  booktitle = {International Workshop on Signal Processing Systems (SiPS)},
  year      = {2017},
  month     = oct,
  publisher = {IEEE},
  abstract  = {This demonstration intends to present AFF3CT (A Fast Forward 3rror Correction Tool). The main objective of AFF3CT is to provide a portable, open source, fast and flexible software to the channel coding community in such a way that researchers can spend more time on channel coding / algorithmic problems instead of software development issues. It is also intended to facilitate the process of hardware verification and debug with the objective of fast prototyping.},
  file      = {:pdf/Cassagne2017a - Fast Simulation and Prototyping with AFF3CT [poster].pdf:PDF;:pdf/Cassagne2017a - Fast Simulation and Prototyping with AFF3CT [abstract].pdf:PDF},
  groups    = {Software Decoders, Hardware Decoders, AFF3CT},
}

@TechReport{Cassagne2014,
  author      = {A. Cassagne},
  title       = {Implémentation multi {GPU} de la méthode Spectral Differences pour un code de {CFD}},
  institution = {Université de Bordeaux / EPSI / CERFACS},
  year        = {2014},
  file        = {:pdf/Cassagne2014 - Implémentation multi GPU de la méthode Spectral Differences pour un code de CFD.pdf:PDF},
}

@MastersThesis{Cassagne2015b,
  author   = {A. Cassagne},
  title    = {Étude et implémentation d’une méthode de calcul pour la simulation numérique sur des architectures modernes},
  school   = {EPSI Bordeaux},
  year     = {2015},
  abstract = {In this thesis we will study some modern hardware architectures using a well-known method in digital simulation : the stencil codes. The current HPC context is quite suitable for the arrival of new technologies leaded by the race to exascale computation. The comparative analysis in this thesis is mainly based on performance (number of floating point operations per second) and on hardware energy efficiency. We will start by formalising and clarifing the stencil method, then we will take a deeper look at three architectures : one standard x86 CPU, one low consumption ARM CPU and one GPU specialized in computations. Thereafter, we will describe some stencil optimisations in order to implement efficient versions of code for each of the architectures. We will explore both well-known methods like Cache Blocking and Register Blocking as well as less known ones such as Dimension Lifted and Transposed and Temporal Blocking. To finish, all these implementations will be tested on a low order stencil using the heat equation discretisation. The analysis will contain three different parts following the three architectures. We will use the Roofline model in order to bound the maximal reachable performance. Then we will study the code internal behavior on CPU and GPU by modifying the problem size. We will also take a look on the weak scalability in caches but only for the CPUs. Lastly, we will present a comparative analysis of energy consumption (also called energy to solution analysis).},
  file     = {:pdf/Cassagne2015b - Étude et implémentation d’une méthode de calcul pour la simulation numérique sur des architectures modernes.pdf:PDF},
}

@TechReport{Eltablawy2017,
  author      = {Alaa Eltablawy and Andrey Vladimirov},
  title       = {Capabilities of Intel {AVX-512} in Intel Xeon Scalable Processors (Skylake)},
  institution = {Colfax International},
  year        = {2017},
  month       = sep,
  abstract    = {This paper reviews the Intel Advanced Vector Extensions 512 (Intel AVX-512) instruction set and answers two critical questions:
1. How do Intel Xeon Scalable processors based on the Skylake architecture (2017) compare to their predecessors based on Broadwell due to AVX-512?
2. How are Intel Xeon processors based on Skylake different from their alternative, Intel Xeon Phi processors with the Knights Landing architecture, which also feature AVX-512?
We address these questions from the programmer’s perspective by demonstrating C language code of microkernels benefitting from AVX-512. For each example, we dig deeper and analyze the compilation practices, resultant assembly, and optimization reports.
In addition to code studies, the paper contains performance measurements for a synthetic benchmark with guidelines on estimating peak performance. In conclusion, we outline the workloads and application
domains that can benefit from the new features of AVX-512 instructions.},
  file        = {:pdf/Eltablawy2017 - Capabilities of Intel AVX-512 in Intel Xeon Scalable Processors (Skylake).pdf:PDF},
  groups      = {Single Instruction Multiple Data (SIMD)},
  url         = {https://colfaxresearch.com/skl-avx512/},
}

@Book{Ryan2009,
  title     = {Channel codes: classical and modern},
  publisher = {Cambridge University Press},
  year      = {2009},
  author    = {W. Ryan and S. Lin},
  month     = sep,
  isbn      = {978-0-511-64182-4},
  abstract  = {Channel coding lies at the heart of digital communication and data storage, and this detailed introduction describes the core theory as well as decoding algorithms, implementation details, and performance analyses.
Professors Ryan and Lin, known for the clarity of their writing, provide the latest information on modern channel codes, including turbo and low-density parity-check (LDPC) codes. They also present detailed coverage of BCH codes, Reed–Solomon codes, convolutional codes, finite-geometry codes, and product codes, providing a one-stop resource for both classical and modern coding techniques.
The opening chapters begin with basic theory to introduce newcomers to the subject, assuming no prior knowledge in the field of channel coding. Subsequent chapters cover the encoding and decoding of the most widely used codes and extend to advanced topics such as code ensemble performance analyses and algebraic code design. Numerous varied and stimulating end-of-chapter problems, 250 in total,
are also included to test and enhance learning, making this an essential resource for students and practitioners alike.},
  file      = {:pdf/Ryan2009 - Channel codes\: classical and modern.pdf:PDF},
  groups    = {Error-Correcting Codes (ECC)},
  url       = {http://www.cambridge.org/9780521848688},
}

@InProceedings{Wang2014a,
  author    = {H. Wang and P. Wu and I. G. Tanase and M. J. Serrano and J. E. Moreira},
  title     = {Simple, Portable and Fast {SIMD} Intrinsic Programming: Generic Simd Library},
  booktitle = {Workshop on Programming Models for SIMD/Vector Processing (WPMVP)},
  year      = {2014},
  pages     = {9--16},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Using SIMD (Single Instruction Multiple Data) is a cost-effective way to explore data parallelism on modern processors. Most processor vendors today provide SIMD engines, such as Altivec/VSX for POWER, SSE/AVX for Intel processors, and NEON for ARM. While high-level SIMD programming models are rapidly evolving, for many SIMD developers, the most effective way to get the performance out of SIMD is still by programming directly via vendor-provided SIMD intrinsics. However, intrinsics programming is both tedious and error-prone, and worst of all, introduces non-portable codes.

This paper presents the Generic SIMD Library (https://github.com/genericsimd/generic_simd/), an open-source, portable C++ interface that provides an abstraction of short vectors and overloads most C/C++ operators for short vectors. The library provides several mappings from platform-specific intrinsics to the generic SIMD intrinsic interface so that codes developed based on the library are portable across different SIMD platforms.

We have evaluated the library with several applications from the multimedia, data analytics and math domains. Compared with platform-specific intrinsics codes, using Generic SIMD Library results in less line-of-code, a 22% reduction on average, and achieves similar performance as platform-specific intrinsics versions.},
  acmid     = {2568059},
  doi       = {10.1145/2568058.2568059},
  file      = {:pdf/Wang2014a - Simple, Portable and Fast SIMD Intrinsic Programming\: Generic Simd Library.pdf:PDF},
  groups    = {Wrapper},
  isbn      = {978-1-4503-2653-7},
  keywords  = {AVX, SIMD, SSE, altivec, generic programming},
  location  = {Orlando, Florida, USA},
  numpages  = {8},
  url       = {http://doi.acm.org/10.1145/2568058.2568059},
}

@InProceedings{Leissa2014,
  author    = {R. Lei\ssa and I. Haffner and S. Hack},
  title     = {Sierra: A {SIMD} Extension for {C++}},
  booktitle = {Workshop on Programming Models for SIMD/Vector Processing (WPMVP)},
  year      = {2014},
  pages     = {17--24},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Nowadays, SIMD hardware is omnipresent in computers. Nonetheless, many software projects make hardly use of SIMD instructions: Applications are usually written in general-purpose languages like C++. However, general-purpose languages only provide poor abstractions for SIMD programming enforcing an error-prone, assembly-like programming style. An alternative are data-parallel languages. They indeed offer more convenience to target SIMD architectures but introduce their own set of problems. In particular, programmers are often unwilling to port their working C++ code to a new programming language.

In this paper we present Sierra: a SIMD extension for C++. It combines the full power of C++ with an intuitive and effective way to address SIMD hardware. With Sierra, the programmer can write efficient, portable and maintainable code. It is particularly easy to enhance existing code to run efficiently on SIMD machines.

In contrast to prior approaches, the programmer has explicit control over the involved vector lengths.},
  acmid     = {2568062},
  doi       = {10.1145/2568058.2568062},
  file      = {:pdf/Leissa2014 - Sierra\: A SIMD Extension for C++.pdf:PDF},
  groups    = {Single Instruction Multiple Data (SIMD)},
  isbn      = {978-1-4503-2653-7},
  keywords  = {SIMD, c++, vectorization},
  location  = {Orlando, Florida, USA},
  numpages  = {8},
  url       = {http://doi.acm.org/10.1145/2568058.2568062},
}

@InProceedings{Tonnellier2016,
  author    = {T. Tonnellier and C. Leroux and B. {Le Gal} and C. J\'ego and B. Gadat and N. Van Wambeke},
  title     = {Hardware Architecture for Lowering the Error Floor of {LTE} Turbo Codes},
  booktitle = {Design and Architectures for Signal and Image Processing Conference (DASIP)},
  year      = {2016},
  pages     = {107--112},
  month     = oct,
  publisher = {IEE},
  abstract  = {Turbo codes are well known error-correcting codes used in many communication standards. However, they suffer from error floors. Recently, a method - denoted as the flip and check algorithm - that lowers the error floor of turbo codes was proposed. This method relies on the identification of the least reliable bits during the turbo decoding process. Gains of about one order of magnitude were reached in terms of error rate performance. In this article, the first hardware implementation of the method is presented. The feasibility and hardware complexity are addressed by studying the impact of the algorithmic parameters of the technique. Synthesis results for FPGA implementations are reported and compared to turbo decoders implementations.},
  doi       = {10.1109/DASIP.2016.7853805},
  file      = {:pdf/Tonnellier2016 - Hardware Architecture for Lowering the Error Floor of LTE Turbo Codes.pdf:PDF},
  groups    = {Turbo Codes},
  keywords  = {Long Term Evolution, decoding, error correction codes, field programmable gate arrays, turbo codes, FPGA, LTE, Long Term Evolution, error floor, error-correcting codes, flip-and-check algorithm, hardware architecture, hardware complexity, turbo codes, turbo decoding process, Computer architecture, Decoding, Error analysis, Hardware, Iterative decoding, Standards, Turbo codes},
}

@InProceedings{Tonnellier2016a,
  author    = {T. Tonnellier and C. Leroux and B. {Le Gal} and C. J\'ego and B. Gadat and N. Van Wambeke},
  title     = {Lowering the Error Floor of Double-Binary Turbo Codes: The Flip and Check Algorithm},
  booktitle = {International Symposium on Turbo Codes and Iterative Information Processing (ISTC)},
  year      = {2016},
  pages     = {156--160},
  month     = sep,
  publisher = {IEEE},
  abstract  = {This article addresses the error floor reduction of double-binary turbo codes. The proposed approach is an extension of a low complexity method originally proposed for decoding binary turbo codes. This method's interest is that it does not need to modify the turbo coding scheme as long as an error detection code is serially concatenated with the turbo code. Simulation results showed that error rate performance gains can reach one order of magnitude in the cases of the DVB-RCS and DVB-RCS2 standards, while keeping a well-handled computational complexity.},
  doi       = {10.1109/ISTC.2016.7593096},
  file      = {:pdf/Tonnellier2016a - Lowering the Error Floor of Double-Binary Turbo Codes\: The Flip and Check Algorithm.pdf:PDF},
  groups    = {Turbo Codes, AFF3CT},
  keywords  = {binary codes, computational complexity, convolutional codes, decoding, error detection codes, recursive estimation, turbo codes, DVB-RCS standard, DVB-RCS2 standard, binary turbo code decoding, computational complexity, double-binary turbo code error floor reduction, error detection code, error rate performance, flip and check algorithm, low complexity method, recursive systematic convolutional code, Decoding, Measurement},
}

@Article{Tonnellier2016b,
  author   = {T. Tonnellier and C. Leroux and B. {Le Gal} and B. Gadat and C. J\'ego and N. Van Wambeke},
  title    = {Lowering the Error Floor of Turbo Codes With {CRC} Verification},
  journal  = {IEEE Wireless Communications Letters (WCL)},
  year     = {2016},
  volume   = {5},
  number   = {4},
  pages    = {404--407},
  month    = aug,
  issn     = {2162-2337},
  abstract = {Decoding performance of turbo codes can flatten at moderately high signal-to-noise ratio. This letter proposes a low complexity method for lowering this error floor. This method rests on the observation of the extrinsic information during the iterative decoding process. A set of q most unreliable bits are identified based on their associated extrinsic information. A total of 2\textsuperscript{q} test patterns are then built by inverting the most unreliable bits. The decoded codeword is identified thanks to a cyclic redundancy check detector. This method keeps the turbo coding scheme unchanged as long as an error detection code is serially concatenated with the turbo code. Simulations were performed on a rate-1/3 Long-Term Evolution turbo code and show an improvement of at least one decade in terms of frame error rate in the error floor region. This low complexity method paves the way for further improvements in lowering the error floor of turbo codes.},
  doi      = {10.1109/LWC.2016.2571283},
  file     = {:pdf/Tonnellier2016b - Lowering the Error Floor of Turbo Codes With CRC Verification.pdf:PDF},
  groups   = {Turbo Codes, AFF3CT},
  keywords = {concatenated codes, cyclic redundancy check codes, error detection codes, error statistics, iterative decoding, turbo codes, CRC verification, Long-Term Evolution turbo code, cyclic redundancy check detector, decoded codeword identificaion, error detection code, frame error rate, iterative decoding process, low complexity method, serially concatenated code, signal-to-noise ratio, turbo code error floor, Cyclic redundancy check codes, Decoding, Error analysis, Iterative decoding, Measurement, Standards, Turbo codes, CRC codes, Turbo codes, error floor region, extrinsic information, iterative decoding process},
}

@Article{Tanner1981,
  author   = {R. Tanner},
  title    = {A Recursive Approach to Low Complexity Codes},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  year     = {1981},
  volume   = {27},
  number   = {5},
  pages    = {533--547},
  month    = sep,
  issn     = {0018-9448},
  abstract = {A method is described for constructing long error-correcting codes from one or more shorter error-correcting codes, referred to as subcodes, and a bipartite graph. A graph is shown which specifies carefully chosen subsets of the digits of the new codes that must be codewords in one of the shorter subcodes. Lower bounds to the rate and the minimum distance of the new code are derived in terms of the parameters of the graph and the subeodes. Both the encoders and decoders proposed are shown to take advantage of the code's explicit decomposition into subcodes to decompose and simplify the associated computational processes. Bounds on the performance of two specific decoding algorithms are established, and the asymptotic growth of the complexity of decoding for two types of codes and decoders is analyzed. The proposed decoders are able to make effective use of probabilistic information supplied by the channel receiver, e.g., reliability information, without greatly increasing the number of computations required. It is shown that choosing a transmission order for the digits that is appropriate for the graph and the subcodes can give the code excellent burst-error correction abilities. The construction principles},
  doi      = {10.1109/TIT.1981.1056404},
  file     = {:pdf/Tanner1981 - A Recursive Approach to Low Complexity Codes.pdf:PDF},
  groups   = {Error-Correcting Codes (ECC)},
  keywords = {Error-correcting codes, Algorithm design and analysis, Bipartite graph, Complexity theory, Decoding, Error correction codes, Helium, Merging, Performance analysis, Product codes, Sorting},
}

@InProceedings{LeGal2017,
  author    = {B. {Le Gal} and C. J\'ego},
  title     = {Low-Latency Software {LDPC} Decoders for x86 Multi-Core Devices},
  booktitle = {International Workshop on Signal Processing Systems (SiPS)},
  year      = {2017},
  pages     = {1--6},
  month     = oct,
  publisher = {IEEE},
  abstract  = {LDPC codes are a family of error correcting codes used in most modern digital communication standards even in future 3GPP 5G standard. Thanks to their high processing power and their parallelization capabilities, prevailing multi-core and many-core devices facilitate real-time implementations of digital communication systems, which were previously implemented on dedicated hardware targets. Through massive frame decoding parallelization, current LDPC decoders throughputs range from hundreds of Mbps up to Gbps. However, inter-frame parallelization involves latency penalties, while in future 5G wireless communication systems, the latency should be reduced as far as possible. To this end, a novel LDPC parallelization approach for LDPC decoding on a multi-core processor device is proposed in this article. It reduces the processing latency down to some microseconds as highlighted by x86 multi-core experimentations.},
  doi       = {10.1109/SiPS.2017.8110001},
  file      = {:pdf/LeGal2017 - Low-Latency Software LDPC Decoders for x86 Multi-Core Devices.pdf:PDF},
  groups    = {Error-Correcting Codes (ECC), LDPC Codes, HoF LDPC - BP},
  keywords  = {3G mobile communication, 5G mobile communication, decoding, digital communication, error correction codes, parity check codes, software radio, LDPC codes, LDPC decoding, digital communication systems, error correcting codes, future 3GPP 5G standard, hardware targets, high processing power, inter-frame parallelization, latency penalties, low-latency software LDPC decoders, many-core devices, massive frame decoding parallelization, modern digital communication standards, multicore processor device, novel LDPC parallelization approach, parallelization capabilities, processing latency, real-time implementations, x86 multicore devices, x86 multicore experimentations, Decoding, Parallel processing, Parity check codes, Program processors, Standards, Throughput},
}

@Article{Richardson2003,
  author   = {T. Richardson and R. Urbanke},
  title    = {The Renaissance of Gallager's Low-Density Parity-Check Codes},
  journal  = {IEEE Communications Magazine},
  year     = {2003},
  volume   = {41},
  number   = {8},
  pages    = {126--131},
  month    = aug,
  issn     = {0163-6804},
  abstract = {LDPC codes were invented in 1960 by R. Gallager. They were largely ignored until the discovery of turbo codes in 1993. Since then, LDPC codes have experienced a renaissance and are now one of the most intensely studied areas in coding. In this article we review the basic structure of LDPC codes and the iterative algorithms that are used to decode them. We also briefly consider the state of the art of LDPC design.},
  comment  = {C'est un papier introductif sur les codes LDPCs.

Les LDPCs sont définis par le décodeur.
Il y a deux grandes familles de décodeur :
  - ceux par message (message passing) (onéreux),
  - les bits flipping (cheap).

Le design du code est essentiel, souvent les codes qui ont une mauvaise convergence sont bon dans le floor.
C'est pas évident d'avoir un code super bon dans la convergence et dans le floor.

Multi-edge type LDPC, je n'ai pas bien compris en quoi ça consiste, c'est sensé améliorer les codes LDPC...},
  doi      = {10.1109/MCOM.2003.1222728},
  file     = {:pdf/Richardson2003 - The Renaissance of Gallager's Low-Density Parity-Check Codes.pdf:PDF},
  groups   = {LDPC Codes},
  keywords = {graph theory, iterative decoding, parity check codes, reviews, Gallager's low-density parity-check codes, LDPC codes, basic structure, decoding, iterative algorithms, Belief propagation, Computational complexity, Iterative algorithms, Iterative decoding, Iterative methods, Maximum likelihood decoding, Parity check codes, Redundancy, Routing, Signal processing},
}

@InProceedings{MacKay1995,
  author    = {D. J. C. MacKay and R. M. Neal},
  title     = {Good Codes Based on Very Sparse Matrices},
  booktitle = {IMA International Conference on Cryptography and Coding (IMA-CCC)},
  year      = {1995},
  pages     = {100--111},
  address   = {UK},
  month     = dec,
  publisher = {Springer},
  doi       = {10.1007/3-540-60693-9_13},
  file      = {:pdf/MacKay1995 - Good Codes Based on Very Sparse Matrices.pdf:PDF},
  groups    = {LDPC Codes},
  isbn      = {978-3-540-49280-1},
  url       = {https://doi.org/10.1007/3-540-60693-9_13},
}

@PhdThesis{Alevizos2012,
  author   = {P. Alevizos},
  title    = {Factor Graphs: Theory and Applications},
  school   = {Technical University of Crete},
  year     = {2012},
  abstract = {Factor graphs (FGs) represent graphically the factorization of a global function into a product of local sub-functions. The global function is usually a multi-variable probability density function (pdf ), where the calculation of a marginal pdf is usually intractable. The sum-product algorithm (SPA) is applied on the FG through message-passing, i.e. exchange of functions, between the FG nodes in a distributed way; the output is a marginal pdf with respect to a variable of interest. Factor graph theory has several applications in many interdisciplinary fields, such as error correction coding theory, detection and estimation, wireless networking, artificial intelligence and many others.
This thesis provides the basic theoretical background in a tutorial way, from first principles. Furthermore, specific FG applications found in the literature are presented. Specifically, coding problems (LDPC, convolutional and parallel concatenated Turbo codes), Bayesian estimation (in the context of network localization) and wireless multi-hop networking (in the context of time scheduling) are analyzed within the FG framework. In all cases, the respective graph, the associated SPA, the message-passing scheduling and the final output are thoroughly presented. The power of FGs as a distributed inference tool is vividly demonstrated.},
  file     = {:pdf/Alevizos2012 - Factor Graphs\: Theory and Applications.pdf:PDF},
  groups   = {Factor Graphs},
}

@Misc{Yedidia2004,
  author    = {J. S. Yedidia and E. Martinian},
  title     = {Quantizing Signals Using Sparse Generator Factor Graph Codes},
  month     = aug,
  year      = {2004},
  note      = {US Patent 6,771,197},
  abstract  = {A method quantizes an input signal of N samples into a string of k symbols drawn from a q-ary alphabet. A complementary method reproduces a minimally distorted version of the input signal from the quantized string, given some distortion measure. First, an [N,k]q linear error-correcting code that has a sparse generator factor graph representation is selected. A fixed mapping from q-ary symbols to samples is selected. A soft-input decoder and an encoder for the SGFG codes is selected. A cost function is determined from the input signal and a distortion measure, using the fixed mapping. The decoder determines an information block corresponding to a code word of the SGFG code with a low cost for the input signal. The input signal can be reproduced using the encoder for the SGFG code, in combination with the fixed mapping. },
  file      = {:pdf/Yedidia2004 - Quantizing Signals Using Sparse Generator Factor Graph Codes.pdf:PDF},
  groups    = {Factor Graphs},
  publisher = {Google Patents},
}

@Misc{Wolf2008,
  author = {J. K. Wolf},
  title  = {An Introduction Error Correcting Codes (Part 1)},
  year   = {2008},
  file   = {:pdf/Wolf2008 - An Introduction Error Correcting Codes (Part 1).pdf:PDF},
  groups = {Error-Correcting Codes (ECC)},
}

@Article{Chandesris2018,
  author   = {L. Chandesris and V. Savin and D. Declercq},
  title    = {Dynamic-SCFlip Decoding of Polar Codes},
  journal  = {IEEE Transactions on Communications (TCOM)},
  year     = {2018},
  volume   = {PP},
  number   = {99},
  pages    = {1},
  issn     = {0090-6778},
  abstract = {This paper proposes a generalization of the recently introduced Successive Cancellation Flip (SCFlip) decoding of polar codes, characterized by a number of extra decoding attempts, where one or several positions are flipped from the standard Successive Cancellation (SC) decoding. To make such an approach effective, we first introduce the concept of higher-order bit-flips, and propose a new metric to determine the bit-flips that are more likely to correct the trajectory of the SC decoding. We then propose a generalized SCFlip decoding algorithm, referred to as Dynamic-SCFlip (D-SCFlip), which dynamically builds a list of candidate bit-flips, while guaranteeing that the next attempt has the highest probability of success among the remaining ones. Simulation results show that D-SCFlip is an effective alternative to SC-List decoding of polar codes, by providing very good error correcting performance, with an average computation complexity close to the one of the SC decoder.},
  doi      = {10.1109/TCOMM.2018.2793887},
  file     = {:pdf/Chandesris2018 - Dynamic-SCFlip Decoding of Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {Computational complexity, Decoding, Error correction, Measurement, Signal to noise ratio, Standards, Polar Codes, SCFlip decoding, order statistic decoding, successive cancellation decoding, SC flip},
}

@InProceedings{Afisiadis2014,
  author    = {O. Afisiadis and A. Balatsoukas-Stimming and A. Burg},
  title     = {A Low-Complexity Improved Successive Cancellation Decoder for Polar Codes},
  booktitle = {Asilomar Conference on Signals, Systems, and Computers (ACSSC)},
  year      = {2014},
  pages     = {2116--2120},
  month     = nov,
  publisher = {IEEE},
  abstract  = {Under successive cancellation (SC) decoding, polar codes are inferior to other codes of similar blocklength in terms of frame error rate. While more sophisticated decoding algorithms such as list- or stack-decoding partially mitigate this performance loss, they suffer from an increase in complexity. In this paper, we describe a new flavor of the SC decoder, called the SC flip decoder. Our algorithm preserves the low memory requirements of the basic SC decoder and adjusts the required decoding effort to the signal quality. In the waterfall region, its average computational complexity is almost as low as that of the SC decoder.},
  doi       = {10.1109/ACSSC.2014.7094848},
  file      = {:pdf/Afisiadis2014 - A Low-Complexity Improved Successive Cancellation Decoder for Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  keywords  = {computational complexity, decoding, error statistics, signal processing, average computational complexity, frame error rate, low-complexity improved SC flip decoder, polar codes, signal quality, successive cancellation decoding, Computational complexity, Decoding, Error analysis, Memory management, Signal to noise ratio, SCFlip},
}

@PhdThesis{Tonnellier2017,
  author   = {T. Tonnellier},
  title    = {Contribution to the Improvement of the Decoding Performance of Turbo Codes : Algorithms and Architecture},
  school   = {Universit{\'e} de Bordeaux},
  year     = {2017},
  abstract = {Since their introduction in the 90’s, turbo codes are considered as one of the most powerful error-correcting code. Thanks to their excellent trade-off between computational complexity and decoding performance, they were chosen in many communication standards.

One way to characterize error-correcting codes is the evolution of the bit error rate as a function of signal-to-noise ratio (SNR). The turbo code error rate performance is divided in two different regions : the waterfall region and the error floor region. In the waterfall region, a slight increase in SNR results in a significant drop in error rate. In the error floor region, the error rate performance is only slightly improved as the SNR grows. This error floor can prevent turbo codes from being used in applications with low error rates requirements. Therefore various constructions optimizations that lower the error floor of turbo codes has been proposed in recent years by scientific community. However, these approaches can not be considered for already standardized turbo codes.

This thesis addresses the problem of lowering the error floor of turbo codes without allowing any modification of the digital communication chain at the transmitter side.
For this purpose, the state-of-the-art post-processing decoding method for turbo codes is detailed. It appears that efficient solutions are expensive to implement due to the required multiplication of computational resources or can strongly impact the overall decoding latency.

Firstly, two decoding algorithms based on the monitoring of decoder’s internal metrics are proposed. The waterfall region is enhanced by the first algorithm. However, the second one marginally lowers the error floor. Then, the study shows that in the error floor region, frames decoded by the turbo decoder are really close to the word originally transmitted. This is demonstrated by a proposition of an analytical prediction of the distribution of the number of bits in errors per erroneous frame. This prediction rests on the distance spectrum of turbo codes. Since the appearance of error floor region is due to only few bits in errors, an identification metric is proposed. This lead to the proposal of an algorithm that can correct residual errors. This algorithm, called Flip-and-Check, rests on the generation of candidate words, followed by verification according to an error-detecting code. Thanks to this decoding algorithm, the error floor of turbo codes encountered in different standards (LTE, CCSDS, DVB-RCS and DVB-RCS2) is lowered by one order of magnitude. This performance improvement is obtained without considering an important vcomputational complexity overhead.

Finally, a hardware decoding architecture implementing the Flip-and-Check algorithm is presented. A preliminary study of the impact of the different parameters of this algorithm is carried out. It leads to the definition of optimal values for some of these parameters. Others has to be adapted according to the gains targeted in terms of decoding performance. The possible integration of this algorithm along with existing turbo decoders is demonstrated thanks to this hardware architecture. This therefore enables the lowering of the error floors of standardized turbo codes.},
  file     = {:pdf/Tonnellier2017 - Contribution to the Improvement of the Decoding Performance of Turbo Codes \: Algorithms and Architecture.pdf:PDF},
  groups   = {Turbo Codes, AFF3CT},
  keywords = {Turbo codes, Residual Errors, Post-processing, Hardware architecture},
  url      = {https://tel.archives-ouvertes.fr/tel-01580476},
}

@InProceedings{Ghaffari2017,
  author    = {A. Ghaffari and M. L\'eonardon and Y. Savaria and C. J\'ego and C. Leroux},
  title     = {Improving Performance of {SCMA} {MPA} Decoders using Estimation of Conditional Probabilities},
  booktitle = {International Conference on New Circuits and Systems (NEWCAS)},
  year      = {2017},
  pages     = {21--24},
  month     = jun,
  abstract  = {Sparse code multiple access (SCMA) is a new type of non-orthogonal modulation suggested for 5G systems offering lower bit-error rate and higher spectral efficiency. There are many challenges when designing high throughput SCMA message passing decoders to meet the standards expected from 5G networks. Particularly, the message passing algorithm (MPA) needs many exponential computations to calculate conditional probabilities in case of Gaussian noise channels. This paper describes a sub-optimal modeling of noise using polynomial probability distributions rather than a normal distribution to eliminate the exponential calculations for MPA detectors. Simulation results demonstrate that an estimated SCMA MPA reaches the desired bit-error rate performance with much lower computational/hardware complexity.},
  doi       = {10.1109/NEWCAS.2017.8010095},
  file      = {:pdf/Ghaffari2017 - Improving Performance of SCMA MPA Decoders using Estimation of Conditional Probabilities.pdf:PDF},
  groups    = {Turbo Codes, AFF3CT},
  keywords  = {5G mobile communication, Gaussian channels, error statistics, multi-access systems, probability, 5G systems, Gaussian noise channels, SCMA MPA decoders, bit-error rate, conditional probabilities, message passing algorithm, nonorthogonal modulation, polynomial probability distributions, sparse code multiple access, 5G mobile communication, Algorithm design and analysis, Decoding, Estimation, Message passing, Probability density function, Throughput, 5G, BER, MPA Decoder, Sparse Code Multiple Access (SCMA), Uplink, iterative multi-user detection},
}

@PhdThesis{Guilloud2004,
  author   = {F. Guilloud},
  title    = {Generic Architecture for {LDPC} Codes Decoding},
  school   = {T{\'e}l{\'e}com ParisTech},
  year     = {2004},
  abstract = {The Low-Density Parity-Check codes are among the most powerful forward error correcting codes, since they enable to get as close as a fraction of a dB from the Shannon limit.
This astonishing performance combined with their relatively simple decoding algorithm make these codes very attractive for the next digital transmission system generations. It is already the case for the next digital satellite broadcasting standard (DVB-S2), where an irregular LDPC code has been chosen to protect the downlink information.

In this thesis, we focused our research on the iterative decoding algorithms and their hardware implementations. We proposed first a suboptimal algorithm named the λ-min algorithm. It reduces significantly the complexity of the decoder without any significant performance loss, as compared to the belief propagation (BP) algorithm.

Then we studied and designed a generic architecture of an LDPC decoder, which has been implemented on a FPGA based platform. This hardware decoder enables to accelerate the simulations more than 500 times as compared to software simulations. Moreover, based on an all-tunable design, our decoder features many facilities: It is possible to configure it for a very wide code family, so that the research for good codes is processed
faster ; thanks to the genericity of the processing components, it is also possible to optimize the internal coding format, and even to compare various decoding algorithms and various processing schedules.

Finally, our experience in the area of LDPC decoders led us to propose a formal framework for analysing the architectures of LDPC decoders. This framework encompasses both the datapath (parallelism, node processors architectures) and the control mode associated to the several decoding schedules. Thus within this framework, a classification of the different state-of-the-art LDPC decoders is proposed. Moreover, some synthesis of efficient and unpublished architectures have been also proposed.},
  file     = {:pdf/Guilloud2004 - Generic Architecture for LDPC Codes Decoding.pdf:PDF},
  groups   = {LDPC Codes, Hardware Decoders},
  keywords = {Low-density parity-check codes, Design, Ldpc,Decoder,Fpga,Vhdl,Architecture,decoder,low density codes,vertical layered, horizontal layered, vertical, horizontal},
  url      = {https://pastel.archives-ouvertes.fr/pastel-00000806/},
}

@TechReport{Cassagne2013a,
  author      = {A. Cassagne and A. George and B. Lorendeau and J-C. Papin and A. Rougier},
  title       = {Concurrent Kernel Execution on Graphic Processing Units},
  institution = {University of Bordeaux},
  year        = {2013},
  month       = jan,
  note        = {Projet d’{\'E}tude et de Recherche (PER)},
  abstract    = {General Purpose Graphic Processing Unit (GPGPU) are now used in high performance computing (HPC) for their massively parallel computing aspect and capabilities. Those devices integrate hundreds of computing unit (computing core). Usually, such a level of parallelism is used to solve simulation problems (heat transfer, …) because of the numerical representation of simulated environment (matrices). Those GPU can be programmed with specific programming languages like CUDA and OpenCL which provide a standard environment (C/C++ libraries). Programs executed on a GPU (also called kernels) are executed sequentially. However, in order to maximize the usage of GPU resources, some advanced features (developed by NVIDIA) allow programmers to execute severals kernels in parallel on the GPU. Unfortunately, concurrent kernels execution is only possible with CUDA on NVIDIA graphics cards. For other cards, OpenCL does not offer this functionality. That is why researchers from University of Virginia (USA) [2], tried to extend OpenCL standard by allowing execution of an "master kernel" which will launch other kernels. In fact, the "master kernel" is a mix of memory-bound and compute-bound kernels. By doing this, they could evaluate the advantage of this kind of solution. Another group of researchers (from University of George Washington and from University of Arkansas), designed a software environment that allows different threads from the same process to share access to the GPU, which wasn’t possible until the introduction of the "Automatic Context Funneling" [2] capabilities in CUDA 4.0. For our PER (Projet d’Etude et de Recherche), we will analyse the benefits and limitations of concurrent kernel execution. We will also determine if parallel kernel execution can be used to avoid the cost of data transfers from the host to the GPU (by starting long computing time kernel before starting data transfers).},
  file        = {:pdf/Cassagne2013a - Concurrent Kernel Execution on Graphic Processing Units.pdf:PDF},
  keywords    = {Computer science, CUDA, nVidia, nVidia GeForce GTX 660, nVidia Quadro 4000, OpenCL, Performance, GPU, Concurrent kernel},
  url         = {https://hgpu.org/?p=10765},
}

@TechReport{Cassagne2013b,
  author      = {A. Cassagne and B. Mortier and D. Pasqualinotto and V. Fr\'echaud},
  title       = {Portage d’un code de lattice {QCD} sur {GPU}},
  institution = {University of Bordeaux},
  year        = {2013},
  month       = mar,
  note        = {Projet d’{\'E}tude et de D{\'e}veloppement (PED)},
  file        = {:pdf/Cassagne2013b - Portage dun code de lattice QCD sur GPU.pdf:PDF},
  keywords    = {GPU, CUDA, OpenCL, QCD},
}

@InProceedings{Zhang2002,
  author    = {J. Zhang and M. Fossorier},
  title     = {Shuffled Belief Propagation Decoding},
  booktitle = {Asilomar Conference on Signals, Systems, and Computers (ACSSC)},
  year      = {2002},
  volume    = {1},
  pages     = {8--15 vol.1},
  month     = nov,
  publisher = {IEEE},
  abstract  = {In this paper, we propose a shuffled version of the belief propagation (BP) algorithm for the decoding of low-density parity-check (LDPC) codes. We show that when the Tanner graph of the code is acyclic and connected, the proposed scheme is optimal in the sense of MAP decoding and converges faster (or at least no slower) than the standard BP algorithm. Interestingly, this new version keeps the computational advantages of the forward-backward implementations of BP decoding. Both serial and parallel implementations are considered. We show by simulation that the new schedule offers better performance/complexity trade-offs.},
  comment   = {C'est le papier qui a introduit le vertical layered.},
  doi       = {10.1109/ACSSC.2002.1197141},
  file      = {:pdf/Zhang2002 - Shuffled Belief Propagation Decoding.pdf:PDF;:pdf/Zhang2002 - Shuffled Belief Propagation Decoding [slides].pdf:PDF},
  groups    = {LDPC Codes},
  issn      = {1058-6393},
  keywords  = {computational complexity, graph theory, maximum likelihood decoding, maximum likelihood estimation, parity check codes, BP algorithm, LDPC codes, MAP decoding, Tanner graph, computational complexity, forward-backward implementations, low-density parity-check codes, maximum a posteriori probability, parallel implementation, serial implementation, shuffled belief propagation decoding, Belief propagation, Code standards, Computational modeling, Concurrent computing, Iterative algorithms, Iterative decoding, Parallel processing, Parity check codes, Processor scheduling, Standards development, vertical layered, layered scheduling, layered},
}

@InProceedings{Yeo2001,
  author    = {E. Yeo and P. Pakzad and B. Nikolic and V. Anantharam},
  title     = {High Throughput Low-Density Parity-Check Decoder Architectures},
  booktitle = {Global Communications Conference (GLOBECOM)},
  year      = {2001},
  volume    = {5},
  pages     = {3019--3024 vol.5},
  publisher = {IEEE},
  abstract  = {Two decoding schedules and the corresponding serialized architectures for low-density parity-check (LDPC) decoders are presented. They are applied to codes with parity-check matrices generated either randomly or using geometric properties of elements in Galois fields. Both decoding schedules have low computational requirements. The original concurrent decoding schedule has a large storage requirement that is dependent on the total number of edges in the underlying bipartite graph, while a new, staggered decoding schedule which uses an approximation of the belief propagation, has a reduced memory requirement that is dependent only on the number of bits in the block. The performance of these decoding schedules is evaluated through simulations on a magnetic recording channel},
  comment   = {Article qui introduit l'horizontal layered pour la première fois.},
  doi       = {10.1109/GLOCOM.2001.965981},
  file      = {:pdf/Yeo2001 - High Throughput Low-Density Parity-Check Decoder Architectures.pdf:PDF},
  groups    = {LDPC Codes},
  keywords  = {Galois fields, belief maintenance, decoding, error detection codes, magnetic recording, Galois fields, LDPC decoders, belief propagation, bipartite graph, concurrent decoding, decoding schedules, low-density parity-check decoders, magnetic recording channel, parity check matrices, serialized architectures, staggered decoding schedule, Bipartite graph, Computational modeling, Computer architecture, Galois fields, Iterative decoding, Magnetic recording, Message passing, Parity check codes, Processor scheduling, Throughput, layered, horizontal layered},
}

@PhdThesis{Li2012a,
  author   = {M. Li},
  title    = {Design, Implementation and Prototyping of an Iterative Receiver for Bit-Interleaved Coded Modulation System dedicated to {DVB-T2}},
  school   = {University of Bretagne Sud},
  year     = {2012},
  abstract = {In 2008, the European Digital Video Broadcasting (DVB) standardization committee issued the second generation of Digital Video Broadcasting-Terrestrial (DVB-T2) standard in order to enable the wide broadcasting of high definition and 3D TV programmes. DVB-T2 has adopted several new technologies to provide more robust reception compared to the first genaration standard. One important technology is the bit interleaved coded modulation (BICM) with doubled signal space diversity plus the usage of low-density parity check (LDPC) codes. Both techniques can be combined at the receiver side through an iterative process between the decoder and demapper in order to further increase the system performance. The object of my study was to design and prototype a DVB-T2 receiver which supports iterative process. The two main contributions to the demapper design are the proposal of a linear approximation of Euclidean distance computation and the derivation of a sub-region detection algorithm for the two-dimensional demapper. Both contributions allows the computational complexity of the demapper to be reduced for its hardware implementation. In order to enable iterative processing between the demapper and the decoder, we investigated the use of vertical shuffled Min-Sum LDPC decoding algorithm. A novel vertical shuffled iterative structure aiming at reducing the latency of iterative processing and the corresponding architecture of the decoder were proposed. The proposed demapper and decoder have been integrated in a real DVB-T2 demodulator and tested in order to validate the efficiency of the proposed architecture. The prototype of a simplified DVB-T2 transceiver has been implemented, in which the receiver supports both non-iterative process and iterative process. We published the first paper related to a DVB-T2 iterative receiver. },
  file     = {:pdf/Li2012a - Design, Implementation and Prototyping of an Iterative Receiver for Bit-Interleaved Coded Modulation System dedicated to DVB-T2.pdf:PDF},
  groups   = {LDPC Codes, Hardware Decoders},
  keywords = {DVB-T2 standard, signal-space diversity, rotated QAM, 2D Demapper, LDPC decoder, iterative receiver, turbo-demodulation, FPGA, prototype, Standard DVB-T2, Diversit{\'e} de constellation, MAQ tourn{\'e}e, D{\'e}mappeur 2D, R{\'e}cepteur it{\'e}ratif, Turbo demodulation, FPGA prototype, layered, vertical layered},
  url      = {https://tel.archives-ouvertes.fr/tel-00719312},
}

@Misc{Cassagne2018a,
  author   = {A. Cassagne and O. Hartmann and M. L\'eonardon and C. Leroux and C. J\'ego},
  title    = {A Fast Forward Error Correction Toolbox: Seminary},
  month    = mar,
  year     = {2018},
  note     = {Presentation at the IMS laboratory, Bordeaux, France.},
  file     = {:pdf/Cassagne2018a - A Fast Forward Error Correction Toolbox\: Seminary.pdf:PDF},
  groups   = {AFF3CT, Error-Correcting Codes (ECC), Polar Codes, Turbo Codes, LDPC Codes},
  keywords = {AFF3CT},
}

@Article{Ghaffari2018,
  author   = {A. Ghaffari and M. L\'eonardon and A. Cassagne and C. Leroux and Y. Savaria},
  title    = {Toward High Performance Implementation of {5G} {SCMA} Algorithms},
  journal  = {IEEE Access},
  year     = {2018},
  note     = {Submitted 15-Jul-2018},
  abstract = { The recent evolution of mobile communication systems toward a 5G network is associated with the search for new types of non-orthogonal modulations such as Sparse Code Multiple Access (SCMA). Such modulations are proposed in response to demands for increasing the number of connected users. SCMA is a non-orthogonal multiple access technique that offers improved Bit-Error Rate (BER) performance and higher spectral efficiency than other comparable techniques, but these improvements come at the cost of complex decoders. There are many challenges in designing near-optimum high throughput SCMA decoders. This paper explores means to enhance the performance of SCMA decoders. To achieve this goal, in this paper, various improvements to the MPA algorithms are proposed. They notably aim at adapting SCMA decoding to the Single Instruction Multiple Data (SIMD) paradigm. An approximate modeling of noise is performed to reduce the complexity of floating point calculations. The effects of forward error corrections such as Polar codes, Turbo codes and LDPC, as well as different ways of accessing memory and improving power efficiency of  modified MPAs are investigated. The results show that the throughput of a SCMA decoder can be increased by 3.1 to 21 times when compared to the original MPA on different computing platforms using the suggested improvements. },
  file     = {:pdf/Ghaffari2018 - Toward High Performance Implementation of 5G SCMA Algorithms.pdf:PDF},
  keywords = {5G, SCMA, Maximum Likelihood (ML), Message Passing Algorithm (MPA), log-MPA, iterative multi-user detection, BER, Single Instruction Multiple Data (SIMD), Intel Advanced Vector Extensions (AVX), Streaming SIMD Extension (SSE), Knights Corner Instruction (KNCI), power efficiency, exponential estimations. },
}

@Article{Li2016a,
  author   = {A. Li and R. G. Maunder and B. M. Al-Hashimi and L. Hanzo},
  title    = {Implementation of a Fully-Parallel Turbo Decoder on a General-Purpose Graphics Processing Unit},
  journal  = {IEEE Access},
  year     = {2016},
  volume   = {4},
  pages    = {5624--5639},
  issn     = {2169-3536},
  abstract = {Turbo codes comprising a parallel concatenation of upper and lower convolutional codes are widely employed in the state-of-the-art wireless communication standards, since they facilitate transmission throughputs that closely approach the channel capacity. However, this necessitates high processing throughputs in order for the turbo code to support real-time communications. In the state-of-the-art turbo code implementations, the processing throughput is typically limited by the data dependences that occur within the forward and backward recursions of the Log-BCJR algorithm, which is employed during turbo decoding. In contrast to the highly serial Log-BCJR turbo decoder, we have recently proposed a novel fully parallel turbo decoder (FPTD) algorithm, which can eliminate the data dependences and perform fully parallel processing. In this paper, we propose an optimized FPTD algorithm, which reformulates the operation of the FPTD algorithm so that the upper and lower decoders have identical operation, in order to support single instruction multiple data operation. This allows us to develop a novel general purpose graphics processing unit (GPGPU) implementation of the FPTD, which has application in software-defined radios and virtualized cloud-radio access networks. As a benefit of its higher degree of parallelism, we show that our FPTD improves the higher processing throughput of the Log-BCJR turbo decoder by between 2.3 and 9.2 times, when employing a high-specification GPGPU. However, this is achieved at the cost of a moderate increase of the overall complexity by between 1.7 and 3.3 times.},
  doi      = {10.1109/ACCESS.2016.2586309},
  file     = {:pdf/Li2016a - Implementation of a Fully-Parallel Turbo Decoder on a General-Purpose Graphics Processing Unit.pdf:PDF},
  groups   = {Error-Correcting Codes (ECC), Turbo Codes, Software Decoders, HoF Turbo - FPTD, HoF Turbo - MAP},
  keywords = {convolutional codes, graphics processing units, parallel processing, radio networks, telecommunication computing, turbo codes, GPGPU, general purpose graphics processing unit, optimized FPTD algorithm, parallel processing, data dependences, Log-BCJR algorithm, channel capacity, wireless communication standards, convolutional codes, turbo codes, fully-parallel turbo decoder, Decoding, Turbo codes, Software defined radio, Radio access networks, Throughput, Parallel processing, Wireless communication, Convolutional codes, Graphics processing units, Fully-parallel turbo decoder, parallel processing, GPGPU computing, software defined radio, could radio access network},
}

@InProceedings{Cammerer2017,
  author    = {S. Cammerer and B. Leible and M. Stahl and J. Hoydis and S. ten Brink},
  title     = {Combining Belief Propagation and Successive Cancellation List Decoding of Polar Codes on a {GPU} Platform},
  booktitle = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2017},
  pages     = {3664--3668},
  month     = mar,
  publisher = {IEEE},
  abstract  = {The decoding performance of polar codes strongly depends on the decoding algorithm used, while also the decoder throughput and its latency mainly depend on the decoding algorithm. In this work, we implement the powerful successive cancellation list (SCL) decoder on a GPU and identify the bottlenecks of this algorithm with respect to parallel computing and its difficulties. The inherent serial decoding property of the SCL algorithm naturally limits the achievable speed-up gains on GPUs when compared to CPU implementations. In order to increase the decoding throughput, we use a hybrid decoding scheme based on the belief propagation (BP) decoder, which can be intra- and inter-frame parallelized. The proposed scheme combines excellent decoding performance and high throughput within the signal-to-noise ratio (SNR) region of interest.},
  doi       = {10.1109/ICASSP.2017.7952840},
  file      = {:pdf/Cammerer2017 - Combining Belief Propagation and Successive Cancellation List Decoding of Polar Codes on a GPU Platform.pdf:PDF},
  groups    = {HoF Turbo - FPTD, Software Decoders, Polar Codes, HoF Polar - SCL},
  issn      = {2379-190X},
  keywords  = {belief maintenance, graphics processing units, parallel processing, parity check codes, telecommunication computing, SNR, signal-to-noise ratio region of interest, BP, hybrid decoding scheme, CPU implementations, serial decoding property, speed-up gains, parallel computing, SCL, successive cancellation list decoder, decoder throughput, decoding algorithm, GPU platform, polar codes, successive cancellation list decoding, belief propagation, Decoding, Throughput, Graphics processing units, Iterative decoding, Belief propagation, Signal to noise ratio, Complexity theory},
}

@InProceedings{Li2016b,
  author    = {Y. Li and R. Liu},
  title     = {High Throughput {GPU} Polar Decoder},
  booktitle = {International Conference on Computer and Communications (ICCC)},
  year      = {2016},
  pages     = {1123--1127},
  month     = oct,
  publisher = {IEEE},
  abstract  = {In this paper, a polar decoder with simplified decoding algorithm, optimized thread execution and memory access on a Graphics Processing Unit (GPU) is proposed. Four main aspects are optimized to improve the throughput of the polar decoder. Firstly, decoding information is arranged to ensure global memory coalesced access. Secondly, to avoid extra computation and access delay of frozen set, semi-unrolled architecture is adopted. Finally, the branch divergence is optimized to decrease the branch delay. The experiments are carried out on Nvidia Tesla K20 platform. Results demonstrate that the proposed decoder achieves about 700Mbps.},
  doi       = {10.1109/CompComm.2016.7924879},
  file      = {:pdf/Li2016b - High Throughput GPU Polar Decoder.pdf:PDF},
  groups    = {Software Decoders, Polar Codes, HoF Polar - SC},
  keywords  = {block codes, decoding, error correction codes, graphics processing units, linear codes, multi-threading, parallel architectures, high throughput GPU polar decoder, decoding algorithm, optimized thread execution, graphics processing unit, global memory coalesced access, access delay, frozen set semiunrolled architecture, branch divergence, branch delay, Nvidia Tesla K20 platform, Message systems, Graphics, Decoding, polar code, Semi-Fast-SSC, CUDA, semiunrolled, coalesced access, branch divergence},
}

@InProceedings{Han2017,
  author    = {X. Han and R. Liu and Z. Liu and L. Zhao},
  title     = {Successive-Cancellation List Decoder of Polar Codes based on {GPU}},
  booktitle = {International Conference on Computer and Communications (ICCC)},
  year      = {2017},
  pages     = {2065--2070},
  month     = dec,
  publisher = {IEEE},
  abstract  = {In this paper, a Graphics Processing Unit (GPU) implementation of a Successive-Cancellation List (SCL) decoder for polar codes is proposed. Based on the compute unified device architecture (CUDA), the strategy to parallelize the decoding procedure is designed to reduce the latency. Moreover, the optimal design of data structures for several kinds of intermediate information is presented. In order to reduce the global memory accesses in list pruning, The GPU-adapted lazy-copy strategy is elaborated. By means of the parallel computing capabilities of GPUs, the proposed decoder achieves high throughput of 41Mbps on NVIDIA GTX 980 and 65Mbps on TITAN X while decoding the code with length of 1024 bits and 32 lists.},
  doi       = {10.1109/CompComm.2017.8322900},
  file      = {:pdf/Han2017 - Successive-Cancellation List Decoder of Polar Codes Based on GPU.pdf:PDF},
  groups    = {HoF Polar - SC, Software Decoders, Polar Codes, HoF Polar - SCL},
  keywords  = {data structures, decoding, graphics processing units, polar codes, GPU, Successive-Cancellation List decoder, compute unified device architecture, data structures, global memory accesses, list pruning, lazy-copy strategy, Graphics Processing Unit, NVIDIA GTX, TITAN X, CUDA, SCL decoder, Arrays, Graphics processing units, Maximum likelihood decoding, Indexes, Mathematical model, GPU, CUDA, polar codes, SCL, parallel decoding},
}

@Article{Keskin2017b,
  author   = {S. Keskin and T. Kocak},
  title    = {{GPU}-Based Gigabit {LDPC} Decoder},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2017},
  volume   = {21},
  number   = {8},
  pages    = {1703--1706},
  month    = aug,
  issn     = {1089-7798},
  abstract = {In this letter, we present design and implementation of a parallel software low-density parity-check (LDPC) decoding algorithm on graphics processing units (GPUs). As a solution for the LDPC decoder, dedicated application-specific integrated circuit or field programmable gate array implementations is proposed in recent years in order to support high throughput despite their long deployment cycle, high design, and especially fixed functionalities. On the other hand, the implementations on GPU as a software solution provide flexible, scalable, and less expensive solutions in a shorter deployment cycle. We present some GPU-based optimizations for a major LDPC decoder algorithm to obtain high throughput in digital communication systems. Experimental results demonstrate that the proposed LDPC decoder achieves more than 1.27 Gb/s peak throughput on a single GPU.},
  doi      = {10.1109/LCOMM.2017.2704113},
  file     = {:pdf/Keskin2017b - GPU-Based Gigabit LDPC Decoder.pdf:PDF},
  groups   = {HoF Polar - SCL, Software Decoders, LDPC Codes, HoF LDPC - BP},
  keywords = {application specific integrated circuits, decoding, digital communication, field programmable gate arrays, graphics processing units, optimisation, parallel algorithms, parallel architectures, parity check codes, digital communication systems, GPU-based optimizations, deployment cycle, field programmable gate array, application-specific integrated circuit, graphics processing units, parallel software low-density parity-check decoding algorithm, GPU-based gigabit LDPC decoder, Graphics processing units, Decoding, Throughput, Instruction sets, Iterative decoding, MATLAB, High throughput, decoding, MSA, LDPC codes, CUDA, GPU, concatenated decoders},
}

@InProceedings{Keskin2017a,
  author    = {S. Keskin and T. Kocak},
  title     = {{GPU} Accelerated Gigabit Level {BCH} and {LDPC} Concatenated Coding System},
  booktitle = {High Performance Extreme Computing Conference (HPEC)},
  year      = {2017},
  pages     = {1--4},
  month     = sep,
  publisher = {IEEE},
  abstract  = {Increasing data traffic and multimedia services in recent years have paved the way for the development of optical transmission methods to be used in high bandwidth communications systems. In order to meet the very high throughput requirements, dedicated application specific integrated circuit and field programmable gate array solutions for low-density parity-check decoding are proposed in recent years. Conversely, software solutions are less expensive, scalable, and flexible and have shorter development cycle. A natural solution to lower the error floor is to concatenate the LDPC code with an algebraic outer code to clean up the residual errors. In this paper, we present the design and parallel software implementation of a major computation algorithm for LDPC decoding on general purpose graphics processing units as inner code and BCH decoding algorithm as outer code to achieve excellent error-correcting performance. The experimental results show that the proposed GPU-based concatenated decoder achieves the maximum decoding throughput of 1.82Gbps at 10 iterations with low bit-error rate (BER).},
  doi       = {10.1109/HPEC.2017.8091021},
  file      = {:pdf/Keskin2017a - GPU Accelerated Gigabit Level BCH and LDPC Concatenated Coding System.pdf:PDF},
  groups    = {HoF Polar - SCL, Software Decoders, LDPC Codes, HoF LDPC - BP},
  keywords  = {application specific integrated circuits, BCH codes, channel coding, concatenated codes, decoding, error correction codes, error statistics, field programmable gate arrays, graphics processing units, parity check codes, multimedia services, high bandwidth communications systems, dedicated application specific integrated circuit, field programmable gate array solutions, low-density parity-check decoding, LDPC code, algebraic outer code, residual errors, low bit-error rate, general purpose graphics processing units, BCH concatenated coding system, LDPC concatenated coding system, BCH decoding algorithm, GPU-based concatenated decoder, Instruction sets, Graphics processing units, Decoding, Iterative decoding, Throughput, Registers},
}

@InProceedings{Kun2018,
  author    = {D. Kun},
  title     = {High Throughput {GPU} {LDPC} Encoder and Decoder for {DVB-S}2},
  booktitle = {Aerospace Conference (AeroConf)},
  year      = {2018},
  pages     = {1--9},
  month     = mar,
  publisher = {IEEE},
  abstract  = {Previous studies that used Graphics Processing Units (GPUs) to decode Low Density Parity Check (LDPC) codes for DVB-S2 employed inter-codeword parallelism and/or Turbo Decoding Message Passing (TDMP) to achieve high throughput. By converting the LDPC parity check matrix into a quasi-cyclic structure, we show that LDPC encoding can be efficiently implemented on a GPU, and a different approach to LDPC decoding with intra-codeword parallelism and early termination that can achieve approximately 100 Mbps increase in throughput per 0.1 dB increase in signal-to-noise ratio and, for some cases, achieve 1 Gbps or greater overall throughput.},
  doi       = {10.1109/AERO.2018.8396831},
  file      = {:pdf/Kun2018 - High Throughput GPU LDPC Encoder and Decoder for DVB-S2.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords  = {decoding, digital video broadcasting, message passing, parity check codes, turbo codes, Graphics Processing Units, Low Density Parity Check codes, DVB-S2 employed inter-codeword parallelism, Turbo Decoding Message Passing, LDPC parity check matrix, LDPC decoding, Parity check codes, Graphics processing units, Decoding, Kernel, Instruction sets, Buffer storage, Indexes},
}

@InProceedings{Bioglio2017a,
  author    = {V. Bioglio and F. Gabry and I. Land},
  title     = {Low-Complexity Puncturing and Shortening of Polar Codes},
  booktitle = {Wireless Communications and Networking Conference Workshops (WCNCW)},
  year      = {2017},
  pages     = {1--6},
  month     = mar,
  publisher = {IEEE},
  abstract  = {In this work, we address the low-complexity construction of shortened and punctured polar codes from a unified view. While several independent puncturing and shortening designs were attempted in the literature, our goal is a unique, low-complexity construction encompassing both techniques in order to achieve any code length and rate. We observe that our solution significantly reduces the construction complexity as compared to state-of-the-art solutions while providing a block error rate performance comparable to constructions that are highly optimized for specific lengths and rates. This makes the constructed polar codes highly suitable for practical application in future communication systems requiring a large set of polar codes with different lengths and rates.},
  doi       = {10.1109/WCNCW.2017.7919040},
  file      = {:pdf/Bioglio2017a - Low-Complexity Puncturing and Shortening of Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  keywords  = {block codes, communication complexity, error correction codes, linear codes, low-complexity puncturing, low-complexity shortening, shortened polar codes, punctured polar codes, low-complexity code construction, code length, code rate, construction complexity reduction, block error rate performance, communication systems, Decoding, Complexity theory, Measurement, Reliability, Algorithm design and analysis, Encoding, Error analysis},
}

@InProceedings{Gabry2017,
  author    = {F. Gabry and V. Bioglio and I. Land and J. Belfiore},
  title     = {Multi-Kernel Construction of Polar Codes},
  booktitle = {International Conference on Communications (ICC)},
  year      = {2017},
  pages     = {761--765},
  month     = may,
  publisher = {IEEE},
  abstract  = {We propose a generalized construction for binary polar codes based on mixing multiple kernels of different sizes in order to construct polar codes of block lengths that are not only powers of integers. This results in a multi-kernel polar code with very good performance while the encoding complexity remains low and the decoding follows the same general structure as for the original Arikan polar codes. The construction provides numerous practical advantages as more code lengths can be achieved without puncturing or shortening. We observe numerically that the error-rate performance of our construction outperforms state-of-the-art constructions using puncturing methods.},
  doi       = {10.1109/ICCW.2017.7962750},
  file      = {:pdf/Gabry2017 - Multi-Kernel Construction of Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {2474-9133},
  keywords  = {binary codes, block codes, multikernel construction, binary polar codes, block lengths, multikernel polar code, Arikan polar codes, Kernel, Decoding, Encoding, Nickel, 5G mobile communication, Complexity theory, Reliability, Polar Codes, Multiple Kernels, Successive Cancellation Decoding, multi kernel, multi-kernel},
}

@Article{Coppolino2018,
  author   = {G. Coppolino and C. Condo and G. Masera and W. J. Gross},
  title    = {A Multi-Kernel Multi-Code Polar Decoder Architecture},
  journal  = {IEEE Transactions on Circuits and Systems (TCAS)},
  year     = {2018},
  pages    = {1--10},
  issn     = {1549-8328},
  abstract = {Polar codes have received increasing attention in the past decade, and have been selected for the next generation of the wireless communication standard. Most research on polar codes has focused on codes constructed from a 2x2 polarization matrix, called binary kernel: codes constructed from binary kernels have code lengths that are bound to powers of 2. A few recent works have proposed construction methods based on multiple kernels of different dimensions, not only binary ones, allowing code lengths different from powers of 2. In this paper, we design and implement the first multi-kernel successive cancellation polar code decoder in literature. It can decode any code constructed with binary and ternary kernels: the architecture, sized for a maximum code length Nmax, is fully flexible in terms of code length, code rate, and kernel sequence. The decoder can achieve a frequency of over 1 GHz in 65 nm CMOS technology, and a throughput of 615 Mb/s. The area occupation ranges between 0.11 mm² for Nmax=256 and 2.01 mm² for Nmax=4096. Implementation results show an unprecedented degree of flexibility: with Nmax=4096, up to 55 code lengths can be decoded with the same hardware, along with any kernel sequence and code rate.},
  doi      = {10.1109/TCSI.2018.2855679},
  file     = {:pdf/Coppolino2018 - A Multi-Kernel Multi-Code Polar Decoder Architecture.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {Decoding, Kernel, Computer architecture, Encoding, Nickel, Hardware, Indexes, Polar codes, multi-kernel, successive-cancellation decoding, hardware implementation, multi kernel, multi-kernel},
}

@InProceedings{Bioglio2017b,
  author    = {V. Bioglio and F. Gabry and I. Land and J. Belfiore},
  title     = {Minimum-Distance Based Construction of Multi-Kernel Polar Codes},
  booktitle = {Global Communications Conference (GLOBECOM)},
  year      = {2017},
  pages     = {1--6},
  month     = dec,
  publisher = {IEEE},
  abstract  = {In this paper, we propose a construction for multi-kernel polar codes based on the maximization of the minimum distance. Compared to the original construction based on density evolution, our new design shows particular advantages for short code lengths, where the polarization effect has less impact on the performance than the distances of the code. We introduce and compute the minimum-distance profile and provide a simple greedy algorithm for the code design. Compared to state-of-the-art punctured or shortened Arikan polar codes, multi-kernel polar codes with our new design show significantly improved error-rate performance.},
  doi       = {10.1109/GLOCOM.2017.8254147},
  file      = {:pdf/Bioglio2017b - Minimum-Distance Based Construction of Multi-Kernel Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  keywords  = {block codes, channel coding, computational complexity, error correction codes, error statistics, greedy algorithms, linear codes, minimum-distance based construction, multikernel polar codes, polarization effect, Arikan polar codes, greedy algorithm, Kernel, Decoding, Encoding, Error analysis, Reliability engineering, Algorithm design and analysis, multi kernel, multi-kernel},
}

@InProceedings{Benammar2017,
  author    = {M. Benammar and V. Bioglio and F. Gabry and I. Land},
  title     = {Multi-Kernel Polar Codes: Proof of Polarization and Error Exponents},
  booktitle = {Information Theory Workshop (ITW)},
  year      = {2017},
  pages     = {101--105},
  month     = nov,
  publisher = {IEEE},
  abstract  = {In this paper, we investigate a novel family of polar codes based on multi-kernel constructions, proving that this construction actually polarizes. To this end, we derive a new and more general proof of polarization, which gives sufficient conditions for kernels to polarize. Finally, we derive the convergence rate of the multi-kernel construction and relate it to the convergence rate of each of the constituent kernels.},
  doi       = {10.1109/ITW.2017.8277949},
  file      = {:pdf/Benammar2017 - Multi-Kernel Polar Codes\: Proof of Polarization and Error Exponents.pdf:PDF},
  groups    = {Polar Codes},
  keywords  = {convergence, theorem proving, proof of polarization, constituent kernels, convergence rate, multikernel construction, error exponents, multikernel polar codes, Kernel, Convergence, Reliability, Random variables, Decoding, Conferences, multi kernel, multi-kernel},
}

@Article{Hanif2017,
  author   = {M. Hanif and M. Ardakani},
  title    = {Fast Successive-Cancellation Decoding of Polar Codes: Identification and Decoding of New Nodes},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2017},
  volume   = {21},
  number   = {11},
  pages    = {2360--2363},
  month    = nov,
  issn     = {1089-7798},
  abstract = {The decoding latency of polar codes can be reduced by implementing fast parallel decoders in the last stages of decoding. In this letter, we present five such decoders corresponding to different frozen-bit sequences to improve the decoding speed of polar codes. Implementing them achieves significant latency reduction without tangibly altering the bit-error-rate performance of the code.},
  comment  = {node, nodes, type 1, type 2, type 3, type 4, type 5, type I, type II, type III, type IV, type V},
  doi      = {10.1109/LCOMM.2017.2740305},
  file     = {:pdf/Hanif2017 - Fast Successive-Cancellation Decoding of Polar Codes\: Identification and Decoding of New Nodes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {Maximum likelihood decoding, Systematics, Indexes, Corporate acquisitions, Block codes, Polar codes, maximum a posteriori, maximum likelihood, systematic codes, non-systematic codes, node, nodes, type 1, type 2, type 3, type 4, type 5, type I, type II, type III, type IV, type V},
}

@Article{Giard2018,
  author    = {P. Giard and A. Balatsoukas-Stimming and G. Sarkis and C. Thibeault and W. J. Gross},
  title     = {Fast Low-Complexity Decoders for Low-Rate Polar Codes},
  journal   = {Springer Journal of Signal Processing Systems (JSPS)},
  year      = {2018},
  volume    = {90},
  number    = {5},
  pages     = {675},
  month     = may,
  abstract  = {Polar codes are capacity-achieving error-correcting codes with an explicit construction that can be decoded with low-complexity algorithms. In this work, we show how the state-of-the-art low-complexity decoding algorithm can be improved to better accommodate low-rate codes. More constituent codes are recognized in the updated algorithm and dedicated hardware is added to efficiently decode these new constituent codes. We also alter the polar code construction to further decrease the latency and increase the throughput with little to no noticeable effect on error-correction performance. Rate-flexible decoders for polar codes of length 1024 and 2048 are implemented on FPGA. Over the previous work, they are shown to have from 22 % to 28 % lower latency and 26 % to 34 % greater throughput when decoding low-rate codes. On 65 nm ASIC CMOS technology, the proposed decoder for a (1024, 512) polar code is shown to compare favorably against the state-of-the-art ASIC decoders. With a clock frequency of 400 MHz and a supply voltage of 0.8 V, it has a latency of 0.41 μs and an area efficiency of 1.8 Gbps/mm 2 for an energy efficiency of 77 pJ/info. bit. At 600 MHz with a supply of 1 V, the latency is reduced to 0.27 μs and the area efficiency increased to 2.7 Gbps/mm 2 at 115 pJ/info. bit.},
  date      = {2018-05-01},
  doi       = {10.1007/s11265-016-1173-y},
  file      = {:pdf/Giard2018 - Fast Low-Complexity Decoders for Low-Rate Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  keywords  = {nodes, node, rep 1, rep one, rep1, rep-1, 0RepSpc, zero rep spc, 0-rep-spc, 001},
  publisher = {Springer},
}

@Article{Shin2013,
  author        = {D. Shin and S. Lim and K. Yang},
  title         = {Design of Length-Compatible Polar Codes Based on the Reduction of Polarizing Matrices},
  journal       = {IEEE Transactions on Communications (TCOM)},
  year          = {2013},
  volume        = {61},
  number        = {7},
  pages         = {2593--2599},
  month         = jul,
  issn          = {0090-6778},
  __markedentry = {[adrien:]},
  abstract      = {Length-compatible polar codes are a class of polar codes which can support a wide range of lengths with a single pair of encoder and decoder. In this paper we propose a method to construct length-compatible polar codes by employing the reduction of the 2\textsuperscript{n}× 2\textsuperscript{n}polarizing matrix proposed by Arikan. The conditions under which a reduced matrix becomes a polarizing matrix supporting a polar code of a given length are first analyzed. Based on these conditions, length-compatible polar codes are constructed in a suboptimal way by codeword-puncturing and information-refreezing processes. They have low encoding and decoding complexity since they can be encoded and decoded in a similar way as a polar code of length 2\textsuperscript{n}. Numerical results show that length-compatible polar codes designed by the proposed method provide a performance gain of about 1.0 - 5.0 dB over those obtained by random puncturing when successive cancellation decoding is employed.},
  doi           = {10.1109/TCOMM.2013.052013.120543},
  file          = {:pdf/Shin2013 - Design of Length-Compatible Polar Codes Based on the Reduction of Polarizing Matrices.pdf:PDF},
  groups        = {Polar Codes},
  keywords      = {decoding, matrix algebra, random codes, polarizing matrix reduction, encoder, decoder, length-compatible polar code, codeword-puncturing, information-refreezing process, encoding complexity, decoding complexity, Decoding, Encoding, Vectors, Probability density function, Complexity theory, Symmetric matrices, Matrix decomposition, Polar codes, polarizing matrix, density evolution, frozen bit, matrix reduction, puncturing},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:AFF3CT\;0\;1\;\;\;A Fast Forward Error Correction Toolbox\;;
1 StaticGroup:Error-Correcting Codes (ECC)\;2\;1\;\;\;\;;
2 StaticGroup:Software Decoders\;0\;1\;\;\;\;;
2 StaticGroup:Hardware Decoders\;0\;1\;\;\;\;;
2 StaticGroup:Polar Codes\;0\;1\;\;\;\;;
2 StaticGroup:Turbo Codes\;0\;1\;\;\;\;;
2 StaticGroup:LDPC Codes\;0\;1\;\;\;\;;
2 StaticGroup:HoF Software Decoders\;2\;1\;\;\;Hall of Fame for Software Decoders\;;
3 StaticGroup:HoF Polar\;2\;1\;\;\;\;;
4 StaticGroup:HoF Polar - SC\;0\;1\;\;\;\;;
4 StaticGroup:HoF Polar - SCAN\;0\;1\;\;\;\;;
4 StaticGroup:HoF Polar - SCL\;0\;1\;\;\;\;;
3 StaticGroup:HoF LDPC\;2\;1\;\;\;\;;
4 StaticGroup:HoF LDPC - BP\;0\;1\;\;\;\;;
4 StaticGroup:HoF LDPC - LP\;0\;1\;\;\;\;;
3 StaticGroup:HoF Turbo\;2\;1\;\;\;\;;
4 StaticGroup:HoF Turbo - MAP\;0\;1\;\;\;\;;
4 StaticGroup:HoF Turbo - FPTD\;0\;1\;\;\;\;;
1 StaticGroup:Single Instruction Multiple Data (SIMD)\;2\;0\;\;\;\;;
2 StaticGroup:Sort\;2\;1\;\;\;\;;
2 StaticGroup:Wrapper\;2\;1\;\;\;\;;
2 StaticGroup:Kernel\;2\;1\;\;\;\;;
1 StaticGroup:C++\;0\;1\;\;\;\;;
1 StaticGroup:Pseudo-Random Number Generator (PRNG)\;2\;1\;\;\;\;;
1 StaticGroup:Factor Graphs\;2\;1\;\;\;\;;
1 StaticGroup:Cloud-RAN\;0\;1\;\;\;\;;
1 StaticGroup:Software Defined Radio (SDR)\;2\;1\;\;\;\;;
1 StaticGroup:Standards\;2\;1\;\;\;\;;
2 StaticGroup:4G\;0\;1\;\;\;\;;
2 StaticGroup:5G\;0\;1\;\;\;\;;
2 StaticGroup:DVB\;2\;1\;\;\;\;;
1 StaticGroup:Computational Fluid Dynamics (CFD)\;0\;1\;\;\;\;;
}
